{
  
    
        "post0": {
            "title": "Scene Classification with GradCam Visualization",
            "content": ". Overview . This notebook will examine behaviors of a visual explanation methods of deep learning model. The model will train classifying to 6 classes (buildings, forest, glacier, mountain, sea, street) for each images using this datasets. The architecture of the model is a self prepared ResNet18_ . Visual explanation methods that will be examined are . - Grad-CAM https://arxiv.org/abs/1610.02391 . Why this is Useful? . To help deep learning practitioners visually debug their models and properly understand where it’s “looking” in an image, Selvaraju et al. created Gradient-weighted Class Activation Mapping, or more simply, Grad-CAM . | Grad-CAM uses the gradients of any target concept (say logits for “dog” or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. . | . Using Kaggle API to download dataset into Colab Environment . !pip install -q kaggle . from google.colab import files files.upload() . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;janmejaybhoi&#34;,&#34;key&#34;:&#34;6cdf252d44b3db77a67e30fff01bf8a0&#34;}&#39;} . !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ . ! chmod 600 ~/.kaggle/kaggle.json . !kaggle datasets download -d puneet6060/intel-image-classification . Downloading intel-image-classification.zip to /content 100% 345M/346M [00:02&lt;00:00, 131MB/s] 100% 346M/346M [00:02&lt;00:00, 131MB/s] . !unzip /content/intel-image-classification . Import necessary libraries . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import random import tensorflow as tf from tensorflow import keras from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 from tensorflow.keras.layers import * from tensorflow.keras.models import Model, load_model from tensorflow.keras.initializers import glorot_uniform from tensorflow.keras.utils import plot_model from tensorflow.keras import backend as K from tensorflow.keras.optimizers import SGD from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler from sklearn.metrics import confusion_matrix, classification_report, accuracy_score from random import sample from IPython.display import display import os import PIL . Read Data . train = {} test = {} path = &quot;/content/intel-image-classification&quot; # Make dictionary storing images for each category under train data. path_train = os.path.join(path, &quot;seg_train/seg_train&quot;) for i in os.listdir(path_train): train[i] = os.listdir(os.path.join(path_train, i)) # Make dictionary storing images for each category under test data. path_test = os.path.join(path, &quot;seg_test/seg_test&quot;) for i in os.listdir(path_test): test[i] = os.listdir(os.path.join(path_test, i)) . Explore data . len_train = np.concatenate(list(train.values())).shape[0] len_test = np.concatenate(list(test.values())).shape[0] print(&quot;Number of images in training data : {}&quot;.format(len_train)) print(&quot;Number of images in testing data : {}&quot;.format(len_test)) . Number of images in training data : 14034 Number of images in testing data : 3000 . # You will see different images each time. fig, axs = plt.subplots(6, 5, figsize = (15, 15)) for i, item in enumerate(os.listdir(path_train)): images = sample(train[item], 5) for j, image in enumerate(images): img = PIL.Image.open(os.path.join(path_train, item, image)) axs[i, j].imshow(img) axs[i, j].set(xlabel = item, xticks = [], yticks = []) fig.tight_layout() . for item in train.keys(): print(item, len(train[item])) . sea 2274 glacier 2404 buildings 2191 street 2382 forest 2271 mountain 2512 . # This is often useful when you want your dataset to be balanced. fig, ax = plt.subplots() ax.pie( [len(train[item]) for item in train], labels = train.keys(), autopct = &quot;%1.1f%%&quot; ) fig.show() . Augment data . # Here we go with zooming, flipping (horizontally and vertically), and rescaling. train_datagen = ImageDataGenerator( zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, rescale=1./255 ) # For test data we only rescale the data. # Never augment test data!!! test_datagen = ImageDataGenerator(rescale=1./255) . Create data generator . # This will make images (including augmented ones) start flowing from the directory to the model. # Note that augmented images are not stored along with the original images. The process happens in memory. # Train generator train_generator = train_datagen.flow_from_directory( path_train, target_size=(256, 256), batch_size=32, class_mode=&#39;categorical&#39; ) # Test generator test_generator = test_datagen.flow_from_directory( path_test, target_size=(256, 256), batch_size=32, class_mode=&#39;categorical&#39; ) . Found 14034 images belonging to 6 classes. Found 3000 images belonging to 6 classes. . Develop Neural Network Architecture . # You can use a different architecture if you like. def res_block(X, filter, stage): # Convolutional_block X_copy = X f1 , f2, f3 = filter # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_conv_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = MaxPool2D((2,2))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_conv_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_conv_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_c&#39;)(X) # Short path X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_conv_copy&#39;, kernel_initializer= glorot_uniform(seed = 0))(X_copy) X_copy = MaxPool2D((2,2))(X_copy) X_copy = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_copy&#39;)(X_copy) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) # Identity Block 1 X_copy = X # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_identity_1_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_identity_1_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_identity_1_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis = 3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_c&#39;)(X) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) # Identity Block 2 X_copy = X # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_identity_2_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_identity_2_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_identity_2_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_c&#39;)(X) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) return X . input_shape = (256,256,3) # Input tensor shape X_input = Input(input_shape) # Zero-padding X = ZeroPadding2D((3,3))(X_input) # 1- stage X = Conv2D(64, (7,7), strides= (2,2), name = &#39;conv1&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_conv1&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = MaxPooling2D((3,3), strides= (2,2))(X) # 2- stage X = res_block(X, filter= [64,64,256], stage= 2) # 3- stage X = res_block(X, filter= [128,128,512], stage= 3) # 4- stage X = res_block(X, filter= [256,256,1024], stage= 4) # 5- stage X = res_block(X, filter= [512,512,2048], stage= 5) # Average Pooling X = AveragePooling2D((2,2), name = &#39;Averagea_Pooling&#39;)(X) # Final layer X = Flatten()(X) X = Dropout(0.4)(X) X = Dense(6, activation = &#39;softmax&#39;, name = &#39;Dense_final&#39;, kernel_initializer= glorot_uniform(seed=0))(X) # Build model. model = Model( inputs= X_input, outputs = X, name = &#39;Resnet18&#39; ) # Check out model summary. model.summary() . Model: &#34;Resnet18&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 256, 256, 3) 0 __________________________________________________________________________________________________ zero_padding2d (ZeroPadding2D) (None, 262, 262, 3) 0 input_1[0][0] __________________________________________________________________________________________________ conv1 (Conv2D) (None, 128, 128, 64) 9472 zero_padding2d[0][0] __________________________________________________________________________________________________ bn_conv1 (BatchNormalization) (None, 128, 128, 64) 256 conv1[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 128, 128, 64) 0 bn_conv1[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 63, 63, 64) 0 activation[0][0] __________________________________________________________________________________________________ res_2_conv_a (Conv2D) (None, 63, 63, 64) 4160 max_pooling2d[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 31, 31, 64) 0 res_2_conv_a[0][0] __________________________________________________________________________________________________ bn_2_conv_a (BatchNormalization (None, 31, 31, 64) 256 max_pooling2d_1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 31, 31, 64) 0 bn_2_conv_a[0][0] __________________________________________________________________________________________________ res_2_conv_b (Conv2D) (None, 31, 31, 64) 36928 activation_1[0][0] __________________________________________________________________________________________________ bn_2_conv_b (BatchNormalization (None, 31, 31, 64) 256 res_2_conv_b[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 31, 31, 64) 0 bn_2_conv_b[0][0] __________________________________________________________________________________________________ res_2_conv_copy (Conv2D) (None, 63, 63, 256) 16640 max_pooling2d[0][0] __________________________________________________________________________________________________ res_2_conv_c (Conv2D) (None, 31, 31, 256) 16640 activation_2[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 31, 31, 256) 0 res_2_conv_copy[0][0] __________________________________________________________________________________________________ bn_2_conv_c (BatchNormalization (None, 31, 31, 256) 1024 res_2_conv_c[0][0] __________________________________________________________________________________________________ bn_2_conv_copy (BatchNormalizat (None, 31, 31, 256) 1024 max_pooling2d_2[0][0] __________________________________________________________________________________________________ add (Add) (None, 31, 31, 256) 0 bn_2_conv_c[0][0] bn_2_conv_copy[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 31, 31, 256) 0 add[0][0] __________________________________________________________________________________________________ res_2_identity_1_a (Conv2D) (None, 31, 31, 64) 16448 activation_3[0][0] __________________________________________________________________________________________________ bn_2_identity_1_a (BatchNormali (None, 31, 31, 64) 256 res_2_identity_1_a[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 31, 31, 64) 0 bn_2_identity_1_a[0][0] __________________________________________________________________________________________________ res_2_identity_1_b (Conv2D) (None, 31, 31, 64) 36928 activation_4[0][0] __________________________________________________________________________________________________ bn_2_identity_1_b (BatchNormali (None, 31, 31, 64) 256 res_2_identity_1_b[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 31, 31, 64) 0 bn_2_identity_1_b[0][0] __________________________________________________________________________________________________ res_2_identity_1_c (Conv2D) (None, 31, 31, 256) 16640 activation_5[0][0] __________________________________________________________________________________________________ bn_2_identity_1_c (BatchNormali (None, 31, 31, 256) 1024 res_2_identity_1_c[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 31, 31, 256) 0 bn_2_identity_1_c[0][0] activation_3[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 31, 31, 256) 0 add_1[0][0] __________________________________________________________________________________________________ res_2_identity_2_a (Conv2D) (None, 31, 31, 64) 16448 activation_6[0][0] __________________________________________________________________________________________________ bn_2_identity_2_a (BatchNormali (None, 31, 31, 64) 256 res_2_identity_2_a[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 31, 31, 64) 0 bn_2_identity_2_a[0][0] __________________________________________________________________________________________________ res_2_identity_2_b (Conv2D) (None, 31, 31, 64) 36928 activation_7[0][0] __________________________________________________________________________________________________ bn_2_identity_2_b (BatchNormali (None, 31, 31, 64) 256 res_2_identity_2_b[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 31, 31, 64) 0 bn_2_identity_2_b[0][0] __________________________________________________________________________________________________ res_2_identity_2_c (Conv2D) (None, 31, 31, 256) 16640 activation_8[0][0] __________________________________________________________________________________________________ bn_2_identity_2_c (BatchNormali (None, 31, 31, 256) 1024 res_2_identity_2_c[0][0] __________________________________________________________________________________________________ add_2 (Add) (None, 31, 31, 256) 0 bn_2_identity_2_c[0][0] activation_6[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 31, 31, 256) 0 add_2[0][0] __________________________________________________________________________________________________ res_3_conv_a (Conv2D) (None, 31, 31, 128) 32896 activation_9[0][0] __________________________________________________________________________________________________ max_pooling2d_3 (MaxPooling2D) (None, 15, 15, 128) 0 res_3_conv_a[0][0] __________________________________________________________________________________________________ bn_3_conv_a (BatchNormalization (None, 15, 15, 128) 512 max_pooling2d_3[0][0] __________________________________________________________________________________________________ activation_10 (Activation) (None, 15, 15, 128) 0 bn_3_conv_a[0][0] __________________________________________________________________________________________________ res_3_conv_b (Conv2D) (None, 15, 15, 128) 147584 activation_10[0][0] __________________________________________________________________________________________________ bn_3_conv_b (BatchNormalization (None, 15, 15, 128) 512 res_3_conv_b[0][0] __________________________________________________________________________________________________ activation_11 (Activation) (None, 15, 15, 128) 0 bn_3_conv_b[0][0] __________________________________________________________________________________________________ res_3_conv_copy (Conv2D) (None, 31, 31, 512) 131584 activation_9[0][0] __________________________________________________________________________________________________ res_3_conv_c (Conv2D) (None, 15, 15, 512) 66048 activation_11[0][0] __________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 15, 15, 512) 0 res_3_conv_copy[0][0] __________________________________________________________________________________________________ bn_3_conv_c (BatchNormalization (None, 15, 15, 512) 2048 res_3_conv_c[0][0] __________________________________________________________________________________________________ bn_3_conv_copy (BatchNormalizat (None, 15, 15, 512) 2048 max_pooling2d_4[0][0] __________________________________________________________________________________________________ add_3 (Add) (None, 15, 15, 512) 0 bn_3_conv_c[0][0] bn_3_conv_copy[0][0] __________________________________________________________________________________________________ activation_12 (Activation) (None, 15, 15, 512) 0 add_3[0][0] __________________________________________________________________________________________________ res_3_identity_1_a (Conv2D) (None, 15, 15, 128) 65664 activation_12[0][0] __________________________________________________________________________________________________ bn_3_identity_1_a (BatchNormali (None, 15, 15, 128) 512 res_3_identity_1_a[0][0] __________________________________________________________________________________________________ activation_13 (Activation) (None, 15, 15, 128) 0 bn_3_identity_1_a[0][0] __________________________________________________________________________________________________ res_3_identity_1_b (Conv2D) (None, 15, 15, 128) 147584 activation_13[0][0] __________________________________________________________________________________________________ bn_3_identity_1_b (BatchNormali (None, 15, 15, 128) 512 res_3_identity_1_b[0][0] __________________________________________________________________________________________________ activation_14 (Activation) (None, 15, 15, 128) 0 bn_3_identity_1_b[0][0] __________________________________________________________________________________________________ res_3_identity_1_c (Conv2D) (None, 15, 15, 512) 66048 activation_14[0][0] __________________________________________________________________________________________________ bn_3_identity_1_c (BatchNormali (None, 15, 15, 512) 2048 res_3_identity_1_c[0][0] __________________________________________________________________________________________________ add_4 (Add) (None, 15, 15, 512) 0 bn_3_identity_1_c[0][0] activation_12[0][0] __________________________________________________________________________________________________ activation_15 (Activation) (None, 15, 15, 512) 0 add_4[0][0] __________________________________________________________________________________________________ res_3_identity_2_a (Conv2D) (None, 15, 15, 128) 65664 activation_15[0][0] __________________________________________________________________________________________________ bn_3_identity_2_a (BatchNormali (None, 15, 15, 128) 512 res_3_identity_2_a[0][0] __________________________________________________________________________________________________ activation_16 (Activation) (None, 15, 15, 128) 0 bn_3_identity_2_a[0][0] __________________________________________________________________________________________________ res_3_identity_2_b (Conv2D) (None, 15, 15, 128) 147584 activation_16[0][0] __________________________________________________________________________________________________ bn_3_identity_2_b (BatchNormali (None, 15, 15, 128) 512 res_3_identity_2_b[0][0] __________________________________________________________________________________________________ activation_17 (Activation) (None, 15, 15, 128) 0 bn_3_identity_2_b[0][0] __________________________________________________________________________________________________ res_3_identity_2_c (Conv2D) (None, 15, 15, 512) 66048 activation_17[0][0] __________________________________________________________________________________________________ bn_3_identity_2_c (BatchNormali (None, 15, 15, 512) 2048 res_3_identity_2_c[0][0] __________________________________________________________________________________________________ add_5 (Add) (None, 15, 15, 512) 0 bn_3_identity_2_c[0][0] activation_15[0][0] __________________________________________________________________________________________________ activation_18 (Activation) (None, 15, 15, 512) 0 add_5[0][0] __________________________________________________________________________________________________ res_4_conv_a (Conv2D) (None, 15, 15, 256) 131328 activation_18[0][0] __________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 7, 7, 256) 0 res_4_conv_a[0][0] __________________________________________________________________________________________________ bn_4_conv_a (BatchNormalization (None, 7, 7, 256) 1024 max_pooling2d_5[0][0] __________________________________________________________________________________________________ activation_19 (Activation) (None, 7, 7, 256) 0 bn_4_conv_a[0][0] __________________________________________________________________________________________________ res_4_conv_b (Conv2D) (None, 7, 7, 256) 590080 activation_19[0][0] __________________________________________________________________________________________________ bn_4_conv_b (BatchNormalization (None, 7, 7, 256) 1024 res_4_conv_b[0][0] __________________________________________________________________________________________________ activation_20 (Activation) (None, 7, 7, 256) 0 bn_4_conv_b[0][0] __________________________________________________________________________________________________ res_4_conv_copy (Conv2D) (None, 15, 15, 1024) 525312 activation_18[0][0] __________________________________________________________________________________________________ res_4_conv_c (Conv2D) (None, 7, 7, 1024) 263168 activation_20[0][0] __________________________________________________________________________________________________ max_pooling2d_6 (MaxPooling2D) (None, 7, 7, 1024) 0 res_4_conv_copy[0][0] __________________________________________________________________________________________________ bn_4_conv_c (BatchNormalization (None, 7, 7, 1024) 4096 res_4_conv_c[0][0] __________________________________________________________________________________________________ bn_4_conv_copy (BatchNormalizat (None, 7, 7, 1024) 4096 max_pooling2d_6[0][0] __________________________________________________________________________________________________ add_6 (Add) (None, 7, 7, 1024) 0 bn_4_conv_c[0][0] bn_4_conv_copy[0][0] __________________________________________________________________________________________________ activation_21 (Activation) (None, 7, 7, 1024) 0 add_6[0][0] __________________________________________________________________________________________________ res_4_identity_1_a (Conv2D) (None, 7, 7, 256) 262400 activation_21[0][0] __________________________________________________________________________________________________ bn_4_identity_1_a (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_1_a[0][0] __________________________________________________________________________________________________ activation_22 (Activation) (None, 7, 7, 256) 0 bn_4_identity_1_a[0][0] __________________________________________________________________________________________________ res_4_identity_1_b (Conv2D) (None, 7, 7, 256) 590080 activation_22[0][0] __________________________________________________________________________________________________ bn_4_identity_1_b (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_1_b[0][0] __________________________________________________________________________________________________ activation_23 (Activation) (None, 7, 7, 256) 0 bn_4_identity_1_b[0][0] __________________________________________________________________________________________________ res_4_identity_1_c (Conv2D) (None, 7, 7, 1024) 263168 activation_23[0][0] __________________________________________________________________________________________________ bn_4_identity_1_c (BatchNormali (None, 7, 7, 1024) 4096 res_4_identity_1_c[0][0] __________________________________________________________________________________________________ add_7 (Add) (None, 7, 7, 1024) 0 bn_4_identity_1_c[0][0] activation_21[0][0] __________________________________________________________________________________________________ activation_24 (Activation) (None, 7, 7, 1024) 0 add_7[0][0] __________________________________________________________________________________________________ res_4_identity_2_a (Conv2D) (None, 7, 7, 256) 262400 activation_24[0][0] __________________________________________________________________________________________________ bn_4_identity_2_a (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_2_a[0][0] __________________________________________________________________________________________________ activation_25 (Activation) (None, 7, 7, 256) 0 bn_4_identity_2_a[0][0] __________________________________________________________________________________________________ res_4_identity_2_b (Conv2D) (None, 7, 7, 256) 590080 activation_25[0][0] __________________________________________________________________________________________________ bn_4_identity_2_b (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_2_b[0][0] __________________________________________________________________________________________________ activation_26 (Activation) (None, 7, 7, 256) 0 bn_4_identity_2_b[0][0] __________________________________________________________________________________________________ res_4_identity_2_c (Conv2D) (None, 7, 7, 1024) 263168 activation_26[0][0] __________________________________________________________________________________________________ bn_4_identity_2_c (BatchNormali (None, 7, 7, 1024) 4096 res_4_identity_2_c[0][0] __________________________________________________________________________________________________ add_8 (Add) (None, 7, 7, 1024) 0 bn_4_identity_2_c[0][0] activation_24[0][0] __________________________________________________________________________________________________ activation_27 (Activation) (None, 7, 7, 1024) 0 add_8[0][0] __________________________________________________________________________________________________ res_5_conv_a (Conv2D) (None, 7, 7, 512) 524800 activation_27[0][0] __________________________________________________________________________________________________ max_pooling2d_7 (MaxPooling2D) (None, 3, 3, 512) 0 res_5_conv_a[0][0] __________________________________________________________________________________________________ bn_5_conv_a (BatchNormalization (None, 3, 3, 512) 2048 max_pooling2d_7[0][0] __________________________________________________________________________________________________ activation_28 (Activation) (None, 3, 3, 512) 0 bn_5_conv_a[0][0] __________________________________________________________________________________________________ res_5_conv_b (Conv2D) (None, 3, 3, 512) 2359808 activation_28[0][0] __________________________________________________________________________________________________ bn_5_conv_b (BatchNormalization (None, 3, 3, 512) 2048 res_5_conv_b[0][0] __________________________________________________________________________________________________ activation_29 (Activation) (None, 3, 3, 512) 0 bn_5_conv_b[0][0] __________________________________________________________________________________________________ res_5_conv_copy (Conv2D) (None, 7, 7, 2048) 2099200 activation_27[0][0] __________________________________________________________________________________________________ res_5_conv_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_29[0][0] __________________________________________________________________________________________________ max_pooling2d_8 (MaxPooling2D) (None, 3, 3, 2048) 0 res_5_conv_copy[0][0] __________________________________________________________________________________________________ bn_5_conv_c (BatchNormalization (None, 3, 3, 2048) 8192 res_5_conv_c[0][0] __________________________________________________________________________________________________ bn_5_conv_copy (BatchNormalizat (None, 3, 3, 2048) 8192 max_pooling2d_8[0][0] __________________________________________________________________________________________________ add_9 (Add) (None, 3, 3, 2048) 0 bn_5_conv_c[0][0] bn_5_conv_copy[0][0] __________________________________________________________________________________________________ activation_30 (Activation) (None, 3, 3, 2048) 0 add_9[0][0] __________________________________________________________________________________________________ res_5_identity_1_a (Conv2D) (None, 3, 3, 512) 1049088 activation_30[0][0] __________________________________________________________________________________________________ bn_5_identity_1_a (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_1_a[0][0] __________________________________________________________________________________________________ activation_31 (Activation) (None, 3, 3, 512) 0 bn_5_identity_1_a[0][0] __________________________________________________________________________________________________ res_5_identity_1_b (Conv2D) (None, 3, 3, 512) 2359808 activation_31[0][0] __________________________________________________________________________________________________ bn_5_identity_1_b (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_1_b[0][0] __________________________________________________________________________________________________ activation_32 (Activation) (None, 3, 3, 512) 0 bn_5_identity_1_b[0][0] __________________________________________________________________________________________________ res_5_identity_1_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_32[0][0] __________________________________________________________________________________________________ bn_5_identity_1_c (BatchNormali (None, 3, 3, 2048) 8192 res_5_identity_1_c[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 3, 3, 2048) 0 bn_5_identity_1_c[0][0] activation_30[0][0] __________________________________________________________________________________________________ activation_33 (Activation) (None, 3, 3, 2048) 0 add_10[0][0] __________________________________________________________________________________________________ res_5_identity_2_a (Conv2D) (None, 3, 3, 512) 1049088 activation_33[0][0] __________________________________________________________________________________________________ bn_5_identity_2_a (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_2_a[0][0] __________________________________________________________________________________________________ activation_34 (Activation) (None, 3, 3, 512) 0 bn_5_identity_2_a[0][0] __________________________________________________________________________________________________ res_5_identity_2_b (Conv2D) (None, 3, 3, 512) 2359808 activation_34[0][0] __________________________________________________________________________________________________ bn_5_identity_2_b (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_2_b[0][0] __________________________________________________________________________________________________ activation_35 (Activation) (None, 3, 3, 512) 0 bn_5_identity_2_b[0][0] __________________________________________________________________________________________________ res_5_identity_2_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_35[0][0] __________________________________________________________________________________________________ bn_5_identity_2_c (BatchNormali (None, 3, 3, 2048) 8192 res_5_identity_2_c[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 3, 3, 2048) 0 bn_5_identity_2_c[0][0] activation_33[0][0] __________________________________________________________________________________________________ activation_36 (Activation) (None, 3, 3, 2048) 0 add_11[0][0] __________________________________________________________________________________________________ Averagea_Pooling (AveragePoolin (None, 1, 1, 2048) 0 activation_36[0][0] __________________________________________________________________________________________________ flatten (Flatten) (None, 2048) 0 Averagea_Pooling[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 2048) 0 flatten[0][0] __________________________________________________________________________________________________ Dense_final (Dense) (None, 6) 12294 dropout[0][0] ================================================================================================== Total params: 19,952,262 Trainable params: 19,909,894 Non-trainable params: 42,368 __________________________________________________________________________________________________ . Compile model . model.compile( optimizer = &quot;adam&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;] ) . Specify callbacks . earlystopping = EarlyStopping( monitor = &#39;loss&#39;, mode = &#39;min&#39;, verbose = 1, patience = 15 ) # Save the best model with lower validation loss checkpointer = ModelCheckpoint( filepath = &quot;weights.hdf5&quot;, verbose = 1, save_best_only = True ) . Model training . # Here we use 1 epoch for demonstration. history = model.fit_generator( train_generator, steps_per_epoch = train_generator.n // 32, epochs = 5, callbacks = [ checkpointer, earlystopping ] ) . /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators. warnings.warn(&#39;`Model.fit_generator` is deprecated and &#39; . Epoch 1/5 438/438 [==============================] - 180s 412ms/step - loss: 0.8367 - accuracy: 0.6858 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 2/5 438/438 [==============================] - 180s 411ms/step - loss: 0.7321 - accuracy: 0.7325 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 3/5 438/438 [==============================] - 180s 411ms/step - loss: 0.6380 - accuracy: 0.7719 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 4/5 438/438 [==============================] - 181s 412ms/step - loss: 0.6049 - accuracy: 0.7787 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 5/5 438/438 [==============================] - 180s 411ms/step - loss: 0.5641 - accuracy: 0.7992 WARNING:tensorflow:Can save best model only with val_loss available, skipping. . Model evaluation . evaluate = model.evaluate_generator( test_generator, steps = test_generator.n // 32, verbose = 1 ) . /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1973: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators. warnings.warn(&#39;`Model.evaluate_generator` is deprecated and &#39; . 93/93 [==============================] - 7s 66ms/step - loss: 1.0486 - accuracy: 0.6116 . labels = { 0: &#39;buildings&#39;, 1: &#39;forest&#39;, 2: &#39;glacier&#39;, 3: &#39;mountain&#39;, 4: &#39;sea&#39;, 5: &#39;street&#39; } . prediction = [] original = [] image = [] count = 0 for i in os.listdir(path_test): for item in os.listdir(os.path.join(path_test, i)): # code to open the image img= PIL.Image.open(os.path.join(path_test, i, item)) # resizing the image to (256,256) img = img.resize((256, 256)) # appending image to the image list image.append(img) # converting image to array img = np.asarray(img, dtype = np.float32) # normalizing the image img = img / 255 # reshaping the image into a 4D array img = img.reshape(-1, 256, 256, 3) # making prediction of the model predict = model.predict(img) # getting the index corresponding to the highest value in the prediction predict = np.argmax(predict) # appending the predicted class to the list prediction.append(labels[predict]) # appending original class to the list original.append(i) . score = accuracy_score(original, prediction) print(&quot;Test Accuracy : {}&quot;.format(score)) . Test Accuracy : 0.6333333333333333 . fig = plt.figure(figsize = (100,100)) for i in range(20): j = random.randint(0, len(image)) fig.add_subplot(20, 1, i+1) plt.xlabel(&quot;Prediction: &quot; + prediction[j] +&quot; Original: &quot; + original[j]) plt.imshow(image[j]) fig.tight_layout() plt.show() . print(classification_report(np.asarray(prediction), np.asarray(original))) # Based on these values, you can try t improve your model. # For the sake of simplicity, hyperparameter tuning and model improvement was not done. . precision recall f1-score support buildings 0.79 0.68 0.73 510 forest 0.85 0.93 0.89 434 glacier 0.22 0.88 0.35 137 mountain 0.25 0.73 0.38 183 sea 0.94 0.41 0.57 1152 street 0.84 0.72 0.77 584 accuracy 0.63 3000 macro avg 0.65 0.72 0.62 3000 weighted avg 0.81 0.63 0.66 3000 . plt.figure(figsize = (7, 5)) cm = confusion_matrix(np.asarray(prediction), np.asarray(original)) sns.heatmap( cm, annot = True, fmt = &quot;d&quot; ) plt.show() . Grad Cam Visualization . def grad_cam(img): # Convert the image to array of type float32 img = np.asarray(img, dtype = np.float32) # Reshape the image from (256,256,3) to (1,256,256,3) img = img.reshape(-1, 256, 256, 3) img_scaled = img / 255 # Name of the average pooling layer and dense final (you can see these names in the model summary) classification_layers = [&quot;Averagea_Pooling&quot;, &quot;Dense_final&quot;] # Last convolutional layer in the model final_conv = model.get_layer(&quot;res_5_identity_2_c&quot;) # Create a model with original model inputs and the last conv_layer as the output final_conv_model = keras.Model(model.inputs, final_conv.output) # Then we create the input for classification layer, which is the output of last conv layer # In our case, output produced by the conv layer is of the shape (1,3,3,2048) # Since the classification input needs the features as input, we ignore the batch dimension classification_input = keras.Input(shape = final_conv.output.shape[1:]) # We iterate through the classification layers, to get the final layer and then append # the layer as the output layer to the classification model. temp = classification_input for layer in classification_layers: temp = model.get_layer(layer)(temp) classification_model = keras.Model(classification_input, temp) # We use gradient tape to monitor the &#39;final_conv_output&#39; to retrive the gradients # corresponding to the predicted class with tf.GradientTape() as tape: # Pass the image through the base model and get the feature map final_conv_output = final_conv_model(img_scaled) # Assign gradient tape to monitor the conv_output tape.watch(final_conv_output) # Pass the feature map through the classification model and use argmax to get the # index of the predicted class and then use the index to get the value produced by final # layer for that class prediction = classification_model(final_conv_output) predicted_class = tf.argmax(prediction[0][0][0]) predicted_class_value = prediction[:,:,:,predicted_class] # Get the gradient corresponding to the predicted class based on feature map. # which is of shape (1,3,3,2048) gradient = tape.gradient(predicted_class_value, final_conv_output) # Since we need the filter values (2048), we reduce the other dimensions, # which would result in a shape of (2048,) gradient_channels = tf.reduce_mean(gradient, axis=(0, 1, 2)) # We then convert the feature map produced by last conv layer(1,6,6,1536) to (6,6,1536) final_conv_output = final_conv_output.numpy()[0] gradient_channels = gradient_channels.numpy() # We multiply the filters in the feature map produced by final conv layer by the # filter values that are used to get the predicted class. By doing this we inrease the # value of areas that helped in making the prediction and lower the vlaue of areas, that # did not contribute towards the final prediction for i in range(gradient_channels.shape[-1]): final_conv_output[:, :, i] *= gradient_channels[i] # We take the mean accross the channels to get the feature map heatmap = np.mean(final_conv_output, axis=-1) # Normalizing the heat map between 0 and 1, to visualize it heatmap_normalized = np.maximum(heatmap, 0) / np.max(heatmap) # Rescaling and converting the type to int heatmap = np.uint8(255 * heatmap_normalized ) # Create the colormap color_map = plt.cm.get_cmap(&#39;jet&#39;) # get only the rb features from the heatmap color_map = color_map(np.arange(256))[:, :3] heatmap = color_map[heatmap] # convert the array to image, resize the image and then convert to array heatmap = keras.preprocessing.image.array_to_img(heatmap) heatmap = heatmap.resize((256, 256)) heatmap = np.asarray(heatmap, dtype = np.float32) # Add the heatmap on top of the original image final_img = heatmap * 0.4 + img[0] final_img = keras.preprocessing.image.array_to_img(final_img) return final_img, heatmap_normalized . fig, axs = plt.subplots(6,3, figsize = (16,32)) count = 0 for _ in range(6): i = random.randint(0, len(image)) gradcam, heatmap = grad_cam(image[i]) axs[count][0].title.set_text(&quot;Original -&quot; + original[i]) axs[count][0].imshow(image[i]) axs[count][1].title.set_text(&quot;Heatmap&quot;) axs[count][1].imshow(heatmap) axs[count][2].title.set_text(&quot;Prediction -&quot; + prediction[i]) axs[count][2].imshow(gradcam) count += 1 fig.tight_layout() . Future Work . To implement and visualize class activation map with Grad-CAM ++ and Score-CAM . - Grad-CAM++ https://arxiv.org/abs/1710.11063 - Score-CAM https://arxiv.org/abs/1910.01279 .",
            "url": "https://janmejaybhoi.github.io/w/computer%20vision/image%20processing/deep%20learning/multi-class%20classification/2021/05/10/GradCam-Resnet.html",
            "relUrl": "/computer%20vision/image%20processing/deep%20learning/multi-class%20classification/2021/05/10/GradCam-Resnet.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Sequential Sentence Classification in Medical Abstracts",
            "content": "Overview: . Classify a Randomized clinical trials (RCTs) abstarct to subclasses for easier to read and understand. | Basically convert a medical abstarct to chunks of sentences of particaular classes like &quot;Background&quot;, &quot;Methods&quot;, &quot;Results&quot; and &quot;Conclusion&quot;. | Its a Many to One Text Classification problem. Where we categorize a sequence to a prticular class. | . Dataset Used: . PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts . PubMed 20k is a subset of PubMed 200k. I.e., any abstract present in PubMed 20k is also present in PubMed 200k. . | PubMed_200k_RCT is the same as PubMed_200k_RCT_numbers_replaced_with_at_sign, except that in the latter all numbers had been replaced by @. (same for PubMed_20k_RCT vs. PubMed_20k_RCT_numbers_replaced_with_at_sign) . | . !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git . Cloning into &#39;pubmed-rct&#39;... remote: Enumerating objects: 33, done. remote: Counting objects: 100% (3/3), done. remote: Compressing objects: 100% (3/3), done. remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 30 Unpacking objects: 100% (33/33), done. . !ls pubmed-rct . PubMed_200k_RCT PubMed_200k_RCT_numbers_replaced_with_at_sign PubMed_20k_RCT PubMed_20k_RCT_numbers_replaced_with_at_sign README.md . !ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign . dev.txt test.txt train.txt . train.txt - training samples. | dev.txt - dev is short for development set, which is another name for validation set (in our case, we&#39;ll be using and referring to this file as our validation set). | test.txt - test samples. | . data_dir = &quot;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/&quot; . import os filenames = [data_dir +filename for filename in os.listdir(data_dir)] . filenames . [&#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt&#39;, &#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt&#39;, &#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt&#39;] . Preprocessing . def get_lines(filename): &quot;&quot;&quot; &quot;&quot;&quot; with open(filename, &quot;r&quot;) as f: return f.readlines() . train_lines = get_lines(&#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt&#39;) train_lines[:10] . [&#39;###24293578 n&#39;, &#39;OBJECTIVE tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) . n&#39;, &#39;METHODS tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks . n&#39;, &#39;METHODS tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers . n&#39;, &#39;METHODS tPain was assessed using the visual analog pain scale ( @-@ mm ) . n&#39;, &#39;METHODS tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) . n&#39;, &#39;METHODS tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured . n&#39;, &#39;RESULTS tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks . n&#39;, &#39;RESULTS tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively . n&#39;, &#39;RESULTS tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group . n&#39;] . Example returned preprocessed sample (a single line from an abstract): . Return all of the lines in the target text file as a list of dictionaries containing the key/value pairs: &quot;line_number&quot; - the position of the line in the abstract (e.g. 3). &quot;target&quot; - the role of the line in the abstract (e.g. OBJECTIVE). . &quot;text&quot; - the text of the line in the abstract. | &quot;total_lines&quot; - the total lines in an abstract sample (e.g. 14) | . [{&#39;line_number&#39;: 0, &#39;target&#39;: &#39;OBJECTIVE&#39;, &#39;text&#39;: &#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;total_lines&#39;: 11}, ...] . def preprocess_text_with_line_numbers(filename): input_lines = get_lines(filename) # get all lines from filename abstract_lines = &quot;&quot; # create an empty abstract abstract_samples = [] # create an empty list of abstracts for line in input_lines: if line.startswith(&quot;###&quot;): # check to see if line is an ID line abstract_id = line abstract_lines = &quot;&quot; # reset the abstract string elif line.isspace(): abstract_line_split = abstract_lines.splitlines() # split the abstract into separate lines for abstract_line_number, abstract_line in enumerate(abstract_line_split): line_data = {} target_text_split = abstract_line.split(&quot; t&quot;) line_data[&quot;target&quot;] = target_text_split[0] line_data[&quot;text&quot;] = target_text_split[1].lower() line_data[&quot;line_number&quot;] = abstract_line_number line_data[&quot;total_lines&quot;] = len(abstract_line_split) - 1 abstract_samples.append(line_data) else: abstract_lines += line return abstract_samples . %%time train_samples = preprocess_text_with_line_numbers(data_dir + &quot;train.txt&quot;) val_samples = preprocess_text_with_line_numbers(data_dir + &quot;dev.txt&quot;) # dev is another name for validation set test_samples = preprocess_text_with_line_numbers(data_dir + &quot;test.txt&quot;) len(train_samples), len(val_samples), len(test_samples) . CPU times: user 520 ms, sys: 123 ms, total: 643 ms Wall time: 648 ms . As we are experimenting Some Text Preprocessing are left (like url and special char removal) , we&#39;ll do it future and see acuuracy deference. . train_samples[:10] . [{&#39;line_number&#39;: 0, &#39;target&#39;: &#39;OBJECTIVE&#39;, &#39;text&#39;: &#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 1, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 2, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 3, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;pain was assessed using the visual analog pain scale ( @-@ mm ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 4, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 5, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 6, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 7, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 8, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 9, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;these differences remained significant at @ weeks .&#39;, &#39;total_lines&#39;: 11}] . import pandas as pd train_df = pd.DataFrame(train_samples) val_df = pd.DataFrame(val_samples) test_df = pd.DataFrame(test_samples) train_df.head(14) . target text line_number total_lines . 0 OBJECTIVE | to investigate the efficacy of @ weeks of dail... | 0 | 11 | . 1 METHODS | a total of @ patients with primary knee oa wer... | 1 | 11 | . 2 METHODS | outcome measures included pain reduction and i... | 2 | 11 | . 3 METHODS | pain was assessed using the visual analog pain... | 3 | 11 | . 4 METHODS | secondary outcome measures included the wester... | 4 | 11 | . 5 METHODS | serum levels of interleukin @ ( il-@ ) , il-@ ... | 5 | 11 | . 6 RESULTS | there was a clinically relevant reduction in t... | 6 | 11 | . 7 RESULTS | the mean difference between treatment arms ( @... | 7 | 11 | . 8 RESULTS | further , there was a clinically relevant redu... | 8 | 11 | . 9 RESULTS | these differences remained significant at @ we... | 9 | 11 | . 10 RESULTS | the outcome measures in rheumatology clinical ... | 10 | 11 | . 11 CONCLUSIONS | low-dose oral prednisolone had both a short-te... | 11 | 11 | . 12 BACKGROUND | emotional eating is associated with overeating... | 0 | 10 | . 13 BACKGROUND | yet , empirical evidence for individual ( trai... | 1 | 10 | . train_df[&quot;target&quot;].value_counts() . METHODS 59353 RESULTS 57953 CONCLUSIONS 27168 BACKGROUND 21727 OBJECTIVE 13839 Name: target, dtype: int64 . train_df[&quot;target&quot;].value_counts().plot(kind = &#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefbe5c6d10&gt; . train_df.total_lines.plot(kind= &quot;hist&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefb31f38d0&gt; . train_sentences = train_df[&quot;text&quot;].tolist() val_sentences = val_df[&quot;text&quot;].tolist() test_sentences = test_df[&quot;text&quot;].tolist() len(train_sentences), len(val_sentences), len(test_sentences) . (180040, 30212, 30135) . train_sentences[:10] . [&#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .&#39;, &#39;outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .&#39;, &#39;pain was assessed using the visual analog pain scale ( @-@ mm ) .&#39;, &#39;secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .&#39;, &#39;serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .&#39;, &#39;there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .&#39;, &#39;the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .&#39;, &#39;further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .&#39;, &#39;these differences remained significant at @ weeks .&#39;] . One Hot Encoder . from sklearn.preprocessing import OneHotEncoder one_hot_encoder = OneHotEncoder(sparse=False) train_labels_one_hot = one_hot_encoder.fit_transform(train_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) val_labels_one_hot = one_hot_encoder.transform(val_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) test_labels_one_hot = one_hot_encoder.transform(test_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) # Check what training labels look like train_labels_one_hot . array([[0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], ..., [0., 0., 0., 0., 1.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.]]) . Label encode labels . from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() train_labels_encoded = label_encoder.fit_transform(train_df[&quot;target&quot;].to_numpy()) val_labels_encoded = label_encoder.transform(val_df[&quot;target&quot;].to_numpy()) test_labels_encoded = label_encoder.transform(test_df[&quot;target&quot;].to_numpy()) # Check what training labels look like train_labels_encoded . array([3, 2, 2, ..., 4, 1, 1]) . num_classes = len(label_encoder.classes_) class_names = label_encoder.classes_ num_classes, class_names . (5, array([&#39;BACKGROUND&#39;, &#39;CONCLUSIONS&#39;, &#39;METHODS&#39;, &#39;OBJECTIVE&#39;, &#39;RESULTS&#39;], dtype=object)) . Model 0: Getting a baseline . Our first model we&#39;ll be a TF-IDF Multinomial Naive Bayes as recommended by Scikit-Learn&#39;s machine learning map. . we&#39;ll create a Scikit-Learn Pipeline which uses the TfidfVectorizer class to convert our abstract sentences to numbers using the TF-IDF (term frequency-inverse document frequecy) algorithm and then learns to classify our sentences using the MultinomialNB aglorithm. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Create a pipeline model_0 = Pipeline([ (&quot;tf-idf&quot;, TfidfVectorizer()), (&quot;clf&quot;, MultinomialNB()) ]) # Fit the pipeline to the training data model_0.fit(X=train_sentences, y=train_labels_encoded); . # Evaluate baseline on validation dataset model_0.score(X=val_sentences, y=val_labels_encoded) . 0.7218323844829869 . baseline_preds = model_0.predict(val_sentences) baseline_preds . array([4, 1, 3, ..., 4, 4, 1]) . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2021-06-16 16:53:10-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2021-06-16 16:53:10 (79.1 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import calculate_results . Model 0 Results . # Calculate baseline results baseline_results = calculate_results(y_true=val_labels_encoded, y_pred=baseline_preds) baseline_results . {&#39;accuracy&#39;: 72.1832384482987, &#39;f1&#39;: 0.6989250353450294, &#39;precision&#39;: 0.7186466952323352, &#39;recall&#39;: 0.7218323844829869} . Prepare Data For Deep Neural Network Models . When our model goes through our sentences, it works best when they&#39;re all the same length (this is important for creating batches of the same size tensors) . Finding the average sentence length in the Dataset. | . import numpy as np import tensorflow as tf from tensorflow.keras import layers . sen_len = [len(sentences.split()) for sentences in train_sentences] avg_sen_len = np.mean(sen_len) avg_sen_len . 26.338269273494777 . import matplotlib.pyplot as plt plt.hist(sen_len,bins=21) # Checking the Sequence Length Distribution and getting most occurance sequence length . (array([4.2075e+04, 7.9624e+04, 3.8291e+04, 1.2725e+04, 4.3900e+03, 1.6450e+03, 7.2600e+02, 2.8900e+02, 1.3600e+02, 5.5000e+01, 2.9000e+01, 1.5000e+01, 1.1000e+01, 9.0000e+00, 8.0000e+00, 5.0000e+00, 2.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00]), array([ 1. , 15.04761905, 29.0952381 , 43.14285714, 57.19047619, 71.23809524, 85.28571429, 99.33333333, 113.38095238, 127.42857143, 141.47619048, 155.52380952, 169.57142857, 183.61904762, 197.66666667, 211.71428571, 225.76190476, 239.80952381, 253.85714286, 267.9047619 , 281.95238095, 296. ]), &lt;a list of 21 Patch objects&gt;) . Looks like the vast majority of sentences are between 0 and 50 tokens in length. . We can use NumPy&#39;s percentile to find the value which covers 95% of the sentence lengthsHow long of a sentesnces cover majority of the data ? (95%) . np.percentile(sen_len,95) . 55.0 . max(sen_len) # max length sentence in training set . 296 . Creaating a text vectorizer layer . Create text vectorize . Section 3.2 of the PubMed 200k RCT paper states the vocabulary size of the PubMed 20k dataset as 68,000. So we&#39;ll use that as our max_tokens parameter. . max_tokens = 68000 . from tensorflow.keras.layers.experimental.preprocessing import TextVectorization text_vectorizer = TextVectorization(max_tokens=max_tokens,standardize=&#39;lower_and_strip_punctuation&#39;, output_sequence_length=55) . text_vectorizer.adapt(train_sentences) . import random target_sentence = random.choice(train_sentences) print(f&quot;Text: n{target_sentence}&quot;) print(f&quot; nLength of text: {len(target_sentence.split())}&quot;) print(f&quot; nVectorized text: n{text_vectorizer([target_sentence])}&quot;) . Text: after @ months of stable-dose treatment , patients receiving milnacipran @ and/or @ mg/d had significant improvement in mfi total and subscale scores ( p &lt; @ vs placebo ) . Length of text: 31 Vectorized text: [[ 21 41 4 34172 19 12 245 12540 727 1306 55 37 194 5 18393 76 3 2072 119 14 44 48 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] . rct_20k_text_vocab = text_vectorizer.get_vocabulary() most_common = rct_20k_text_vocab[:5] least_common = rct_20k_text_vocab[-5:] print(f&quot;Number of words in vocabulary: {len(rct_20k_text_vocab)}&quot;), print(f&quot;Most common words in the vocabulary: {most_common}&quot;) print(f&quot;Least common words in the vocabulary: {least_common}&quot;) . Number of words in vocabulary: 64841 Most common words in the vocabulary: [&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;and&#39;, &#39;of&#39;] Least common words in the vocabulary: [&#39;aainduced&#39;, &#39;aaigroup&#39;, &#39;aachener&#39;, &#39;aachen&#39;, &#39;aaacp&#39;] . text_vectorizer.get_config() . {&#39;dtype&#39;: &#39;string&#39;, &#39;max_tokens&#39;: 68000, &#39;name&#39;: &#39;text_vectorization&#39;, &#39;ngrams&#39;: None, &#39;output_mode&#39;: &#39;int&#39;, &#39;output_sequence_length&#39;: 55, &#39;pad_to_max_tokens&#39;: False, &#39;split&#39;: &#39;whitespace&#39;, &#39;standardize&#39;: &#39;lower_and_strip_punctuation&#39;, &#39;trainable&#39;: True, &#39;vocabulary_size&#39;: 64841} . Create custom text embedding . To create a richer numerical representation of our text, we can use an embedding. . The input_dim parameter defines the size of our vocabulary. And the output_dim parameter defines the dimension of the embedding output. . Once created, our embedding layer will take the integer outputs of our text_vectorization layer as inputs and convert them to feature vectors of size output_dim. . token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), output_dim= 128, mask_zero=True, input_length=55) print(f&quot;Sentence before Vectorization : n{target_sentence} n&quot;) vec_sentence = text_vectorizer([target_sentence]) print(f&quot;Sentence After vectorization : n {vec_sentence} n&quot;) embed_sentence = token_embed(vec_sentence) print(f&quot;Embedding Sentence : n{embed_sentence} n&quot;) . Sentence before Vectorization : after @ months of stable-dose treatment , patients receiving milnacipran @ and/or @ mg/d had significant improvement in mfi total and subscale scores ( p &lt; @ vs placebo ) . Sentence After vectorization : [[ 21 41 4 34172 19 12 245 12540 727 1306 55 37 194 5 18393 76 3 2072 119 14 44 48 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] Embedding Sentence : [[[ 0.01845442 0.00490598 0.02234909 ... -0.0251475 0.02369246 -0.03116907] [ 0.03540225 -0.01559786 -0.00104294 ... -0.0325914 -0.01999564 -0.03067011] [ 0.01375339 -0.0441448 -0.03379129 ... -0.04563415 -0.04586704 0.03887606] ... [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449] [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449] [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449]]] . train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)) valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)) test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot)) . len(train_dataset) , train_dataset . (180040, &lt;TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)&gt;) . train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE) valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE) test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE) train_dataset . &lt;PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)&gt; . Model 1: Conv1D with token embedding . All of our deep models will follow a similar structure: . Input (text) -&gt; Tokenize -&gt; Embedding -&gt; Layers -&gt; Output (label probability) . inputs = layers.Input(shape = (1,),dtype = tf.string) text_vector = text_vectorizer(inputs) embed = token_embed(text_vector) x = layers.Conv1D(filters = 64, kernel_size= 5, padding=&quot;same&quot;,activation=&quot;relu&quot;,kernel_regularizer=tf.keras.regularizers.L2(0.01))(embed) x = layers.GlobalMaxPool1D()(x) x = layers.Dropout(0.2)(x) outputs = layers.Dense(num_classes,activation=&quot;softmax&quot;)(x) model = tf.keras.Model(inputs,outputs) model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 1)] 0 _________________________________________________________________ text_vectorization (TextVect (None, 55) 0 _________________________________________________________________ embedding (Embedding) (None, 55, 128) 8299648 _________________________________________________________________ conv1d (Conv1D) (None, 55, 64) 41024 _________________________________________________________________ global_max_pooling1d (Global (None, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense (Dense) (None, 5) 325 ================================================================= Total params: 8,340,997 Trainable params: 8,340,997 Non-trainable params: 0 _________________________________________________________________ . len(train_dataset) . 5627 . model.compile(optimizer=&#39;Adam&#39;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model_1_history = model.fit(train_dataset, steps_per_epoch=int(0.1 * len(train_dataset)), epochs = 10, validation_data = valid_dataset, validation_steps=int(0.1 * len(valid_dataset)),) . Epoch 1/10 562/562 [==============================] - 58s 101ms/step - loss: 1.1068 - accuracy: 0.6201 - val_loss: 0.7933 - val_accuracy: 0.7487 Epoch 2/10 562/562 [==============================] - 56s 100ms/step - loss: 0.7738 - accuracy: 0.7517 - val_loss: 0.7109 - val_accuracy: 0.7736 Epoch 3/10 562/562 [==============================] - 56s 100ms/step - loss: 0.7241 - accuracy: 0.7625 - val_loss: 0.6671 - val_accuracy: 0.7872 Epoch 4/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6906 - accuracy: 0.7751 - val_loss: 0.6546 - val_accuracy: 0.7846 Epoch 5/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6888 - accuracy: 0.7782 - val_loss: 0.6390 - val_accuracy: 0.7902 Epoch 6/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6796 - accuracy: 0.7763 - val_loss: 0.6246 - val_accuracy: 0.7949 Epoch 7/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6509 - accuracy: 0.7854 - val_loss: 0.6183 - val_accuracy: 0.7999 Epoch 8/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6389 - accuracy: 0.7971 - val_loss: 0.6009 - val_accuracy: 0.8122 Epoch 9/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6379 - accuracy: 0.7930 - val_loss: 0.6430 - val_accuracy: 0.7902 Epoch 10/10 562/562 [==============================] - 56s 99ms/step - loss: 0.6402 - accuracy: 0.7942 - val_loss: 0.6062 - val_accuracy: 0.8039 . model.evaluate(valid_dataset) . 945/945 [==============================] - 5s 5ms/step - loss: 0.5962 - accuracy: 0.8083 . [0.5962496995925903, 0.8082880973815918] . model_1_pred_probs = model.predict(valid_dataset) model_1_pred_probs . array([[6.4009082e-01, 1.5787475e-01, 1.1690257e-02, 1.7012680e-01, 2.0217434e-02], [2.1156156e-01, 7.0904827e-01, 1.1327777e-03, 7.0969328e-02, 7.2881221e-03], [1.7376225e-01, 8.4552346e-03, 1.4700086e-03, 8.1595337e-01, 3.5914412e-04], ..., [2.6505839e-04, 9.3591970e-04, 1.3166884e-02, 1.3237984e-04, 9.8549968e-01], [2.4481107e-02, 5.5618656e-01, 4.2747390e-02, 1.6767828e-02, 3.5981715e-01], [9.1729127e-02, 8.6072636e-01, 1.3366650e-02, 1.7221529e-02, 1.6956387e-02]], dtype=float32) . model_1_preds = tf.argmax(model_1_pred_probs, axis=1) model_1_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 1, 1])&gt; . Model 1 Results . model_1_results = calculate_results(y_true=val_labels_encoded, y_pred=model_1_preds) model_1_results . {&#39;accuracy&#39;: 80.8288097444724, &#39;f1&#39;: 0.8054731032957875, &#39;precision&#39;: 0.8047795569207609, &#39;recall&#39;: 0.808288097444724} . Model 2: Feature extraction with pretrained token embedding (USE) . Here We use Universal Sentence Encoder here from TF-HUB. . Since we&#39;re moving towards replicating the model architecture in Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it mentions they used a pretrained GloVe embedding as a way to initialise their token embeddings. . The model structure will look like: . Inputs (string) -&gt; Pretrained embeddings from TensorFlow Hub (Universal Sentence Encoder) -&gt; Layers -&gt; Output (prediction probabilities) . import tensorflow_hub as hub tf_hub_embedding_layer = hub.KerasLayer(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;, trainable=False, name=&quot;universal_sentence_encoder&quot;) . Beautiful, now our pretrained USE is downloaded and instantiated as a hub.KerasLayer instance, let&#39;s test it out on a random sentence . random_training_sentence = random.choice(train_sentences) print(f&quot;Random training sentence: n{random_training_sentence} n&quot;) use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence]) print(f&quot;Sentence after embedding: n{use_embedded_sentence[0][:30]} (truncated output)... n&quot;) print(f&quot;Length of sentence embedding: n{len(use_embedded_sentence[0])}&quot;) . Random training sentence: the subjects in the test group practiced cthe , while those in the control group did `` the @th radio calisthenics &#39;&#39; , an official recommended calisthenics for promoting healthcare in china , @ times a week , and @ weeks practicing overall . Sentence after embedding: [-0.04633829 -0.03948136 0.02896223 -0.07942989 -0.01642509 0.03321299 0.04372008 -0.01696645 -0.03029537 0.0055867 0.0817182 0.0511634 0.02764668 -0.04249508 0.02601314 0.02671395 -0.08149037 0.02041833 -0.05118315 -0.02549118 -0.02749129 -0.02932397 -0.07956979 -0.02773482 -0.03448042 0.03734251 -0.066794 0.01424676 -0.06487625 -0.05911339] (truncated output)... Length of sentence embedding: 512 . inputs = layers.Input(shape=[], dtype=tf.string) pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding x = layers.Dense(128, activation=&quot;relu&quot;)(pretrained_embedding) # add a fully connected layer on top of the embedding # x = layers.Dropout(0.2)(x) outputs = layers.Dense(5, activation=&quot;softmax&quot;,kernel_regularizer=None)(x) # create the output layer model_2 = tf.keras.Model(inputs=inputs, outputs=outputs) # Compile the model model_2.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) . model_2.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None,)] 0 _________________________________________________________________ universal_sentence_encoder ( (None, 512) 256797824 _________________________________________________________________ dense_1 (Dense) (None, 128) 65664 _________________________________________________________________ dense_2 (Dense) (None, 5) 645 ================================================================= Total params: 256,864,133 Trainable params: 66,309 Non-trainable params: 256,797,824 _________________________________________________________________ . model_2_history = model_2.fit(train_dataset, steps_per_epoch=int(0.1 * len(train_dataset)), epochs = 10, validation_data = valid_dataset, validation_steps=int(0.1 * len(valid_dataset))) . Epoch 1/10 562/562 [==============================] - 9s 12ms/step - loss: 0.9225 - accuracy: 0.6502 - val_loss: 0.7968 - val_accuracy: 0.6908 Epoch 2/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7692 - accuracy: 0.7017 - val_loss: 0.7549 - val_accuracy: 0.7071 Epoch 3/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7528 - accuracy: 0.7127 - val_loss: 0.7393 - val_accuracy: 0.7128 Epoch 4/10 562/562 [==============================] - 7s 12ms/step - loss: 0.7198 - accuracy: 0.7244 - val_loss: 0.7123 - val_accuracy: 0.7287 Epoch 5/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7267 - accuracy: 0.7219 - val_loss: 0.6904 - val_accuracy: 0.7317 Epoch 6/10 562/562 [==============================] - 6s 12ms/step - loss: 0.7185 - accuracy: 0.7254 - val_loss: 0.6832 - val_accuracy: 0.7350 Epoch 7/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6862 - accuracy: 0.7390 - val_loss: 0.6673 - val_accuracy: 0.7463 Epoch 8/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6764 - accuracy: 0.7435 - val_loss: 0.6558 - val_accuracy: 0.7483 Epoch 9/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6742 - accuracy: 0.7430 - val_loss: 0.6571 - val_accuracy: 0.7480 Epoch 10/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6690 - accuracy: 0.7479 - val_loss: 0.6518 - val_accuracy: 0.7563 . model_2.evaluate(valid_dataset) . 945/945 [==============================] - 9s 9ms/step - loss: 0.6559 - accuracy: 0.7511 . [0.6559162139892578, 0.7510591745376587] . model_2_pred_probs = model_2.predict(valid_dataset) model_2_pred_probs . array([[4.20623630e-01, 4.38490957e-01, 6.25136832e-04, 1.32154837e-01, 8.10540374e-03], [3.23061615e-01, 5.82224965e-01, 1.88950659e-03, 9.16711688e-02, 1.15277513e-03], [4.91145968e-01, 4.19516787e-02, 1.88930240e-02, 4.06143695e-01, 4.18655388e-02], ..., [7.41388998e-04, 7.53519998e-04, 2.20944248e-02, 2.34126986e-04, 9.76176500e-01], [3.42314062e-03, 6.01526648e-02, 1.98868886e-01, 8.81478831e-04, 7.36673832e-01], [4.39007245e-02, 8.41824710e-01, 1.04148194e-01, 9.37066041e-04, 9.18926578e-03]], dtype=float32) . model_2_preds = tf.argmax(model_2_pred_probs, axis=1) model_2_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 0, ..., 4, 4, 1])&gt; . Model 2 Results . model_2_results = calculate_results(y_true=val_labels_encoded, y_pred=model_2_preds) model_2_results . {&#39;accuracy&#39;: 75.10591817820733, &#39;f1&#39;: 0.746383502412015, &#39;precision&#39;: 0.7457372543188239, &#39;recall&#39;: 0.7510591817820733} . Model 3: Conv1D with character embedding . Creating a character-level tokenizer . The Neural Networks for Joint Sentence Classification in Medical Paper Abstracts paper mentions their model uses a hybrid of token and character embeddings. . The difference between a character and token embedding is that the character embedding is created using sequences split into characters (e.g. hello -&gt; [h, e, l, l, o]) where as a token embedding is created on sequences split into tokens. . Token level embeddings split sequences into tokens (words) and embeddings each of them, character embeddings split sequences into characters and creates a feature vector for each. . Before we can vectorize our sequences on a character-level we&#39;ll need to split them into characters. Let&#39;s write a function to do so . &quot; &quot;.join(list(train_sentences[0])) . &#39;t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) .&#39; . def split_chars(text): return &quot; &quot;.join(list(text)) split_chars(random_training_sentence) . &#34;t h e s u b j e c t s i n t h e t e s t g r o u p p r a c t i c e d c t h e , w h i l e t h o s e i n t h e c o n t r o l g r o u p d i d ` ` t h e @ t h r a d i o c a l i s t h e n i c s &#39; &#39; , a n o f f i c i a l r e c o m m e n d e d c a l i s t h e n i c s f o r p r o m o t i n g h e a l t h c a r e i n c h i n a , @ t i m e s a w e e k , a n d @ w e e k s p r a c t i c i n g o v e r a l l .&#34; . train_chars = [split_chars(sentence) for sentence in train_sentences] val_chars = [split_chars(sentence) for sentence in val_sentences] test_chars = [split_chars(sentence) for sentence in test_sentences] print(train_chars[0]) . t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) . . train_chars[:5] . [&#39;t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) .&#39;, &#39;a t o t a l o f @ p a t i e n t s w i t h p r i m a r y k n e e o a w e r e r a n d o m i z e d @ : @ ; @ r e c e i v e d @ m g / d a y o f p r e d n i s o l o n e a n d @ r e c e i v e d p l a c e b o f o r @ w e e k s .&#39;, &#39;o u t c o m e m e a s u r e s i n c l u d e d p a i n r e d u c t i o n a n d i m p r o v e m e n t i n f u n c t i o n s c o r e s a n d s y s t e m i c i n f l a m m a t i o n m a r k e r s .&#39;, &#39;p a i n w a s a s s e s s e d u s i n g t h e v i s u a l a n a l o g p a i n s c a l e ( @ - @ m m ) .&#39;, &#39;s e c o n d a r y o u t c o m e m e a s u r e s i n c l u d e d t h e w e s t e r n o n t a r i o a n d m c m a s t e r u n i v e r s i t i e s o s t e o a r t h r i t i s i n d e x s c o r e s , p a t i e n t g l o b a l a s s e s s m e n t ( p g a ) o f t h e s e v e r i t y o f k n e e o a , a n d @ - m i n w a l k d i s t a n c e ( @ m w d ) .&#39;] . char_lens = [len(sentence) for sentence in train_sentences] avg_char_lens = sum(char_lens)/len(char_lens) avg_char_lens . 149.3662574983337 . import matplotlib.pyplot as plt plt.hist(char_lens,bins =25) . (array([1.2341e+04, 5.0061e+04, 5.6508e+04, 3.4345e+04, 1.5779e+04, 6.5770e+03, 2.4690e+03, 1.0890e+03, 4.4400e+02, 1.9900e+02, 1.0500e+02, 5.0000e+01, 2.4000e+01, 1.9000e+01, 1.1000e+01, 7.0000e+00, 4.0000e+00, 3.0000e+00, 0.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]), array([1.0000e+00, 5.6400e+01, 1.1180e+02, 1.6720e+02, 2.2260e+02, 2.7800e+02, 3.3340e+02, 3.8880e+02, 4.4420e+02, 4.9960e+02, 5.5500e+02, 6.1040e+02, 6.6580e+02, 7.2120e+02, 7.7660e+02, 8.3200e+02, 8.8740e+02, 9.4280e+02, 9.9820e+02, 1.0536e+03, 1.1090e+03, 1.1644e+03, 1.2198e+03, 1.2752e+03, 1.3306e+03, 1.3860e+03]), &lt;a list of 25 Patch objects&gt;) . Okay, looks like most of our sequences are between 0 and 200 characters long. . Let&#39;s use NumPy&#39;s percentile to figure out what length covers 95% of our sequences . output_seq_char_len = int(np.percentile(char_lens, 95)) output_seq_char_len . 290 . random.choice(train_sentences) . &#39;in patients with her@-positive metastatic breast cancer , the addition of pertuzumab to trastuzumab and docetaxel , as compared with the addition of placebo , significantly improved the median overall survival to @ months and extended the results of previous analyses showing the efficacy of this drug combination .&#39; . import string alphabet = string.ascii_lowercase + string.digits + string.punctuation alphabet . &#39;abcdefghijklmnopqrstuvwxyz0123456789!&#34;#$%&amp; &#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~&#39; . NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS, output_sequence_length=output_seq_char_len, standardize=&quot;lower_and_strip_punctuation&quot;, name=&quot;char_vectorizer&quot;) # Adapt character vectorizer to training characters char_vectorizer.adapt(train_chars) . char_vocab = char_vectorizer.get_vocabulary() print(f&quot;Number of different characters in character vocab: {len(char_vocab)}&quot;) print(f&quot;5 most common characters: {char_vocab[:5]}&quot;) print(f&quot;5 least common characters: {char_vocab[-5:]}&quot;) . Number of different characters in character vocab: 28 5 most common characters: [&#39;&#39;, &#39;[UNK]&#39;, &#39;e&#39;, &#39;t&#39;, &#39;i&#39;] 5 least common characters: [&#39;k&#39;, &#39;x&#39;, &#39;z&#39;, &#39;q&#39;, &#39;j&#39;] . random_train_chars = random.choice(train_chars) print(f&quot;Charified text: n{random_train_chars}&quot;) print(f&quot; nLength of chars: {len(random_train_chars.split())}&quot;) vectorized_chars = char_vectorizer([random_train_chars]) print(f&quot; nVectorized chars: n{vectorized_chars}&quot;) print(f&quot; nLength of vectorized chars: {len(vectorized_chars[0])}&quot;) . Charified text: a c h e s t x - r a y w a s t a k e n a t p o s t o p e r a t i v e d a y @ , a n d t h e r e s i d u a l i n t r a a b d o m i n a l g a s v o l u m e w a s m e a s u r e d . Length of chars: 88 Vectorized chars: [[ 5 11 13 2 9 3 24 8 5 19 20 5 9 3 5 23 2 6 5 3 14 7 9 3 7 14 2 8 5 3 4 21 2 10 5 19 5 6 10 3 13 2 8 2 9 4 10 16 5 12 4 6 3 8 5 5 22 10 7 15 4 6 5 12 18 5 9 21 7 12 16 15 2 20 5 9 15 2 5 9 16 8 2 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] Length of vectorized chars: 290 . Creating a character-level embedding . We&#39;ve got a way to vectorize our character-level sequences, now&#39;s time to create a character-level embedding. . The input dimension (input_dim) will be equal to the number of different characters in our char_vocab (28). And since we&#39;re following the structure of the model in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, the output dimension of the character embedding (output_dim) will be 25. . char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, output_dim= 25, mask_zero= True, name= &quot;char_embed&quot;) # Test out character embedding layer print(f&quot;Charified text (before vectorization and embedding): n{random_train_chars} n&quot;) char_embed_example = char_embed(char_vectorizer([random_train_chars])) print(f&quot;Embedded chars (after vectorization and embedding): n{char_embed_example} n&quot;) print(f&quot;Character embedding shape: {char_embed_example.shape}&quot;) . Charified text (before vectorization and embedding): a c h e s t x - r a y w a s t a k e n a t p o s t o p e r a t i v e d a y @ , a n d t h e r e s i d u a l i n t r a a b d o m i n a l g a s v o l u m e w a s m e a s u r e d . Embedded chars (after vectorization and embedding): [[[ 0.01198739 0.01475773 0.00853588 ... 0.04154284 -0.00139939 -0.04978269] [ 0.03823442 0.02977749 -0.03549073 ... -0.03217635 -0.00892054 0.03190038] [ 0.00683529 0.01736682 0.00283471 ... -0.00423067 -0.00172087 0.01615829] ... [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704] [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704] [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704]]] Character embedding shape: (1, 290, 25) . Before fitting our model on the data, we&#39;ll create char-level batched PrefetchedDataset&#39;s. . train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE) val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE) train_char_dataset . &lt;PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)&gt; . Building a Conv1D model to fit on character embeddings . Now we&#39;ve got a way to turn our character-level sequences into numbers (char_vectorizer) as well as numerically represent them as an embedding (char_embed) let&#39;s test how effective they are at encoding the information in our sequences by creating a character-level sequence model. . The model will have the same structure as our custom token embedding model (model_1) except it&#39;ll take character-level sequences as input instead of token-level sequences. . Input (character-level text) -&gt; Tokenize -&gt; Embedding -&gt; Layers (Conv1D, GlobalMaxPool1D) -&gt; Output (label probability) . inputs = layers.Input(shape=(1,), dtype=&quot;string&quot;) char_vectors = char_vectorizer(inputs) char_embeddings = char_embed(char_vectors) x = layers.Conv1D(64, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;,kernel_regularizer=tf.keras.regularizers.L2(0.01))(char_embeddings) x = layers.GlobalMaxPool1D()(x) outputs = layers.Dense(num_classes, activation=&quot;softmax&quot;)(x) model_3 = tf.keras.Model(inputs=inputs, outputs=outputs, name=&quot;model_3_conv1D_char_embedding&quot;) # Compile model model_3.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) model_3.summary() . Model: &#34;model_3_conv1D_char_embedding&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 1)] 0 _________________________________________________________________ char_vectorizer (TextVectori (None, 290) 0 _________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 _________________________________________________________________ conv1d_1 (Conv1D) (None, 290, 64) 8064 _________________________________________________________________ global_max_pooling1d_1 (Glob (None, 64) 0 _________________________________________________________________ dense_3 (Dense) (None, 5) 325 ================================================================= Total params: 10,139 Trainable params: 10,139 Non-trainable params: 0 _________________________________________________________________ . model_3_history = model_3.fit(train_char_dataset, steps_per_epoch=int(0.1 * len(train_char_dataset)), epochs=10, validation_data=val_char_dataset, validation_steps=int(0.1 * len(val_char_dataset))) . Epoch 1/10 562/562 [==============================] - 14s 23ms/step - loss: 1.4009 - accuracy: 0.4576 - val_loss: 1.2213 - val_accuracy: 0.5432 Epoch 2/10 562/562 [==============================] - 13s 23ms/step - loss: 1.1807 - accuracy: 0.5530 - val_loss: 1.1203 - val_accuracy: 0.6001 Epoch 3/10 562/562 [==============================] - 13s 23ms/step - loss: 1.1121 - accuracy: 0.5955 - val_loss: 1.0601 - val_accuracy: 0.6303 Epoch 4/10 562/562 [==============================] - 13s 23ms/step - loss: 1.0576 - accuracy: 0.6207 - val_loss: 1.0272 - val_accuracy: 0.6400 Epoch 5/10 562/562 [==============================] - 13s 23ms/step - loss: 1.0327 - accuracy: 0.6365 - val_loss: 1.0117 - val_accuracy: 0.6592 Epoch 6/10 562/562 [==============================] - 13s 24ms/step - loss: 1.0137 - accuracy: 0.6437 - val_loss: 0.9768 - val_accuracy: 0.6576 Epoch 7/10 562/562 [==============================] - 13s 24ms/step - loss: 0.9968 - accuracy: 0.6497 - val_loss: 0.9487 - val_accuracy: 0.6729 Epoch 8/10 562/562 [==============================] - 13s 24ms/step - loss: 0.9578 - accuracy: 0.6701 - val_loss: 0.9473 - val_accuracy: 0.6712 Epoch 9/10 562/562 [==============================] - 13s 23ms/step - loss: 0.9539 - accuracy: 0.6697 - val_loss: 0.9498 - val_accuracy: 0.6659 Epoch 10/10 562/562 [==============================] - 13s 23ms/step - loss: 0.9476 - accuracy: 0.6668 - val_loss: 0.9276 - val_accuracy: 0.6689 . model_3.evaluate(val_char_dataset) . 945/945 [==============================] - 6s 7ms/step - loss: 0.9418 - accuracy: 0.6693 . [0.941790759563446, 0.6693366765975952] . model_3_pred_probs = model_3.predict(val_char_dataset) model_3_pred_probs . array([[0.21880805, 0.47745466, 0.06887063, 0.17308971, 0.06177694], [0.23530294, 0.43719485, 0.00282957, 0.3047214 , 0.01995131], [0.15760659, 0.33806348, 0.04908546, 0.3893838 , 0.06586069], ..., [0.01113645, 0.02624117, 0.0150859 , 0.00669696, 0.9408395 ], [0.01904003, 0.16237463, 0.07217802, 0.01535511, 0.7310522 ], [0.17195038, 0.6881715 , 0.10410943, 0.01864745, 0.01712121]], dtype=float32) . model_3_preds = tf.argmax(model_3_pred_probs, axis=1) model_3_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 3, ..., 4, 4, 1])&gt; . Model 3 Results . model_3_results = calculate_results(y_true=val_labels_encoded, y_pred=model_3_preds) model_3_results . {&#39;accuracy&#39;: 66.93366874089766, &#39;f1&#39;: 0.6597343453210113, &#39;precision&#39;: 0.6638601787800059, &#39;recall&#39;: 0.6693366874089766} . Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer) . In moving closer to build a model similar to the one in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it&#39;s time we tackled the hybrid token embedding layer they speak of. . . This hybrid token embedding layer is a combination of token embeddings and character embeddings. In other words, they create a stacked embedding to represent sequences before passing them to the sequence label prediction layer | . To start replicating (or getting close to replicating) the model in Figure 1, we&#39;re going to go through the following steps: . Create a token-level model (similar to model_1) | Create a character-level model (similar to model_3 with a slight modification to reflect the paper) | Combine (using layers.Concatenate) the outputs of 1 and 2 | Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts | Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output | 1 # Token_level Model (using Pretrained -- Universal Sentence Encoder) token_inputs = layers.Input(shape = [], dtype= tf.string, name = &quot;token_input&quot;) token_embedding = tf_hub_embedding_layer(token_inputs) token_dense = layers.Dense(128,activation=&quot;relu&quot;)(token_embedding) token_model = tf.keras.Model(inputs = token_inputs, outputs = token_dense) 2 # char_level Model char_inputs = layers.Input(shape=(1,), dtype= tf.string, name=&quot;char_input&quot;) char_vectors = char_vectorizer(char_inputs) char_embedding = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(25,activation=&quot;relu&quot;))(char_embedding) char_model = tf.keras.Model(inputs= char_inputs, # char_dense = layers.Dense(128,activation=&quot;relu&quot;)(char_bilstm) outputs =char_bi_lstm) 3 # Now Concatenate token_model and char_model concat_layer = layers.Concatenate(name = &quot;token_char_hybrid&quot;)([token_model.output, char_model.output]) 4 # Add Some Layer on top of concat_layer concat_dropout = layers.Dropout(0.5)(concat_layer) concat_dense = layers.Dense(256,activation=&quot;relu&quot;)(concat_dropout) final_dropout = layers.Dropout(0.2)(concat_dense) output_layer = layers.Dense(num_classes,activation=&quot;softmax&quot;)(final_dropout) model_4 = tf.keras.Model(inputs = [token_model.input, char_model.input], outputs = output_layer, name=&quot;model_4_token_and_char_embeddings&quot;) . model_4.summary() . Model: &#34;model_4_token_and_char_embeddings&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== char_input (InputLayer) [(None, 1)] 0 __________________________________________________________________________________________________ token_input (InputLayer) [(None,)] 0 __________________________________________________________________________________________________ char_vectorizer (TextVectorizat (None, 290) 0 char_input[0][0] __________________________________________________________________________________________________ universal_sentence_encoder (Ker (None, 512) 256797824 token_input[0][0] __________________________________________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 char_vectorizer[1][0] __________________________________________________________________________________________________ dense_4 (Dense) (None, 128) 65664 universal_sentence_encoder[1][0] __________________________________________________________________________________________________ bidirectional (Bidirectional) (None, 50) 10200 char_embed[1][0] __________________________________________________________________________________________________ token_char_hybrid (Concatenate) (None, 178) 0 dense_4[0][0] bidirectional[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 178) 0 token_char_hybrid[0][0] __________________________________________________________________________________________________ dense_5 (Dense) (None, 256) 45824 dropout_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 256) 0 dense_5[0][0] __________________________________________________________________________________________________ dense_6 (Dense) (None, 5) 1285 dropout_2[0][0] ================================================================================================== Total params: 256,922,547 Trainable params: 124,723 Non-trainable params: 256,797,824 __________________________________________________________________________________________________ . Visualize the Hybrid Model . tf.keras.utils.plot_model( model_4, to_file=&#39;model.png&#39;, show_shapes=False, show_dtype=False, show_layer_names=True, rankdir=&#39;TB&#39;, expand_nested=False, dpi=96 ) . model_4.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we&#39;ll stick with Adam metrics=[&quot;accuracy&quot;]) . train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels # Prefetch and batch train data train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Repeat same steps validation data val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars)) val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels)) val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) . train_char_token_dataset, val_char_token_dataset . (&lt;PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)&gt;, &lt;PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)&gt;) . model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters steps_per_epoch=int(0.1 * len(train_char_token_dataset)), epochs=10, validation_data=val_char_token_dataset, validation_steps=int(0.1 * len(val_char_token_dataset))) . Epoch 1/10 562/562 [==============================] - 167s 289ms/step - loss: 0.9316 - accuracy: 0.6286 - val_loss: 0.7693 - val_accuracy: 0.7048 Epoch 2/10 562/562 [==============================] - 162s 288ms/step - loss: 0.7773 - accuracy: 0.6966 - val_loss: 0.7133 - val_accuracy: 0.7271 Epoch 3/10 562/562 [==============================] - 163s 290ms/step - loss: 0.7541 - accuracy: 0.7097 - val_loss: 0.6872 - val_accuracy: 0.7424 Epoch 4/10 562/562 [==============================] - 163s 290ms/step - loss: 11.8752 - accuracy: 0.7228 - val_loss: 0.6679 - val_accuracy: 0.7463 Epoch 5/10 562/562 [==============================] - 163s 291ms/step - loss: 0.7405 - accuracy: 0.7171 - val_loss: 0.6611 - val_accuracy: 0.7473 Epoch 6/10 562/562 [==============================] - 164s 291ms/step - loss: 0.7305 - accuracy: 0.7208 - val_loss: 0.6454 - val_accuracy: 0.7580 Epoch 7/10 562/562 [==============================] - 164s 292ms/step - loss: 0.7063 - accuracy: 0.7299 - val_loss: 0.6443 - val_accuracy: 0.7600 Epoch 8/10 562/562 [==============================] - 164s 292ms/step - loss: 0.6984 - accuracy: 0.7340 - val_loss: 0.6335 - val_accuracy: 0.7557 Epoch 9/10 562/562 [==============================] - 162s 289ms/step - loss: 0.6996 - accuracy: 0.7311 - val_loss: 0.6434 - val_accuracy: 0.7557 Epoch 10/10 562/562 [==============================] - 162s 289ms/step - loss: 0.7018 - accuracy: 0.7309 - val_loss: 0.6321 - val_accuracy: 0.7603 . model_4.evaluate(val_char_token_dataset) . 945/945 [==============================] - 58s 61ms/step - loss: 23792656.0000 - accuracy: 0.7598 . [23792656.0, 0.7598305344581604] . model_4_pred_probs = model_4.predict(val_char_token_dataset) model_4_pred_probs . array([[4.81065422e-01, 3.40999186e-01, 1.79997389e-03, 1.66568533e-01, 9.56688821e-03], [2.74215668e-01, 6.57577336e-01, 1.48074096e-03, 6.47516549e-02, 1.97461597e-03], [4.36756134e-01, 5.47802337e-02, 5.64954877e-02, 4.24435914e-01, 2.75322516e-02], ..., [1.14926996e-04, 4.47407336e-04, 1.09152636e-02, 4.81208008e-05, 9.88474309e-01], [9.39613860e-03, 6.19878434e-02, 1.74138308e-01, 2.82176491e-03, 7.51655936e-01], [6.84660450e-02, 8.80190194e-01, 4.18375172e-02, 2.41007935e-03, 7.09615275e-03]], dtype=float32) . model_4_preds = tf.argmax(model_4_pred_probs, axis=1) model_4_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 0, ..., 4, 4, 1])&gt; . model_4_results = calculate_results(y_true=val_labels_encoded, y_pred=model_4_preds) model_4_results . {&#39;accuracy&#39;: 75.98305309148682, &#39;f1&#39;: 0.7545412454754323, &#39;precision&#39;: 0.7561886438045121, &#39;recall&#39;: 0.7598305309148683} . Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings . As it&#39;s a Sequential classification problem the sequences come in a particular order. Like OBJECTIVE comes first rather then CONCLUSION. . Abstracts typically come in a sequential order, such as: . OBJECTIVE ... | METHODS ... | METHODS ... | METHODS ... | RESULTS ... | CONCLUSIONS ... | . Or . BACKGROUND ... | OBJECTIVE ... | METHODS ... | METHODS ... | RESULTS ... | RESULTS ... | CONCLUSIONS ... | . Here we do some Feature Engineering so that our model can learn the order sentences in the Abstract and know where the sentence appear in the Abstract. The &quot;line_number&quot; and &quot;total_lines&quot; columns are features which didn&#39;t necessarily come with the training data but can be passed to our model as a positional embedding. . . Note: Positional Embedding make your model include the information about order of the input. The positional encoding step allows the model to recognize which part of the sequence an input belongs to. . But to avoid our model thinking a line with &quot;line_number&quot;=5 is five times greater than a line with &quot;line_number&quot;=1, we&#39;ll use one-hot-encoding to encode our &quot;line_number&quot; and &quot;total_lines&quot; features. . That is why we have to use one-hot encoding. We use tf.one_hot for it. . train_df[&quot;line_number&quot;].value_counts() . 0 15000 1 15000 2 15000 3 15000 4 14992 5 14949 6 14758 7 14279 8 13346 9 11981 10 10041 11 7892 12 5853 13 4152 14 2835 15 1861 16 1188 17 751 18 462 19 286 20 162 21 101 22 66 23 33 24 22 25 14 26 7 27 4 28 3 29 1 30 1 Name: line_number, dtype: int64 . train_df.line_number.plot.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fef52380690&gt; . Looking at the distribution of the &quot;line_number&quot; column, it looks like the majority of lines have a position of 15 or less. . Knowing this, let&#39;s set the depth parameter of tf.one_hot to 15. . train_line_numbers_one_hot = tf.one_hot(train_df[&quot;line_number&quot;].to_numpy(),depth= 15) val_line_numbers_one_hot = tf.one_hot(val_df[&quot;line_number&quot;].to_numpy(),depth= 15) test_line_numbers_one_hot = tf.one_hot(test_df[&quot;line_number&quot;].to_numpy(),depth= 15) . train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20] . (TensorShape([180040, 15]), &lt;tf.Tensor: shape=(20, 15), dtype=float32, numpy= array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;) . We could create a one-hot tensor which has room for all of the potential values of &quot;line_number&quot; (depth=30), however, this would end up in a tensor of double the size of our current one (depth=15) where the vast majority of values are 0. Plus, only ~2,000/180,000 samples have a &quot;line_number&quot; value of over 15. So we would not be gaining much information about our data for doubling our feature space. This kind of problem is called the curse of dimensionality. However, since this we&#39;re working with deep models, it might be worth trying to throw as much information at the model as possible and seeing what happens. I&#39;ll leave exploring values of the depth parameter as an extension. . We can do the same above process for the total line also in data. . train_df[&quot;total_lines&quot;].value_counts() . 11 24468 10 23639 12 22113 9 19400 13 18438 14 14610 8 12285 15 10768 7 7464 16 7429 17 5202 6 3353 18 3344 19 2480 20 1281 5 1146 21 770 22 759 23 264 4 215 24 200 25 182 26 81 28 58 3 32 30 31 27 28 Name: total_lines, dtype: int64 . train_df.total_lines.plot.hist(); . It shows that majority of data has line number below 20. We can perform numpy percentile to check this. . np.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples . 20.0 . train_total_lines_one_hot = tf.one_hot(train_df[&quot;total_lines&quot;].to_numpy(), depth=20) val_total_lines_one_hot = tf.one_hot(val_df[&quot;total_lines&quot;].to_numpy(), depth=20) test_total_lines_one_hot = tf.one_hot(test_df[&quot;total_lines&quot;].to_numpy(), depth=20) # Check shape and samples of total lines one-hot tensor train_total_lines_one_hot.shape, train_total_lines_one_hot[:10] . (TensorShape([180040, 20]), &lt;tf.Tensor: shape=(10, 20), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;) . Creating The Beast (tribrid embedding model) . Steps for Creating the Model: . Create a token-level model (similar to model_1) | Create a character-level model (similar to model_3 with a slight modification to reflect the paper) | Create a &quot;line_number&quot; model (takes in one-hot-encoded &quot;line_number&quot; tensor and passes it through a non-linear layer) | Create a &quot;total_lines&quot; model (takes in one-hot-encoded &quot;total_lines&quot; tensor and passes it through a non-linear layer) | Combine (using layers.Concatenate) the outputs of 1 and 2 into a token-character-hybrid embedding and pass it series of output to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts | Combine (using layers.Concatenate) the outputs of 3, 4 and 5 into a token-character-positional tribrid embedding | Create an output layer to accept the tribrid embedding and output predicted label probabilities | Combine the inputs of 1, 2, 3, 4 and outputs of 7 into a tf.keras.Model | # 1. Token Model token_inputs = layers.Input(shape=[], dtype=&quot;string&quot;, name=&quot;token_inputs&quot;) token_embeddings = tf_hub_embedding_layer(token_inputs) token_outputs = layers.Dense(128, activation=&quot;relu&quot;)(token_embeddings) token_model = tf.keras.Model(inputs=token_inputs, outputs=token_outputs) # 2. Char Model char_inputs = layers.Input(shape=(1,), dtype= tf.string, name=&quot;char_input&quot;) char_vectors = char_vectorizer(char_inputs) char_embedding = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(25,activation=&quot;relu&quot;))(char_embedding) char_model = tf.keras.Model(inputs= char_inputs, # char_dense = layers.Dense(128,activation=&quot;relu&quot;)(char_bilstm) outputs =char_bi_lstm) # 3. Line numbers inputs line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=&quot;line_number_input&quot;) x = layers.Dense(32, activation=&quot;relu&quot;)(line_number_inputs) line_number_model = tf.keras.Model(inputs=line_number_inputs, outputs=x) # 4. Total lines inputs total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=&quot;total_lines_input&quot;) y = layers.Dense(32, activation=&quot;relu&quot;)(total_lines_inputs) total_line_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y) # 5. Combine token and char embeddings into a hybrid embedding combined_embeddings = layers.Concatenate(name=&quot;token_char_hybrid_embedding&quot;)([token_model.output, char_model.output]) z = layers.Dense(256, activation=&quot;relu&quot;)(combined_embeddings) z = layers.Dropout(0.5)(z) # 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding z = layers.Concatenate(name=&quot;token_char_positional_embedding&quot;)([line_number_model.output, total_line_model.output, z]) # 7. Create output layer output_layer = layers.Dense(5, activation=&quot;softmax&quot;, name=&quot;output_layer&quot;)(z) # 8. Put together model model_5 = tf.keras.Model(inputs=[line_number_model.input, total_line_model.input, token_model.input, char_model.input], outputs=output_layer) . model_5.summary() . Model: &#34;model_8&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== char_input (InputLayer) [(None, 1)] 0 __________________________________________________________________________________________________ token_inputs (InputLayer) [(None,)] 0 __________________________________________________________________________________________________ char_vectorizer (TextVectorizat (None, 290) 0 char_input[0][0] __________________________________________________________________________________________________ universal_sentence_encoder (Ker (None, 512) 256797824 token_inputs[0][0] __________________________________________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 char_vectorizer[2][0] __________________________________________________________________________________________________ dense_7 (Dense) (None, 128) 65664 universal_sentence_encoder[2][0] __________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 50) 10200 char_embed[2][0] __________________________________________________________________________________________________ token_char_hybrid_embedding (Co (None, 178) 0 dense_7[0][0] bidirectional_1[0][0] __________________________________________________________________________________________________ line_number_input (InputLayer) [(None, 15)] 0 __________________________________________________________________________________________________ total_lines_input (InputLayer) [(None, 20)] 0 __________________________________________________________________________________________________ dense_10 (Dense) (None, 256) 45824 token_char_hybrid_embedding[0][0] __________________________________________________________________________________________________ dense_8 (Dense) (None, 32) 512 line_number_input[0][0] __________________________________________________________________________________________________ dense_9 (Dense) (None, 32) 672 total_lines_input[0][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 256) 0 dense_10[0][0] __________________________________________________________________________________________________ token_char_positional_embedding (None, 320) 0 dense_8[0][0] dense_9[0][0] dropout_3[0][0] __________________________________________________________________________________________________ output_layer (Dense) (None, 5) 1605 token_char_positional_embedding[0 ================================================================================================== Total params: 256,924,051 Trainable params: 126,227 Non-trainable params: 256,797,824 __________________________________________________________________________________________________ . from tensorflow.keras.utils import plot_model plot_model(model_5) . model_5.compile(loss =tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), optimizer=&quot;Adam&quot;, metrics= [&quot;accuracy&quot;]) . train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers train_total_lines_one_hot, # total lines train_sentences, # train tokens train_chars)) # train chars train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately # Validation dataset val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot, val_total_lines_one_hot, val_sentences, val_chars)) val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels)) val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately # Check input shapes train_pos_char_token_dataset, val_pos_char_token_dataset . (&lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt;, &lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt;) . history_model_5 = model_5.fit(train_pos_char_token_dataset, steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)), epochs=10, validation_data=val_pos_char_token_dataset, validation_steps=int(0.1 * len(val_pos_char_token_dataset))) . Epoch 1/10 562/562 [==============================] - 166s 296ms/step - loss: 1.0464 - accuracy: 0.7606 - val_loss: 0.9803 - val_accuracy: 0.8042 Epoch 2/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9671 - accuracy: 0.8126 - val_loss: 0.9522 - val_accuracy: 0.8235 Epoch 3/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9502 - accuracy: 0.8240 - val_loss: 0.9400 - val_accuracy: 0.8301 Epoch 4/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9417 - accuracy: 0.8320 - val_loss: 0.9343 - val_accuracy: 0.8314 Epoch 5/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9384 - accuracy: 0.8335 - val_loss: 0.9268 - val_accuracy: 0.8378 Epoch 6/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9414 - accuracy: 0.8290 - val_loss: 0.9242 - val_accuracy: 0.8391 Epoch 7/10 562/562 [==============================] - 167s 298ms/step - loss: 0.9288 - accuracy: 0.8379 - val_loss: 0.9248 - val_accuracy: 0.8388 Epoch 8/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9249 - accuracy: 0.8428 - val_loss: 0.9119 - val_accuracy: 0.8507 Epoch 9/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9270 - accuracy: 0.8385 - val_loss: 0.9236 - val_accuracy: 0.8411 Epoch 10/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9226 - accuracy: 0.8458 - val_loss: 0.9117 - val_accuracy: 0.8411 . model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1) model_5_pred_probs . 945/945 [==============================] - 60s 62ms/step . array([[0.55797863, 0.11753111, 0.01437984, 0.28599715, 0.02411333], [0.5643995 , 0.12752993, 0.05248916, 0.24036951, 0.01521185], [0.35432863, 0.09137847, 0.1489166 , 0.3426122 , 0.06276409], ..., [0.02403956, 0.04742315, 0.01662572, 0.02715172, 0.88475984], [0.02105152, 0.34086674, 0.04562447, 0.02258216, 0.56987506], [0.10035124, 0.76991165, 0.05545753, 0.04023053, 0.03404894]], dtype=float32) . model_5_preds = tf.argmax(model_5_pred_probs, axis=1) model_5_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 0, ..., 4, 4, 1])&gt; . model_5_results = calculate_results(y_true=val_labels_encoded, y_pred=model_5_preds) model_5_results . {&#39;accuracy&#39;: 84.80074142724744, &#39;f1&#39;: 0.8445848608307035, &#39;precision&#39;: 0.8499109192748401, &#39;recall&#39;: 0.8480074142724745} . all_model_results = pd.DataFrame({&quot;baseline&quot;: baseline_results, &quot;custom_token_embed_conv1d&quot;: model_1_results, &quot;pretrained_token_embed&quot;: model_2_results, &quot;custom_char_embed_conv1d&quot;: model_3_results, &quot;hybrid_char_token_embed&quot;: model_4_results, &quot;tribrid_pos_char_token_embed&quot;: model_5_results}) all_model_results = all_model_results.transpose() all_model_results . accuracy precision recall f1 . baseline 72.183238 | 0.718647 | 0.721832 | 0.698925 | . custom_token_embed_conv1d 80.828810 | 0.804780 | 0.808288 | 0.805473 | . pretrained_token_embed 75.105918 | 0.745737 | 0.751059 | 0.746384 | . custom_char_embed_conv1d 66.933669 | 0.663860 | 0.669337 | 0.659734 | . hybrid_char_token_embed 75.983053 | 0.756189 | 0.759831 | 0.754541 | . tribrid_pos_char_token_embed 84.800741 | 0.849911 | 0.848007 | 0.844585 | . all_model_results[&quot;accuracy&quot;] = all_model_results[&quot;accuracy&quot;]/100 . all_model_results.plot(kind=&quot;bar&quot;, figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0)); . all_model_results.sort_values(&quot;f1&quot;, ascending=False)[&quot;f1&quot;].plot(kind=&quot;bar&quot;, figsize=(10, 7)); . model_5.save(&quot;tribrid_model&quot;) . INFO:tensorflow:Assets written to: tribrid_model/assets . INFO:tensorflow:Assets written to: tribrid_model/assets . model_path = &quot;/content/tribrid_model&quot; . loaded_model = tf.keras.models.load_model(model_path) . loaded_pred_probs = loaded_model.predict(val_pos_char_token_dataset, verbose=1) loaded_preds = tf.argmax(loaded_pred_probs, axis=1) loaded_preds[:10] . 945/945 [==============================] - 61s 63ms/step . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 2, 2, 4, 4, 4, 4, 1])&gt; . loaded_model_results = calculate_results(val_labels_encoded, loaded_preds) loaded_model_results . {&#39;accuracy&#39;: 84.80074142724744, &#39;f1&#39;: 0.8445848608307035, &#39;precision&#39;: 0.8499109192748401, &#39;recall&#39;: 0.8480074142724745} . Evaluate model on test dataset . To make our model&#39;s performance more comparable with the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, let&#39;s make predictions on the test dataset and evaluate them. . test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot, test_total_lines_one_hot, test_sentences, test_chars)) test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot) test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels)) test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Check shapes test_pos_char_token_dataset . &lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt; . test_pred_probs = loaded_model.predict(test_pos_char_token_dataset, verbose=1) test_preds = tf.argmax(test_pred_probs, axis=1) test_preds[:10] . 942/942 [==============================] - 58s 62ms/step . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 2, 2, 2, 4, 4, 4, 1, 1, 0])&gt; . loaded_model_test_results = calculate_results(y_true=test_labels_encoded, y_pred=test_preds) loaded_model_test_results . {&#39;accuracy&#39;: 84.48979591836735, &#39;f1&#39;: 0.8415058973152829, &#39;precision&#39;: 0.8456318536204341, &#39;recall&#39;: 0.8448979591836735} . %%time # Get list of class names of test predictions test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds] test_pred_classes . CPU times: user 3.43 s, sys: 10 ms, total: 3.44 s Wall time: 3.44 s . test_df[&quot;prediction&quot;] = test_pred_classes # create column with test prediction class names test_df[&quot;pred_prob&quot;] = tf.reduce_max(test_pred_probs, axis=1).numpy() # get the maximum prediction probability test_df[&quot;correct&quot;] = test_df[&quot;prediction&quot;] == test_df[&quot;target&quot;] # create binary column for whether the prediction is right or not test_df.head(20) . target text line_number total_lines prediction pred_prob correct . 0 BACKGROUND | this study analyzed liver function abnormaliti... | 0 | 8 | OBJECTIVE | 0.359128 | False | . 1 RESULTS | a post hoc analysis was conducted with the use... | 1 | 8 | METHODS | 0.395141 | False | . 2 RESULTS | liver function tests ( lfts ) were measured at... | 2 | 8 | METHODS | 0.857965 | False | . 3 RESULTS | survival analyses were used to assess the asso... | 3 | 8 | METHODS | 0.733716 | False | . 4 RESULTS | the percentage of patients with abnormal lfts ... | 4 | 8 | RESULTS | 0.734855 | True | . 5 RESULTS | when mean hemodynamic profiles were compared i... | 5 | 8 | RESULTS | 0.888223 | True | . 6 RESULTS | multivariable analyses revealed that patients ... | 6 | 8 | RESULTS | 0.569608 | True | . 7 CONCLUSIONS | abnormal lfts are common in the adhf populatio... | 7 | 8 | CONCLUSIONS | 0.476394 | True | . 8 CONCLUSIONS | elevated meld-xi scores are associated with po... | 8 | 8 | CONCLUSIONS | 0.583600 | True | . 9 BACKGROUND | minimally invasive endovascular aneurysm repai... | 0 | 12 | BACKGROUND | 0.633004 | True | . 10 BACKGROUND | the aim of this study was to analyse the cost-... | 1 | 12 | OBJECTIVE | 0.445351 | False | . 11 METHODS | resource use was determined from the amsterdam... | 2 | 12 | METHODS | 0.773279 | True | . 12 METHODS | the analysis was performed from a provider per... | 3 | 12 | METHODS | 0.865364 | True | . 13 METHODS | all costs were calculated as if all patients h... | 4 | 12 | METHODS | 0.490311 | True | . 14 RESULTS | a total of @ patients were randomized . | 5 | 12 | RESULTS | 0.788061 | True | . 15 RESULTS | the @-day mortality rate was @ per cent after ... | 6 | 12 | RESULTS | 0.816789 | True | . 16 RESULTS | at @months , the total mortality rate for evar... | 7 | 12 | RESULTS | 0.890838 | True | . 17 RESULTS | the mean cost difference between evar and or w... | 8 | 12 | RESULTS | 0.881075 | True | . 18 RESULTS | the incremental cost-effectiveness ratio per p... | 9 | 12 | RESULTS | 0.832961 | True | . 19 RESULTS | there was no significant difference in quality... | 10 | 12 | RESULTS | 0.817365 | True | . Future Work . As we trained our above Models with subset of actual data(PubMed 20k), training the same model with larger samples(PubMed 200k) might have chance of Increase in Accuracy. . | Except Universal Sentence Encoder, we&#39;ll try to replace embedding layers with preratined embedding (Contex Independent) like Word2Vec, GloVe and FastText and compare between them. . | Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the TensorFlow Hub BERT PubMed expert (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results? Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the TensorFlow Hub guide). Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf What happens if you were to merge our line_number and total_lines features for each sequence? For example, created a X_of_Y feature instead? Does this effect model performance? . | . Note: The main difference above is a consequence of the fact Word2vec and Glove do not take into account word order in their training - ELMo and BERT take into account word order (ELMo uses LSTMS; BERT uses Transformer - an attention based model with positional encodings to represent word positions). . .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/text%20classification/health%20care/deep%20learning/2021/04/10/Sentence-Classification.html",
            "relUrl": "/natural%20language%20processing/text%20classification/health%20care/deep%20learning/2021/04/10/Sentence-Classification.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Quora Insincere Questions Classification Using BERT",
            "content": ". Figure 1: BERT Classification Model . Overview . Predicting whether a question asked on Quora is sincere or not.The Model input a Question of Quora and Output &quot;sincere&quot; or &quot;Insincere&quot;. So Basically its a Binary Classification problem with Text data. . | As Quora has a large database and solving this probelm with Recurrent Neural Network (LSTM, GRU, Conv-1D) is little hard, So here we use State-Of-Art Transformer BERT(Bidirectional Encoder Representations from Transformers). . | The pretrained BERT model used in this project is available on TensorFlow Hub. . !nvidia-smi . Wed Oct 7 12:07:44 2020 +--+ | NVIDIA-SMI 455.23.05 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 35C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | | | | ERR! | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . Install TensorFlow and TensorFlow Model Garden . import tensorflow as tf print(tf.version.VERSION) . 2.3.0 . !pip install -q tensorflow==2.3.0 . !git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git . Cloning into &#39;models&#39;... remote: Enumerating objects: 2650, done. remote: Counting objects: 100% (2650/2650), done. remote: Compressing objects: 100% (2318/2318), done. remote: Total 2650 (delta 512), reused 1350 (delta 299), pack-reused 0 Receiving objects: 100% (2650/2650), 34.01 MiB | 3.93 MiB/s, done. Resolving deltas: 100% (512/512), done. Note: checking out &#39;400d68abbccda2f0f6609e3a924467718b144233&#39;. You are in &#39;detached HEAD&#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt; . !pip install -Uqr models/official/requirements.txt # you may have to restart the runtime afterwards . |████████████████████████████████| 61kB 1.9MB/s |████████████████████████████████| 194kB 7.4MB/s |████████████████████████████████| 14.5MB 240kB/s |████████████████████████████████| 9.5MB 56.1MB/s |████████████████████████████████| 460kB 70.0MB/s |████████████████████████████████| 102kB 12.7MB/s |████████████████████████████████| 25.9MB 95kB/s |████████████████████████████████| 174kB 66.8MB/s |████████████████████████████████| 3.5MB 24.2MB/s |████████████████████████████████| 1.1MB 69.9MB/s |████████████████████████████████| 358kB 81.1MB/s |████████████████████████████████| 1.1MB 79.4MB/s |████████████████████████████████| 11.6MB 222kB/s |████████████████████████████████| 36.7MB 89kB/s |████████████████████████████████| 276kB 63.8MB/s |████████████████████████████████| 2.2MB 59.1MB/s |████████████████████████████████| 92kB 11.7MB/s |████████████████████████████████| 81kB 11.8MB/s |████████████████████████████████| 501kB 61.7MB/s Building wheel for psutil (setup.py) ... done Building wheel for py-cpuinfo (setup.py) ... done Building wheel for pyyaml (setup.py) ... done Building wheel for proto-plus (setup.py) ... done ERROR: tensorflow 2.3.0 has requirement numpy&lt;1.19.0,&gt;=1.16.0, but you&#39;ll have numpy 1.19.2 which is incompatible. ERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you&#39;ll have scipy 1.5.2 which is incompatible. ERROR: google-cloud-storage 1.18.1 has requirement google-resumable-media&lt;0.5.0dev,&gt;=0.3.1, but you&#39;ll have google-resumable-media 1.1.0 which is incompatible. ERROR: google-api-core 1.22.4 has requirement google-auth&lt;2.0dev,&gt;=1.21.1, but you&#39;ll have google-auth 1.17.2 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: albumentations 0.1.12 has requirement imgaug&lt;0.2.7,&gt;=0.2.5, but you&#39;ll have imgaug 0.2.9 which is incompatible. . Download and Import the Quora Insincere Questions Dataset . import numpy as np import tensorflow as tf import tensorflow_hub as hub import sys sys.path.append(&#39;models&#39;) from official.nlp.data import classifier_data_lib from official.nlp.bert import tokenization from official.nlp import optimization . print(&quot;TF Version: &quot;, tf.__version__) print(&quot;Eager mode: &quot;, tf.executing_eagerly()) print(&quot;Hub version: &quot;, hub.__version__) print(&quot;GPU is&quot;, &quot;available&quot; if tf.config.experimental.list_physical_devices(&quot;GPU&quot;) else &quot;NOT AVAILABLE&quot;) . TF Version: 2.3.0 Eager mode: True Hub version: 0.9.0 GPU is available . A downloadable copy of the Quora Insincere Questions Classification data can be found https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip. Decompress and read the data into a pandas DataFrame. . import numpy as np import pandas as pd # reading the dataset from the link or you can download it from kaggle alos # https://www.kaggle.com/c/quora-insincere-questions-classification/data df = pd.read_csv(&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;, compression = &#39;zip&#39;, low_memory = False) # checking the no of row and columns in the dataset df.shape . (1306122, 3) . df.head(20) . qid question_text target . 0 00002165364db923c7e6 | How did Quebec nationalists see their province... | 0 | . 1 000032939017120e6e44 | Do you have an adopted dog, how would you enco... | 0 | . 2 0000412ca6e4628ce2cf | Why does velocity affect time? Does velocity a... | 0 | . 3 000042bf85aa498cd78e | How did Otto von Guericke used the Magdeburg h... | 0 | . 4 0000455dfa3e01eae3af | Can I convert montra helicon D to a mountain b... | 0 | . 5 00004f9a462a357c33be | Is Gaza slowly becoming Auschwitz, Dachau or T... | 0 | . 6 00005059a06ee19e11ad | Why does Quora automatically ban conservative ... | 0 | . 7 0000559f875832745e2e | Is it crazy if I wash or wipe my groceries off... | 0 | . 8 00005bd3426b2d0c8305 | Is there such a thing as dressing moderately, ... | 0 | . 9 00006e6928c5df60eacb | Is it just me or have you ever been in this ph... | 0 | . 10 000075f67dd595c3deb5 | What can you say about feminism? | 0 | . 11 000076f3b42776c692de | How were the Calgary Flames founded? | 0 | . 12 000089792b3fc8026741 | What is the dumbest, yet possibly true explana... | 0 | . 13 000092a90bcfbfe8cd88 | Can we use our external hard disk as a OS as w... | 0 | . 14 000095680e41a9a6f6e3 | I am 30, living at home and have no boyfriend.... | 0 | . 15 0000a89942e3143e333a | What do you know about Bram Fischer and the Ri... | 0 | . 16 0000b8e1279eaa0a7062 | How difficult is it to find a good instructor ... | 0 | . 17 0000bc0f62500f55959f | Have you licked the skin of a corpse? | 0 | . 18 0000ce6c31f14d3e09ec | Do you think Amazon will adopt an in house app... | 0 | . 19 0000d329332845b8a7fa | How many baronies might exist within a county ... | 0 | . df.target.plot(kind = &#39;hist&#39;, title = &#39;target distribution&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;target distribution&#39;}, ylabel=&#39;Frequency&#39;&gt; . As you can see here its a imbalanced dataset, we have to set our split data show that both the target labels should be present in train and test set. So we use stratified sampling to overcome this. . Create tf.data.Datasets for Training and Evaluation . . Note: As our dataset has a class imbalance issue, we use stratify sampling to overcome this. . from sklearn.model_selection import train_test_split train_df , remaining = train_test_split(df, random_state = 42 , train_size = 0.0075, stratify = df.target.values) valid_df , _ = train_test_split(remaining, random_state= 42 , train_size = 0.00075, stratify =remaining.target.values) # ensure the shape of both train and test data train_df.shape , valid_df.shape . ((9795, 3), (972, 3)) . As the dataset is preety huge,the whole dataset take much longer to train so we use only a small train and test portion, set a ratio of 90% train and 10% test.Secondly to overcome the io bottleneck we use tf.data.Dataset pipelines. . with tf.device(&#39;/cpu:0&#39;): train_data = tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values)) valid_data = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values)) # reading from tensorflow data pipeline for text,label in train_data.take(1): print(text) print(label) . tf.Tensor(b&#39;Why are unhealthy relationships so desirable?&#39;, shape=(), dtype=string) tf.Tensor(0, shape=(), dtype=int64) . Download a Pre-trained BERT Model from TensorFlow Hub . &quot;&quot;&quot; Each line of the dataset is composed of the review text and its label - Data preprocessing consists of transforming text to BERT input features: input_word_ids, input_mask, segment_ids - In the process, tokenizing the text is done with the provided BERT model tokenizer &quot;&quot;&quot; # Label categories label_list = [0,1] # maximum length of (token) input sequences max_seq_length = 128 # Define the batch size train_batch_size = 32 # Get BERT layer and tokenizer: # BERT details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2 bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;, trainable=True) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case) . tokenizer.wordpiece_tokenizer.tokenize(&#39;hey, how are you ?&#39;) . [&#39;hey&#39;, &#39;##,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;?&#39;] . tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize(&#39;hey, how are you ?&#39;)) . [4931, 29623, 2129, 2024, 2017, 1029] . Tokenize and Preprocess Text for BERT . We&#39;ll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExamples using classifier_data_lib&#39;s constructor InputExample provided in the BERT library. . def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer): example = classifier_data_lib.InputExample(guid = None, text_a= text.numpy(), text_b =None, label= label.numpy()) feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer) return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id) . You want to use Dataset.map to apply this function to each element of the dataset. Dataset.map runs in graph mode. . Graph tensors do not have a value. | In graph mode you can only use TensorFlow Ops and functions. | . So you can&#39;t .map this function directly: You need to wrap it in a tf.py_function. The tf.py_function will pass regular tensors (with a value and a .numpy() method to access it), to the wrapped python function. . Wrap a Python Function into a TensorFlow op for Eager Execution . def to_feature_map(text, label): input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature,inp =[text, label], Tout=[tf.int32,tf.int32,tf.int32,tf.int32]) input_ids.set_shape([max_seq_length]) input_mask.set_shape([max_seq_length]) segment_ids.set_shape([max_seq_length]) label_id.set_shape([]) x = { &#39;input_word_ids&#39;: input_ids, &#39;input_mask&#39;: input_mask, &#39;input_type_ids&#39;: segment_ids } return (x,label_id) . Create a TensorFlow Input Pipeline with tf.data . with tf.device(&#39;/cpu:0&#39;): # train train_data = (train_data.map(to_feature_map, num_parallel_calls =tf.data.experimental.AUTOTUNE) .shuffle(1000) .batch(32, drop_remainder = True) .prefetch(tf.data.experimental.AUTOTUNE)) # valid valid_data = (valid_data.map(to_feature_map, num_parallel_calls =tf.data.experimental.AUTOTUNE) .batch(32, drop_remainder = True) .prefetch(tf.data.experimental.AUTOTUNE)) . The resulting tf.data.Datasets return (features, labels) pairs, as expected by keras.Model.fit: . train_data.element_spec . ({&#39;input_mask&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_type_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_word_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(32,), dtype=tf.int32, name=None)) . valid_data.element_spec . ({&#39;input_mask&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_type_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_word_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(32,), dtype=tf.int32, name=None)) . Add a Classification Head to the BERT Layer . Figure 3: BERT Layer . def create_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_word_ids&quot;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids]) drop = tf.keras.layers.Dropout(0.4)(pooled_output) output = tf.keras.layers.Dense(1, activation= &#39;sigmoid&#39;, name = &#39;output&#39;)(drop) model = tf.keras.Model( inputs = { &#39;input_word_ids&#39;: input_word_ids, &#39;input_mask&#39;: input_mask, &#39;input_type_ids&#39;: input_type_ids }, outputs = output ) return model . Fine-Tune BERT for Text Classification . model = create_model() model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=2e-5), loss = tf.keras.losses.BinaryCrossentropy(), metrics = [tf.keras.metrics.BinaryAccuracy()]) model.summary() . Model: &#34;functional_7&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_word_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_word_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 768) 0 keras_layer[4][0] __________________________________________________________________________________________________ output (Dense) (None, 1) 769 dropout_4[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 109,483,009 Non-trainable params: 1 __________________________________________________________________________________________________ . tf.keras.utils.plot_model(model=model, show_shapes=True, dpi =75) . epochs = 4 history = model.fit(train_data, validation_data=valid_data, epochs = epochs, verbose = 1) . Epoch 1/4 306/306 [==============================] - ETA: 0s - loss: 0.0148 - binary_accuracy: 0.9953WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_test_batch_end` time: 0.1383s). Check your callbacks. . WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_test_batch_end` time: 0.1383s). Check your callbacks. . 306/306 [==============================] - 146s 478ms/step - loss: 0.0148 - binary_accuracy: 0.9953 - val_loss: 0.2203 - val_binary_accuracy: 0.9500 Epoch 2/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0068 - binary_accuracy: 0.9979 - val_loss: 0.2746 - val_binary_accuracy: 0.9594 Epoch 3/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0089 - binary_accuracy: 0.9971 - val_loss: 0.2665 - val_binary_accuracy: 0.9479 Epoch 4/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0081 - binary_accuracy: 0.9975 - val_loss: 0.2688 - val_binary_accuracy: 0.9573 . Evaluate the BERT Text Classification Model . import matplotlib.pyplot as plt def plot_graphs(history, metric): plt.plot(history.history[metric]) plt.plot(history.history[&#39;val_&#39;+metric], &#39;&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(metric) plt.legend([metric, &#39;val_&#39;+metric]) plt.show() . plot_graphs(history, &#39;binary_accuracy&#39;) . plot_graphs(history, &#39;loss&#39;) . sample_examples = [&#39;Are you ashamed of being an Indian?&#39;,&#39; you are a racist&#39;, &#39; Its really helpfull, thank you&#39;, &#39; Thanks for you help&#39;,] test_data = tf.data.Dataset.from_tensor_slices((sample_examples, [0]*len(sample_examples))) test_data = (test_data.map(to_feature_map).batch(1)) preds = model.predict(test_data) threshold = 0.7 [&#39;Insincere&#39; if pred&gt;= threshold else &#39;Sincere&#39; for pred in preds] . [&#39;Insincere&#39;, &#39;Insincere&#39;, &#39;Sincere&#39;, &#39;Sincere&#39;] . Useful Links . https://jalammar.github.io/illustrated-transformer/ | https://jalammar.github.io/illustrated-bert/ | . https://nlp.seas.harvard.edu/2018/04/03/attention.html | https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/ | .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/text%20classification/deep%20learning/2021/03/10/Quora_Insincere-Classification-BERT.html",
            "relUrl": "/natural%20language%20processing/text%20classification/deep%20learning/2021/03/10/Quora_Insincere-Classification-BERT.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Sentiment Analysis of Play store apps review with BERT and PyTorch",
            "content": "Overview . In this Project, we&#39;ll learn how to fine-tune BERT for sentiment analysis. You&#39;ll do the required text preprocessing (special tokens, padding, and attention masks) and build a Sentiment Classifier using the amazing Transformers library by Hugging Face! . You&#39;ll learn how to: . Intuitively understand what BERT | Preprocess text data for BERT and build PyTorch Dataset (tokenization, attention masks, and padding) | Use Transfer Learning to build Sentiment Classifier using the Transformers library by Hugging Face | Evaluate the model on test data | Predict sentiment on raw text | . !nvidia-smi . Mon Apr 20 19:22:31 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . What is BERT? . BERT (introduced in this paper) stands for Bidirectional Encoder Representations from Transformers. If you don&#39;t know what most of that means - you&#39;ve come to the right place! Let&#39;s unpack the main ideas: . Bidirectional - to understand the text you&#39;re looking you&#39;ll have to look back (at the previous words) and forward (at the next words) | Transformers - The Attention Is All You Need paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. his in a sentence refers to Jim). | (Pre-trained) contextualized word embeddings - The ELMO paper introduced a way to encode words based on their meaning/context. Nails has multiple meanings - fingernails and metal nails. | . BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence. Let&#39;s look at examples of these tasks: . Masked Language Modeling (Masked LM) . The objective of this task is to guess the masked tokens. Let&#39;s look at an example, and try to not make it harder than it has to be: . That&#39;s [mask] she [mask] -&gt; That&#39;s what she said . Next Sentence Prediction (NSP) . Given a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let&#39;s continue with the example: . Input = [CLS] That&#39;s [mask] she [mask]. [SEP] Hahaha, nice! [SEP] . Label = IsNext . Input = [CLS] That&#39;s [mask] she [mask]. [SEP] Dwight, you ignorant [mask]! [SEP] . Label = NotNext . The training corpus was comprised of two entries: Toronto Book Corpus (800M words) and English Wikipedia (2,500M words). While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder. . BERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large). . Is This Thing Useful in Practice? . The BERT paper was released along with the source code and pre-trained models. . The best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance! . Setup . We&#39;ll need the Transformers library by Hugging Face: . . Tip: An IPython magic extension for printing date and time stamps, version numbers, and hardware information. . !pip install -q -U watermark . !pip install -qq transformers . %reload_ext watermark %watermark -v -p numpy,pandas,torch,transformers . CPython 3.6.9 IPython 5.5.0 numpy 1.18.2 pandas 1.0.3 torch 1.4.0 transformers 2.8.0 . import transformers from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup import torch import numpy as np import pandas as pd import seaborn as sns from pylab import rcParams import matplotlib.pyplot as plt from matplotlib import rc from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, classification_report from collections import defaultdict from textwrap import wrap from torch import nn, optim from torch.utils.data import Dataset, DataLoader import torch.nn.functional as F %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; sns.set(style=&#39;whitegrid&#39;, palette=&#39;muted&#39;, font_scale=1.2) HAPPY_COLORS_PALETTE = [&quot;#01BEFE&quot;, &quot;#FFDD00&quot;, &quot;#FF7D00&quot;, &quot;#FF006D&quot;, &quot;#ADFF02&quot;, &quot;#8F00FF&quot;] sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE)) rcParams[&#39;figure.figsize&#39;] = 12, 8 RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) torch.manual_seed(RANDOM_SEED) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) device . device(type=&#39;cuda&#39;, index=0) . Data Exploration . We&#39;ll load the Google Play app reviews dataset, that we&#39;ve put together in the previous part: . !gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV !gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv . Downloading... From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV To: /content/apps.csv 100% 134k/134k [00:00&lt;00:00, 50.2MB/s] Downloading... From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv To: /content/reviews.csv 7.17MB [00:00, 33.4MB/s] . df = pd.read_csv(&quot;reviews.csv&quot;) df.head() . userName userImage content score thumbsUpCount reviewCreatedVersion at replyContent repliedAt sortOrder appId . 0 Andrew Thomas | https://lh3.googleusercontent.com/a-/AOh14GiHd... | Update: After getting a response from the deve... | 1 | 21 | 4.17.0.3 | 2020-04-05 22:25:57 | According to our TOS, and the term you have ag... | 2020-04-05 15:10:24 | most_relevant | com.anydo | . 1 Craig Haines | https://lh3.googleusercontent.com/-hoe0kwSJgPQ... | Used it for a fair amount of time without any ... | 1 | 11 | 4.17.0.3 | 2020-04-04 13:40:01 | It sounds like you logged in with a different ... | 2020-04-05 15:11:35 | most_relevant | com.anydo | . 2 steven adkins | https://lh3.googleusercontent.com/a-/AOh14GiXw... | Your app sucks now!!!!! Used to be good but no... | 1 | 17 | 4.17.0.3 | 2020-04-01 16:18:13 | This sounds odd! We are not aware of any issue... | 2020-04-02 16:05:56 | most_relevant | com.anydo | . 3 Lars Panzerbjørn | https://lh3.googleusercontent.com/a-/AOh14Gg-h... | It seems OK, but very basic. Recurring tasks n... | 1 | 192 | 4.17.0.2 | 2020-03-12 08:17:34 | We do offer this option as part of the Advance... | 2020-03-15 06:20:13 | most_relevant | com.anydo | . 4 Scott Prewitt | https://lh3.googleusercontent.com/-K-X1-YsVd6U... | Absolutely worthless. This app runs a prohibit... | 1 | 42 | 4.17.0.2 | 2020-03-14 17:41:01 | We&#39;re sorry you feel this way! 90% of the app ... | 2020-03-15 23:45:51 | most_relevant | com.anydo | . df.shape . (15746, 11) . We have about 16k examples. Let&#39;s check for missing values: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 15746 entries, 0 to 15745 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 userName 15746 non-null object 1 userImage 15746 non-null object 2 content 15746 non-null object 3 score 15746 non-null int64 4 thumbsUpCount 15746 non-null int64 5 reviewCreatedVersion 13533 non-null object 6 at 15746 non-null object 7 replyContent 7367 non-null object 8 repliedAt 7367 non-null object 9 sortOrder 15746 non-null object 10 appId 15746 non-null object dtypes: int64(2), object(9) memory usage: 1.3+ MB . Great, no missing values in the score and review texts! Do we have class imbalance? . sns.countplot(df.score) plt.xlabel(&#39;review score&#39;); . That&#39;s hugely imbalanced, but it&#39;s okay. We&#39;re going to convert the dataset into negative, neutral and positive sentiment: . def to_sentiment(rating): rating = int(rating) if rating &lt;= 2: return 0 elif rating == 3: return 1 else: return 2 df[&#39;sentiment&#39;] = df.score.apply(to_sentiment) . class_names = [&#39;negative&#39;, &#39;neutral&#39;, &#39;positive&#39;] . ax = sns.countplot(df.sentiment) plt.xlabel(&#39;review sentiment&#39;) ax.set_xticklabels(class_names); . The balance was (mostly) restored. . Data Preprocessing . You might already know that Machine Learning models don&#39;t work with raw text. You need to convert text to numbers (of some sort). BERT requires even more attention (good one, right?). Here are the requirements: . Add special tokens to separate sentences and do classification | Pass sequences of constant length (introduce padding) | Create array of 0s (pad token) and 1s (real token) called attention mask | . The Transformers library provides (you&#39;ve guessed it) a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers that do the heavy lifting for us! . PRE_TRAINED_MODEL_NAME = &#39;bert-base-cased&#39; . You can use a cased and uncased version of BERT and tokenizer. I&#39;ve experimented with both. The cased version works better. Intuitively, that makes sense, since &quot;BAD&quot; might convey more sentiment than &quot;bad&quot;. . Let&#39;s load a pre-trained BertTokenizer: . tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) . We&#39;ll use this text to understand the tokenization process: . sample_txt = &#39;When was I last outside? I am stuck at home for 2 weeks.&#39; . Some basic operations can convert the text to tokens and tokens to unique integers (ids): . tokens = tokenizer.tokenize(sample_txt) token_ids = tokenizer.convert_tokens_to_ids(tokens) print(f&#39; Sentence: {sample_txt}&#39;) print(f&#39; Tokens: {tokens}&#39;) print(f&#39;Token IDs: {token_ids}&#39;) . Sentence: When was I last outside? I am stuck at home for 2 weeks. Tokens: [&#39;When&#39;, &#39;was&#39;, &#39;I&#39;, &#39;last&#39;, &#39;outside&#39;, &#39;?&#39;, &#39;I&#39;, &#39;am&#39;, &#39;stuck&#39;, &#39;at&#39;, &#39;home&#39;, &#39;for&#39;, &#39;2&#39;, &#39;weeks&#39;, &#39;.&#39;] Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119] . Special Tokens . [SEP] - marker for ending of a sentence . tokenizer.sep_token, tokenizer.sep_token_id . (&#39;[SEP]&#39;, 102) . [CLS] - we must add this token to the start of each sentence, so BERT knows we&#39;re doing classification . tokenizer.cls_token, tokenizer.cls_token_id . (&#39;[CLS]&#39;, 101) . There is also a special token for padding: . tokenizer.pad_token, tokenizer.pad_token_id . (&#39;[PAD]&#39;, 0) . BERT understands tokens that were in the training set. Everything else can be encoded using the [UNK] (unknown) token: . tokenizer.unk_token, tokenizer.unk_token_id . (&#39;[UNK]&#39;, 100) . All of that work can be done using the encode_plus() method: . encoding = tokenizer.encode_plus( sample_txt, max_length=32, add_special_tokens=True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, # Return PyTorch tensors ) encoding.keys() . dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]) . The token ids are now stored in a Tensor and padded to a length of 32: . print(len(encoding[&#39;input_ids&#39;][0])) encoding[&#39;input_ids&#39;][0] . 32 . tensor([ 101, 1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . The attention mask has the same length: . print(len(encoding[&#39;attention_mask&#39;][0])) encoding[&#39;attention_mask&#39;] . 32 . tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) . We can inverse the tokenization to have a look at the special tokens: . tokenizer.convert_ids_to_tokens(encoding[&#39;input_ids&#39;][0]) . [&#39;[CLS]&#39;, &#39;When&#39;, &#39;was&#39;, &#39;I&#39;, &#39;last&#39;, &#39;outside&#39;, &#39;?&#39;, &#39;I&#39;, &#39;am&#39;, &#39;stuck&#39;, &#39;at&#39;, &#39;home&#39;, &#39;for&#39;, &#39;2&#39;, &#39;weeks&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;] . Choosing Sequence Length . BERT works with fixed-length sequences. We&#39;ll use a simple strategy to choose the max length. Let&#39;s store the token length of each review: . token_lens = [] for txt in df.content: tokens = tokenizer.encode(txt, max_length=512) token_lens.append(len(tokens)) . and plot the distribution: . sns.distplot(token_lens) plt.xlim([0, 256]); plt.xlabel(&#39;Token count&#39;); . Most of the reviews seem to contain less than 128 tokens, but we&#39;ll be on the safe side and choose a maximum length of 160. . MAX_LEN = 160 . We have all building blocks required to create a PyTorch dataset. Let&#39;s do it: . class GPReviewDataset(Dataset): def __init__(self, reviews, targets, tokenizer, max_len): self.reviews = reviews self.targets = targets self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.reviews) def __getitem__(self, item): review = str(self.reviews[item]) target = self.targets[item] encoding = self.tokenizer.encode_plus( review, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, ) return { &#39;review_text&#39;: review, &#39;input_ids&#39;: encoding[&#39;input_ids&#39;].flatten(), &#39;attention_mask&#39;: encoding[&#39;attention_mask&#39;].flatten(), &#39;targets&#39;: torch.tensor(target, dtype=torch.long) } . The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it&#39;ll be easier to evaluate the predictions from our model. Let&#39;s split the data: . df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED) df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED) . df_train.shape, df_val.shape, df_test.shape . ((14171, 12), (787, 12), (788, 12)) . We also need to create a couple of data loaders. Here&#39;s a helper function to do it: . def create_data_loader(df, tokenizer, max_len, batch_size): ds = GPReviewDataset( reviews=df.content.to_numpy(), targets=df.sentiment.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 ) . BATCH_SIZE = 16 train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE) test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE) . Let&#39;s have a look at an example batch from our training data loader: . data = next(iter(train_data_loader)) data.keys() . dict_keys([&#39;review_text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;targets&#39;]) . print(data[&#39;input_ids&#39;].shape) print(data[&#39;attention_mask&#39;].shape) print(data[&#39;targets&#39;].shape) . torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16]) . Sentiment Classification with BERT and Hugging Face . There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use BertForSequenceClassification, BertForQuestionAnswering or something else. . But who cares, right? We&#39;re hardcore! We&#39;ll use the basic BertModel and build our sentiment classifier on top of it. Let&#39;s load the model: . bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) . And try to use it on the encoding of our sample text: . last_hidden_state, pooled_output = bert_model( input_ids=encoding[&#39;input_ids&#39;], attention_mask=encoding[&#39;attention_mask&#39;] ) . The last_hidden_state is a sequence of hidden states of the last layer of the model. Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state: . last_hidden_state.shape . torch.Size([1, 32, 768]) . We have the hidden state for each of our 32 tokens (the length of our example sequence). But why 768? This is the number of hidden units in the feedforward-networks. We can verify that by checking the config: . bert_model.config.hidden_size . 768 . You can think of the pooled_output as a summary of the content, according to BERT. Albeit, you might try and do better. Let&#39;s look at the shape of the output: . pooled_output.shape . torch.Size([1, 768]) . We can use all of this knowledge to create a classifier that uses the BERT model: . class SentimentClassifier(nn.Module): def __init__(self, n_classes): super(SentimentClassifier, self).__init__() self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) . Our classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we&#39;re returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work. . This should work like any other PyTorch model. Let&#39;s create an instance and move it to the GPU: . model = SentimentClassifier(len(class_names)) model = model.to(device) . We&#39;ll move the example batch of our training data to the GPU: . input_ids = data[&#39;input_ids&#39;].to(device) attention_mask = data[&#39;attention_mask&#39;].to(device) print(input_ids.shape) # batch size x seq length print(attention_mask.shape) # batch size x seq length . torch.Size([16, 160]) torch.Size([16, 160]) . To get the predicted probabilities from our trained model, we&#39;ll apply the softmax function to the outputs: . F.softmax(model(input_ids, attention_mask), dim=1) . tensor([[0.5879, 0.0842, 0.3279], [0.4308, 0.1888, 0.3804], [0.4871, 0.1766, 0.3363], [0.3364, 0.0778, 0.5858], [0.4025, 0.1040, 0.4935], [0.3599, 0.1026, 0.5374], [0.5054, 0.1552, 0.3394], [0.5962, 0.1464, 0.2574], [0.3274, 0.1967, 0.4759], [0.3026, 0.1118, 0.5856], [0.4103, 0.1571, 0.4326], [0.4879, 0.2121, 0.3000], [0.3811, 0.1477, 0.4712], [0.3354, 0.1354, 0.5292], [0.3999, 0.2822, 0.3179], [0.5075, 0.1684, 0.3242]], device=&#39;cuda:0&#39;, grad_fn=&lt;SoftmaxBackward&gt;) . Training . To reproduce the training procedure from the BERT paper, we&#39;ll use the AdamW optimizer provided by Hugging Face. It corrects weight decay, so it&#39;s similar to the original paper. We&#39;ll also use a linear scheduler with no warmup steps: . EPOCHS = 10 optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False) total_steps = len(train_data_loader) * EPOCHS scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(device) . How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning: . Batch size: 16, 32 | Learning rate (Adam): 5e-5, 3e-5, 2e-5 | Number of epochs: 2, 3, 4 | . We&#39;re going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy. . Let&#39;s continue with writing a helper function for training our model for one epoch: . def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples ): model = model.train() losses = [] correct_predictions = 0 for d in data_loader: input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) . Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We&#39;re avoiding exploding gradients by clipping the gradients of the model using clip_gradnorm. . Let&#39;s write another one that helps us evaluate the model on a given data loader: . def eval_model(model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) . Using those two, we can write our training loop. We&#39;ll also store the training history: . %%time history = defaultdict(list) best_accuracy = 0 for epoch in range(EPOCHS): print(f&#39;Epoch {epoch + 1}/{EPOCHS}&#39;) print(&#39;-&#39; * 10) train_acc, train_loss = train_epoch( model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train) ) print(f&#39;Train loss {train_loss} accuracy {train_acc}&#39;) val_acc, val_loss = eval_model( model, val_data_loader, loss_fn, device, len(df_val) ) print(f&#39;Val loss {val_loss} accuracy {val_acc}&#39;) print() history[&#39;train_acc&#39;].append(train_acc) history[&#39;train_loss&#39;].append(train_loss) history[&#39;val_acc&#39;].append(val_acc) history[&#39;val_loss&#39;].append(val_loss) if val_acc &gt; best_accuracy: torch.save(model.state_dict(), &#39;best_model_state.bin&#39;) best_accuracy = val_acc . Epoch 1/10 - Train loss 0.7330631300571541 accuracy 0.6653729447463129 Val loss 0.5767546480894089 accuracy 0.7776365946632783 Epoch 2/10 - Train loss 0.4158683338330777 accuracy 0.8420012701997036 Val loss 0.5365073362737894 accuracy 0.832274459974587 Epoch 3/10 - Train loss 0.24015077009679367 accuracy 0.922023851527768 Val loss 0.5074492372572422 accuracy 0.8716645489199493 Epoch 4/10 - Train loss 0.16012676668187295 accuracy 0.9546962105708843 Val loss 0.6009970247745514 accuracy 0.8703939008894537 Epoch 5/10 - Train loss 0.11209654617575301 accuracy 0.9675393409074872 Val loss 0.7367783848941326 accuracy 0.8742058449809403 Epoch 6/10 - Train loss 0.08572274737026433 accuracy 0.9764307388328276 Val loss 0.7251267762482166 accuracy 0.8843710292249047 Epoch 7/10 - Train loss 0.06132202987342602 accuracy 0.9833462705525369 Val loss 0.7083295831084251 accuracy 0.889453621346887 Epoch 8/10 - Train loss 0.050604159273123096 accuracy 0.9849693035071626 Val loss 0.753860274553299 accuracy 0.8907242693773825 Epoch 9/10 - Train loss 0.04373276197092931 accuracy 0.9862395032107826 Val loss 0.7506809896230697 accuracy 0.8919949174078781 Epoch 10/10 - Train loss 0.03768671146314381 accuracy 0.9880036694658105 Val loss 0.7431786182522774 accuracy 0.8932655654383737 CPU times: user 29min 54s, sys: 13min 28s, total: 43min 23s Wall time: 43min 43s . Note that we&#39;re storing the state of the best model, indicated by the highest validation accuracy. . Whoo, this took some time! We can look at the training vs validation accuracy: . plt.plot(history[&#39;train_acc&#39;], label=&#39;train accuracy&#39;) plt.plot(history[&#39;val_acc&#39;], label=&#39;validation accuracy&#39;) plt.title(&#39;Training history&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend() plt.ylim([0, 1]); . The training accuracy starts to approach 100% after 10 epochs or so. You might try to fine-tune the parameters a bit more, but this will be good enough for us. . Uncomment the next cell and download the pre-trained model that you have saved. . # model = SentimentClassifier(len(class_names)) # model.load_state_dict(torch.load(&#39;best_model_state.bin&#39;)) # model = model.to(device) . Evaluation . So how good is our model on predicting sentiment? Let&#39;s start by calculating the accuracy on the test data: . test_acc, _ = eval_model( model, test_data_loader, loss_fn, device, len(df_test) ) test_acc.item() . 0.883248730964467 . The accuracy is about 1% lower on the test set. Our model seems to generalize well. . We&#39;ll define a helper function to get the predictions from our model: . def get_predictions(model, data_loader): model = model.eval() review_texts = [] predictions = [] prediction_probs = [] real_values = [] with torch.no_grad(): for d in data_loader: texts = d[&quot;review_text&quot;] input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) probs = F.softmax(outputs, dim=1) review_texts.extend(texts) predictions.extend(preds) prediction_probs.extend(probs) real_values.extend(targets) predictions = torch.stack(predictions).cpu() prediction_probs = torch.stack(prediction_probs).cpu() real_values = torch.stack(real_values).cpu() return review_texts, predictions, prediction_probs, real_values . This is similar to the evaluation function, except that we&#39;re storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs): . y_review_texts, y_pred, y_pred_probs, y_test = get_predictions( model, test_data_loader ) . Let&#39;s have a look at the classification report . print(classification_report(y_test, y_pred, target_names=class_names)) . precision recall f1-score support negative 0.89 0.87 0.88 245 neutral 0.83 0.85 0.84 254 positive 0.92 0.93 0.92 289 accuracy 0.88 788 macro avg 0.88 0.88 0.88 788 weighted avg 0.88 0.88 0.88 788 . Looks like it is really hard to classify neutral (3 stars) reviews. And I can tell you from experience, looking at many reviews, those are hard to classify. . We&#39;ll continue with the confusion matrix: . def show_confusion_matrix(confusion_matrix): hmap = sns.heatmap(confusion_matrix, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;) hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha=&#39;right&#39;) hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha=&#39;right&#39;) plt.ylabel(&#39;True sentiment&#39;) plt.xlabel(&#39;Predicted sentiment&#39;); cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm, index=class_names, columns=class_names) show_confusion_matrix(df_cm) . This confirms that our model is having difficulty classifying neutral reviews. It mistakes those for negative and positive at a roughly equal frequency. . That&#39;s a good overview of the performance of our model. But let&#39;s have a look at an example from our test data: . idx = 2 review_text = y_review_texts[idx] true_sentiment = y_test[idx] pred_df = pd.DataFrame({ &#39;class_names&#39;: class_names, &#39;values&#39;: y_pred_probs[idx] }) . print(&quot; n&quot;.join(wrap(review_text))) print() print(f&#39;True sentiment: {class_names[true_sentiment]}&#39;) . I used to use Habitica, and I must say this is a great step up. I&#39;d like to see more social features, such as sharing tasks - only one person has to perform said task for it to be checked off, but only giving that person the experience and gold. Otherwise, the price for subscription is too steep, thus resulting in a sub-perfect score. I could easily justify $0.99/month or eternal subscription for $15. If that price could be met, as well as fine tuning, this would be easily worth 5 stars. True sentiment: neutral . Now we can look at the confidence of each sentiment of our model: . sns.barplot(x=&#39;values&#39;, y=&#39;class_names&#39;, data=pred_df, orient=&#39;h&#39;) plt.ylabel(&#39;sentiment&#39;) plt.xlabel(&#39;probability&#39;) plt.xlim([0, 1]); . Predicting on Raw Text . Let&#39;s use our model to predict the sentiment of some raw text: . review_text = &quot;I love completing my todos! Best app ever!!!&quot; . We have to use the tokenizer to encode the text: . encoded_review = tokenizer.encode_plus( review_text, max_length=MAX_LEN, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, ) . Let&#39;s get the predictions from our model: . input_ids = encoded_review[&#39;input_ids&#39;].to(device) attention_mask = encoded_review[&#39;attention_mask&#39;].to(device) output = model(input_ids, attention_mask) _, prediction = torch.max(output, dim=1) print(f&#39;Review text: {review_text}&#39;) print(f&#39;Sentiment : {class_names[prediction]}&#39;) . Review text: I love completing my todos! Best app ever!!! Sentiment : positive . Summary . Nice job! WE learned how to use BERT for sentiment analysis. You built a custom classifier using the Hugging Face library and trained it on our app reviews dataset! . Next, I&#39;ll learn how to optimize better and deploy our trained transformers using some webframework. . References . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | L11 Language Models - Alec Radford (OpenAI) | The Illustrated BERT, ELMo, and co. | BERT Fine-Tuning Tutorial with PyTorch | How to Fine-Tune BERT for Text Classification? | Huggingface Transformers | BERT Explained: State of the art language model for NLP | .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/transformers/text%20classification/deep%20learning/2021/02/10/Sentiment-Analysis.html",
            "relUrl": "/natural%20language%20processing/transformers/text%20classification/deep%20learning/2021/02/10/Sentiment-Analysis.html",
            "date": " • Feb 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "hey there . I’m Janmejay Bhoi, a passionate self-taught and aspiring for Data Science job roles from Odisha, India. My passion for Data Science/AI lies with dreaming up ideas and making them come true with elegant interfaces. I take great care in the experience, architecture, and code quality of the things I build. Currently i’m focusing on Machine Learning and Natural Language Processing. I am also an open-source enthusiast and maintainer. . Education: . Veer Surendra Sai University Of Technology, Burla,Odisha (Aug 2016- Sep 2020) | Jupiter Science College, Bhubaneswar,Odisha (May 2013- June 2015) | . Timeline: . Natural Language Processing Intern (July 2021 – Sep 2021) | Data Analyst Intern, National Aluminium Company Limited (May 2019 – June 2019) | . Skills: . Languages - . Python, C++ | . Database - . SQL, Spark | . Machine Learning - . Regression And Classification | Ensembel Learning (Bagging &amp; Boosting) | Exploratory data analysis | Dimensionality Reduction | Clusteing | Statistical Analysis | . Deep Learning (NLP) - . Text Mining | Sentiment Analysis | Text Summarization | Named Entity Recognition | LSTM, GRU, Conv1D | Transformers | Embedding | . Libraries - . Numpy, Pandas | NLTK, SpaCy | Stats Model | Hugging face | TenssorFlow, PyTorch | Scikit Learn | FastText | TextHero | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://janmejaybhoi.github.io/w/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://janmejaybhoi.github.io/w/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}