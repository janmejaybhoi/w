{
  
    
        "post0": {
            "title": "EDA & forecasting of Covid-19 Data ",
            "content": "How to download data from kaggle . # ! pip install -q kaggle . # files.upload() . # ! cp kaggle.json ~/.kaggle/ # ! chmod 600 ~/.kaggle/kaggle.json . . . Loading of Notebook might take some time because of Plotly visualizations. Kindly be patient! . What is COVID-19? . COVID-19 is a respiratory illness caused by a new virus. Symptoms include fever, coughing, sore throat and shortness of breath. The virus can spread from person to person, but good hygiene can prevent infection. . Related Information about COVID-19 . COVID-19 may not be fatal but it spreads faster than other diseases, like common cold. Every virus has Basic Reproduction number (R0) which implies how many people will get the disease from the infected person. As per inital reseach work R0 of COVID-19 is 2.7. . Currently the goal of all scientists around the world is to &quot;Flatten the Curve&quot;. COVID-19 currently has exponential growth rate around the world which we will be seeing in the notebook ahead. Flattening the Curve typically implies even if the number of Confirmed Cases are increasing but the distribution of those cases should be over longer timestamp. To put it in simple words if say suppose COVID-19 is going infect 100K people then those many people should be infected in 1 year but not in a month. . The sole reason to Flatten the Curve is to reudce the load on the Medical Systems so as to increase the focus of Research to find the Medicine for the disease. . Every Pandemic has four stages: . Stage 1: Confirmed Cases come from other countries . Stage 2: Local Transmission Begins . Stage 3: Communities impacted with local transimission . Stage 4: Significant Transmission with no end in sight . Italy, USA, UK and France are the two countries which are currently in Stage 4 While India is in on the edge of Stage 3. . Other ways to tackle the disease like Corona other than Travel Ban, Cross-Border shutdown, Ban on immigrants are Testing, Contact Tracing and Quarantine. . Objective of the Notebook . Objective of this notebook is to study COVID-19 outbreak with the help of some basic visualizations techniques. Comparison of China where the COVID-19 originally originated from with the Rest of the World. Perform predictions and Time Series forecasting in order to study the impact and spread of the COVID-19 in comming days. . Let&#39;s get Started . Importing required Python Packages and Libraries . !pip install pmdarima . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting pmdarima Downloading pmdarima-2.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB) |████████████████████████████████| 1.8 MB 8.5 MB/s Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.24.3) Collecting statsmodels&gt;=0.13.2 Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB) |████████████████████████████████| 9.8 MB 46.2 MB/s Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.3.5) Requirement already satisfied: setuptools!=50.0.0,&gt;=38.6.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (57.4.0) Requirement already satisfied: Cython!=0.29.18,!=0.29.31,&gt;=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.29.32) Requirement already satisfied: scikit-learn&gt;=0.22 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.0.2) Requirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.7.3) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.1.0) Requirement already satisfied: numpy&gt;=1.21 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.21.6) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;pmdarima) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;pmdarima) (2022.2.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.19-&gt;pmdarima) (1.15.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.22-&gt;pmdarima) (3.1.0) Requirement already satisfied: patsy&gt;=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels&gt;=0.13.2-&gt;pmdarima) (0.5.2) Requirement already satisfied: packaging&gt;=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels&gt;=0.13.2-&gt;pmdarima) (21.3) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=21.3-&gt;statsmodels&gt;=0.13.2-&gt;pmdarima) (3.0.9) Installing collected packages: statsmodels, pmdarima Attempting uninstall: statsmodels Found existing installation: statsmodels 0.12.2 Uninstalling statsmodels-0.12.2: Successfully uninstalled statsmodels-0.12.2 Successfully installed pmdarima-2.0.1 statsmodels-0.13.2 . %%capture !pip3 install prophet . import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import matplotlib.pyplot as plt import seaborn as sns #!pip install plotly import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots import numpy as np import datetime as dt from datetime import timedelta from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score,silhouette_samples from sklearn.linear_model import LinearRegression,Ridge,Lasso from sklearn.svm import SVR from sklearn.metrics import mean_squared_error,r2_score import statsmodels.api as sm from statsmodels.tsa.api import Holt,SimpleExpSmoothing,ExponentialSmoothing from prophet import Prophet from sklearn.preprocessing import PolynomialFeatures from statsmodels.tsa.stattools import adfuller from pmdarima.arima import auto_arima # from pyramid.arima import auto_arima std=StandardScaler() #pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.6f&#39; % x) . !gdown --id 1N7yV6DLbfwWyioq3JB4GLYt0pVIdcteH . /usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don&#39;t need to pass it anymore to use a file ID. category=FutureWarning, Downloading... From: https://drive.google.com/uc?id=1N7yV6DLbfwWyioq3JB4GLYt0pVIdcteH To: /content/Covid19.zip 100% 8.93M/8.93M [00:00&lt;00:00, 32.4MB/s] . !unzip &quot;/content/Covid19.zip&quot; . Archive: /content/Covid19.zip inflating: covid_19_data.csv inflating: time_series_covid_19_confirmed.csv inflating: time_series_covid_19_confirmed_US.csv inflating: time_series_covid_19_deaths.csv inflating: time_series_covid_19_deaths_US.csv inflating: time_series_covid_19_recovered.csv . covid=pd.read_csv(&quot;/content/covid_19_data.csv&quot;) covid.head() . SNo ObservationDate Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 1 | 01/22/2020 | Anhui | Mainland China | 1/22/2020 17:00 | 1.0 | 0.0 | 0.0 | . 1 2 | 01/22/2020 | Beijing | Mainland China | 1/22/2020 17:00 | 14.0 | 0.0 | 0.0 | . 2 3 | 01/22/2020 | Chongqing | Mainland China | 1/22/2020 17:00 | 6.0 | 0.0 | 0.0 | . 3 4 | 01/22/2020 | Fujian | Mainland China | 1/22/2020 17:00 | 1.0 | 0.0 | 0.0 | . 4 5 | 01/22/2020 | Gansu | Mainland China | 1/22/2020 17:00 | 0.0 | 0.0 | 0.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(&quot;Size/Shape of the dataset: &quot;,covid.shape) print(&quot;Checking for null values: n&quot;,covid.isnull().sum()) print(&quot;Checking Data-type of each column: n&quot;,covid.dtypes) . Size/Shape of the dataset: (306429, 8) Checking for null values: SNo 0 ObservationDate 0 Province/State 78100 Country/Region 0 Last Update 0 Confirmed 0 Deaths 0 Recovered 0 dtype: int64 Checking Data-type of each column: SNo int64 ObservationDate object Province/State object Country/Region object Last Update object Confirmed float64 Deaths float64 Recovered float64 dtype: object . covid.drop([&quot;SNo&quot;],1,inplace=True) . covid[&quot;ObservationDate&quot;]=pd.to_datetime(covid[&quot;ObservationDate&quot;]) . grouped_country=covid.groupby([&quot;Country/Region&quot;,&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) . grouped_country[&quot;Active Cases&quot;]=grouped_country[&quot;Confirmed&quot;]-grouped_country[&quot;Recovered&quot;]-grouped_country[&quot;Deaths&quot;] grouped_country[&quot;log_confirmed&quot;]=np.log(grouped_country[&quot;Confirmed&quot;]) grouped_country[&quot;log_active&quot;]=np.log(grouped_country[&quot;Active Cases&quot;]) . Datewise analysis . datewise=covid.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise[&quot;Days Since&quot;]=datewise.index-datewise.index.min() . print(&quot;Basic Information&quot;) print(&quot;Totol number of countries with Disease Spread: &quot;,len(covid[&quot;Country/Region&quot;].unique())) print(&quot;Total number of Confirmed Cases around the World: &quot;,datewise[&quot;Confirmed&quot;].iloc[-1]) print(&quot;Total number of Recovered Cases around the World: &quot;,datewise[&quot;Recovered&quot;].iloc[-1]) print(&quot;Total number of Deaths Cases around the World: &quot;,datewise[&quot;Deaths&quot;].iloc[-1]) print(&quot;Total number of Active Cases around the World: &quot;,(datewise[&quot;Confirmed&quot;].iloc[-1]-datewise[&quot;Recovered&quot;].iloc[-1]-datewise[&quot;Deaths&quot;].iloc[-1])) print(&quot;Total number of Closed Cases around the World: &quot;,datewise[&quot;Recovered&quot;].iloc[-1]+datewise[&quot;Deaths&quot;].iloc[-1]) print(&quot;Approximate number of Confirmed Cases per Day around the World: &quot;,np.round(datewise[&quot;Confirmed&quot;].iloc[-1]/datewise.shape[0])) print(&quot;Approximate number of Recovered Cases per Day around the World: &quot;,np.round(datewise[&quot;Recovered&quot;].iloc[-1]/datewise.shape[0])) print(&quot;Approximate number of Death Cases per Day around the World: &quot;,np.round(datewise[&quot;Deaths&quot;].iloc[-1]/datewise.shape[0])) print(&quot;Approximate number of Confirmed Cases per hour around the World: &quot;,np.round(datewise[&quot;Confirmed&quot;].iloc[-1]/((datewise.shape[0])*24))) print(&quot;Approximate number of Recovered Cases per hour around the World: &quot;,np.round(datewise[&quot;Recovered&quot;].iloc[-1]/((datewise.shape[0])*24))) print(&quot;Approximate number of Death Cases per hour around the World: &quot;,np.round(datewise[&quot;Deaths&quot;].iloc[-1]/((datewise.shape[0])*24))) print(&quot;Number of Confirmed Cases in last 24 hours: &quot;,datewise[&quot;Confirmed&quot;].iloc[-1]-datewise[&quot;Confirmed&quot;].iloc[-2]) print(&quot;Number of Recovered Cases in last 24 hours: &quot;,datewise[&quot;Recovered&quot;].iloc[-1]-datewise[&quot;Recovered&quot;].iloc[-2]) print(&quot;Number of Death Cases in last 24 hours: &quot;,datewise[&quot;Deaths&quot;].iloc[-1]-datewise[&quot;Deaths&quot;].iloc[-2]) . Basic Information Totol number of countries with Disease Spread: 229 Total number of Confirmed Cases around the World: 169951560.0 Total number of Recovered Cases around the World: 107140669.0 Total number of Deaths Cases around the World: 3533619.0 Total number of Active Cases around the World: 59277272.0 Total number of Closed Cases around the World: 110674288.0 Approximate number of Confirmed Cases per Day around the World: 344031.0 Approximate number of Recovered Cases per Day around the World: 216884.0 Approximate number of Death Cases per Day around the World: 7153.0 Approximate number of Confirmed Cases per hour around the World: 14335.0 Approximate number of Recovered Cases per hour around the World: 9037.0 Approximate number of Death Cases per hour around the World: 298.0 Number of Confirmed Cases in last 24 hours: 480835.0 Number of Recovered Cases in last 24 hours: 507600.0 Number of Death Cases in last 24 hours: 10502.0 . fig=px.bar(x=datewise.index,y=datewise[&quot;Confirmed&quot;]-datewise[&quot;Recovered&quot;]-datewise[&quot;Deaths&quot;]) fig.update_layout(title=&quot;Distribution of Number of Active Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,) fig.show() . . . Active Cases = Number of Confirmed Cases - Number of Recovered Cases - Number of Death Cases . Increase in number of Active Cases is probably an indication of Recovered case or Death case number is dropping in comparison to number of Confirmed Cases drastically. Will look for the conclusive evidence for the same in the notebook ahead. . fig=px.bar(x=datewise.index,y=datewise[&quot;Recovered&quot;]+datewise[&quot;Deaths&quot;]) fig.update_layout(title=&quot;Distribution of Number of Closed Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;) fig.show() . . . Closed Cases = Number of Recovered Cases + Number of Death Cases . Increase in number of Closed classes imply either more patients are getting recovered from the disease or more pepole are dying because of COVID-19 . datewise[&quot;WeekOfYear&quot;]=datewise.index.weekofyear week_num=[] weekwise_confirmed=[] weekwise_recovered=[] weekwise_deaths=[] w=1 for i in list(datewise[&quot;WeekOfYear&quot;].unique()): weekwise_confirmed.append(datewise[datewise[&quot;WeekOfYear&quot;]==i][&quot;Confirmed&quot;].iloc[-1]) weekwise_recovered.append(datewise[datewise[&quot;WeekOfYear&quot;]==i][&quot;Recovered&quot;].iloc[-1]) weekwise_deaths.append(datewise[datewise[&quot;WeekOfYear&quot;]==i][&quot;Deaths&quot;].iloc[-1]) week_num.append(w) w=w+1 fig=go.Figure() fig.add_trace(go.Scatter(x=week_num, y=weekwise_confirmed, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=week_num, y=weekwise_recovered, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=week_num, y=weekwise_deaths, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Death Cases&#39;)) fig.update_layout(title=&quot;Weekly Growth of different types of Cases in India&quot;, xaxis_title=&quot;Week Number&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(18,5)) sns.barplot(x=week_num,y=pd.Series(weekwise_confirmed).diff().fillna(0),ax=ax1) sns.barplot(x=week_num,y=pd.Series(weekwise_deaths).diff().fillna(0),ax=ax2) ax1.set_xlabel(&quot;Week Number&quot;) ax2.set_xlabel(&quot;Week Number&quot;) ax1.set_ylabel(&quot;Number of Confirmed Cases&quot;) ax2.set_ylabel(&quot;Number of Death Cases&quot;) ax1.set_title(&quot;Weekly increase in Number of Confirmed Cases&quot;) ax2.set_title(&quot;Weekly increase in Number of Death Cases&quot;) . Text(0.5, 1.0, &#39;Weekly increase in Number of Death Cases&#39;) . 32nd week id currently going on. . The death toll was low in 14th week, as it was expected to rise looking at the trend of infection of death trend of previous few weeks. . Number of Death cases were consistently dropping since 14th week, upto 19th week. After which it&#39;s again showing a spike for two consecutive weeks. . We are somehow able to reduce the Death Numbers or maybe able to control it somehow, but new infections are increasing with considerable speed recording 800k+ cases in 21st week which is a staggering number. . The number infections are increasing every week, recording 1.2M+ Confirmed Cases in 24th week. 25th Week has recorded another peak in number of Confirmed Cases (1.5M+) . The infection rate is increasing with every passing week. . Growth rate of Confirmed, Recovered and Death Cases . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;, name=&#39;Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Recovered&quot;], mode=&#39;lines+markers&#39;, name=&#39;Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Deaths&quot;], mode=&#39;lines+markers&#39;, name=&#39;Death Cases&#39;)) fig.update_layout(title=&quot;Growth of different types of cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . Moratality and Recovery Rate analysis around the World . datewise[&quot;Mortality Rate&quot;]=(datewise[&quot;Deaths&quot;]/datewise[&quot;Confirmed&quot;])*100 datewise[&quot;Recovery Rate&quot;]=(datewise[&quot;Recovered&quot;]/datewise[&quot;Confirmed&quot;])*100 datewise[&quot;Active Cases&quot;]=datewise[&quot;Confirmed&quot;]-datewise[&quot;Recovered&quot;]-datewise[&quot;Deaths&quot;] datewise[&quot;Closed Cases&quot;]=datewise[&quot;Recovered&quot;]+datewise[&quot;Deaths&quot;] print(&quot;Average Mortality Rate&quot;,datewise[&quot;Mortality Rate&quot;].mean()) print(&quot;Median Mortality Rate&quot;,datewise[&quot;Mortality Rate&quot;].median()) print(&quot;Average Recovery Rate&quot;,datewise[&quot;Recovery Rate&quot;].mean()) print(&quot;Median Recovery Rate&quot;,datewise[&quot;Recovery Rate&quot;].median()) #Plotting Mortality and Recovery Rate fig = make_subplots(rows=2, cols=1, subplot_titles=(&quot;Recovery Rate&quot;, &quot;Mortatlity Rate&quot;)) fig.add_trace( go.Scatter(x=datewise.index, y=(datewise[&quot;Recovered&quot;]/datewise[&quot;Confirmed&quot;])*100,name=&quot;Recovery Rate&quot;), row=1, col=1 ) fig.add_trace( go.Scatter(x=datewise.index, y=(datewise[&quot;Deaths&quot;]/datewise[&quot;Confirmed&quot;])*100,name=&quot;Mortality Rate&quot;), row=2, col=1 ) fig.update_layout(height=1000,legend=dict(x=-0.1,y=1.2,traceorder=&quot;normal&quot;)) fig.update_xaxes(title_text=&quot;Date&quot;, row=1, col=1) fig.update_yaxes(title_text=&quot;Recovery Rate&quot;, row=1, col=1) fig.update_xaxes(title_text=&quot;Date&quot;, row=1, col=2) fig.update_yaxes(title_text=&quot;Mortality Rate&quot;, row=1, col=2) fig.show() . Average Mortality Rate 3.398557417508881 Median Mortality Rate 2.772038814120292 Average Recovery Rate 51.148201824468615 Median Recovery Rate 56.426751740200025 . . . Mortality rate = (Number of Death Cases / Number of Confirmed Cases) x 100 . Recovery Rate= (Number of Recoverd Cases / Number of Confirmed Cases) x 100 . Mortality rate is showing a considerable for a pretty long time, which is positive sign . Recovery Rate has started to pick up again which is a good sign, another supportive reason to why number of Closed Cases are increasing . print(&quot;Average increase in number of Confirmed Cases every day: &quot;,np.round(datewise[&quot;Confirmed&quot;].diff().fillna(0).mean())) print(&quot;Average increase in number of Recovered Cases every day: &quot;,np.round(datewise[&quot;Recovered&quot;].diff().fillna(0).mean())) print(&quot;Average increase in number of Deaths Cases every day: &quot;,np.round(datewise[&quot;Deaths&quot;].diff().fillna(0).mean())) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;].diff().fillna(0),mode=&#39;lines+markers&#39;, name=&#39;Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Recovered&quot;].diff().fillna(0),mode=&#39;lines+markers&#39;, name=&#39;Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Deaths&quot;].diff().fillna(0),mode=&#39;lines+markers&#39;, name=&#39;Death Cases&#39;)) fig.update_layout(title=&quot;Daily increase in different types of Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . Average increase in number of Confirmed Cases every day: 344030.0 Average increase in number of Recovered Cases every day: 216884.0 Average increase in number of Deaths Cases every day: 7153.0 . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;].diff().rolling(window=7).mean(),mode=&#39;lines+markers&#39;, name=&#39;Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Recovered&quot;].diff().rolling(window=7).mean(),mode=&#39;lines+markers&#39;, name=&#39;Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Deaths&quot;].diff().rolling(window=7).mean(),mode=&#39;lines+markers&#39;, name=&#39;Death Cases&#39;)) fig.update_layout(title=&quot;7 Days Rolling Mean of Daily Increase of Confirmed, Recovered and Death Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . Growth Factor . Growth factor is the factor by which a quantity multiplies itself over time. The formula used is: . Formula: Every day&#39;s new (Confirmed,Recovered,Deaths) / new (Confirmed,Recovered,Deaths) on the previous day. . A growth factor above 1 indicates an increase correspoding cases. . A growth factor above 1 but trending downward is a positive sign, whereas a growth factor constantly above 1 is the sign of exponential growth. . A growth factor constant at 1 indicates there is no change in any kind of cases. . print(&quot;Average growth factor of number of Confirmed Cases: &quot;,(datewise[&quot;Confirmed&quot;]/datewise[&quot;Confirmed&quot;].shift()).mean()) print(&quot;Median growth factor of number of Confirmed Cases: &quot;,(datewise[&quot;Confirmed&quot;]/datewise[&quot;Confirmed&quot;].shift()).median()) print(&quot;Average growth factor of number of Recovered Cases: &quot;,(datewise[&quot;Recovered&quot;]/datewise[&quot;Recovered&quot;].shift()).mean()) print(&quot;Median growth factor of number of Recovered Cases: &quot;,(datewise[&quot;Recovered&quot;]/datewise[&quot;Recovered&quot;].shift()).median()) print(&quot;Average growth factor of number of Death Cases: &quot;,(datewise[&quot;Deaths&quot;]/datewise[&quot;Deaths&quot;].shift()).mean()) print(&quot;Median growth factor of number of Death Cases: &quot;,(datewise[&quot;Deaths&quot;]/datewise[&quot;Deaths&quot;].shift()).median()) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;]/datewise[&quot;Confirmed&quot;].shift(), mode=&#39;lines&#39;, name=&#39;Growth Factor of Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Recovered&quot;]/datewise[&quot;Recovered&quot;].shift(), mode=&#39;lines&#39;, name=&#39;Growth Factor of Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Deaths&quot;]/datewise[&quot;Deaths&quot;].shift(), mode=&#39;lines&#39;, name=&#39;Growth Factor of Death Cases&#39;)) fig.update_layout(title=&quot;Datewise Growth Factor of different types of cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Growth Factor&quot;, legend=dict(x=0,y=-0.4,traceorder=&quot;normal&quot;)) fig.show() . Average growth factor of number of Confirmed Cases: 1.0281591322080432 Median growth factor of number of Confirmed Cases: 1.0105328040968438 Average growth factor of number of Recovered Cases: 1.033783342773454 Median growth factor of number of Recovered Cases: 1.0112782082196978 Average growth factor of number of Death Cases: 1.027312583713661 Median growth factor of number of Death Cases: 1.0071398973639754 . . . Growth Factor for Active and Closed Cases . Growth factor is the factor by which a quantity multiplies itself over time. The formula used is: . Formula: Every day&#39;s new (Active and Closed Cases) / new (Active and Closed Cases) on the previous day. . A growth factor above 1 indicates an increase correspoding cases. . A growth factor above 1 but trending downward is a positive sign. . A growth factor constant at 1 indicates there is no change in any kind of cases. . A growth factor below 1 indicates real positive sign implying more patients are getting recovered or dying as compared to the Confirmed Cases. . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=(datewise[&quot;Confirmed&quot;]-datewise[&quot;Recovered&quot;]-datewise[&quot;Deaths&quot;])/(datewise[&quot;Confirmed&quot;]-datewise[&quot;Recovered&quot;]-datewise[&quot;Deaths&quot;]).shift(), mode=&#39;lines&#39;, name=&#39;Growth Factor of Active Cases&#39;)) fig.add_trace(go.Scatter(x=datewise.index, y=(datewise[&quot;Recovered&quot;]+datewise[&quot;Deaths&quot;])/(datewise[&quot;Recovered&quot;]+datewise[&quot;Deaths&quot;]).shift(), mode=&#39;lines&#39;, name=&#39;Growth Factor of Closed Cases&#39;)) fig.update_layout(title=&quot;Datewise Growth Factor of Active and Closed Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Growth Factor&quot;, legend=dict(x=0,y=-0.4,traceorder=&quot;normal&quot;)) fig.show() . . . Growth Factor constantly above 1 is an clear indication of Exponential increase in all form of cases. . Rate of Doubling for Confirmed Cases around the World . c=560 double_days=[] C=[] while(1): double_days.append(datewise[datewise[&quot;Confirmed&quot;]&lt;=c].iloc[[-1]][&quot;Days Since&quot;][0]) C.append(c) c=c*2 if(c&lt;datewise[&quot;Confirmed&quot;].max()): continue else: break . doubling_rate=pd.DataFrame(list(zip(C,double_days)),columns=[&quot;No. of cases&quot;,&quot;Days since first Case&quot;]) doubling_rate[&quot;Number of days for doubling&quot;]=doubling_rate[&quot;Days since first Case&quot;].diff().fillna(doubling_rate[&quot;Days since first Case&quot;]) doubling_rate . No. of cases Days since first Case Number of days for doubling . 0 560 | 0 days | 0 days | . 1 1120 | 2 days | 2 days | . 2 2240 | 4 days | 2 days | . 3 4480 | 5 days | 1 days | . 4 8960 | 8 days | 3 days | . 5 17920 | 11 days | 3 days | . 6 35840 | 16 days | 5 days | . 7 71680 | 25 days | 9 days | . 8 143360 | 50 days | 25 days | . 9 286720 | 58 days | 8 days | . 10 573440 | 64 days | 6 days | . 11 1146880 | 72 days | 8 days | . 12 2293760 | 86 days | 14 days | . 13 4587520 | 114 days | 28 days | . 14 9175040 | 152 days | 38 days | . 15 18350080 | 194 days | 42 days | . 16 36700160 | 260 days | 66 days | . 17 73400320 | 327 days | 67 days | . 18 146800640 | 458 days | 131 days | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Doubling Rate is fluctuating very much, which ideally supposed to increase if we are successfully faltening the curve. . Number of days requried for increase in Confirmed Cases by 300K . c1=100000 days_300k=[] C1=[] while(1): days_300k.append(datewise[datewise[&quot;Confirmed&quot;]&lt;=c1].iloc[[-1]][&quot;Days Since&quot;][0]) C1.append(c1) c1=c1+300000 if(c1&lt;datewise[&quot;Confirmed&quot;].max()): continue else: break . rate_300k=pd.DataFrame(list(zip(C1,days_300k)),columns=[&quot;No. of Cases&quot;,&quot;Days Since first Case&quot;]) rate_300k[&quot;Days requried for rise of 300K&quot;]=rate_300k[&quot;Days Since first Case&quot;].diff().fillna(rate_300k[&quot;Days Since first Case&quot;].iloc[[0]][0]) fig=go.Figure() fig.add_trace(go.Scatter(x=rate_300k[&quot;No. of Cases&quot;], y=rate_300k[&quot;Days requried for rise of 300K&quot;].dt.days, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Confirmed Cases&#39;)) fig.update_layout(title=&quot;Number of Days required for increase in number of cases by 300K&quot;, xaxis_title=&quot;Number of Cases&quot;,yaxis_title=&quot;Number of Days&quot;) fig.show() . . . It&#39;s hardly taking a day or two for rise in cases by 300k, which is pretty much a clear indication that we are still not able to &quot;Flatten the curve&quot; . Countrywise Analysis . countrywise=covid[covid[&quot;ObservationDate&quot;]==covid[&quot;ObservationDate&quot;].max()].groupby([&quot;Country/Region&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}).sort_values([&quot;Confirmed&quot;],ascending=False) countrywise[&quot;Mortality&quot;]=(countrywise[&quot;Deaths&quot;]/countrywise[&quot;Confirmed&quot;])*100 countrywise[&quot;Recovery&quot;]=(countrywise[&quot;Recovered&quot;]/countrywise[&quot;Confirmed&quot;])*100 . country_last_24_confirmed=[] country_last_24_recovered=[] country_last_24_deaths=[] for country in countrywise.index: country_last_24_confirmed.append((grouped_country.loc[country].iloc[-1]-grouped_country.loc[country].iloc[-2])[&quot;Confirmed&quot;]) country_last_24_recovered.append((grouped_country.loc[country].iloc[-1]-grouped_country.loc[country].iloc[-2])[&quot;Recovered&quot;]) country_last_24_deaths.append((grouped_country.loc[country].iloc[-1]-grouped_country.loc[country].iloc[-2])[&quot;Deaths&quot;]) . Last_24_Hours_country=pd.DataFrame(list(zip(countrywise.index,country_last_24_confirmed,country_last_24_recovered,country_last_24_deaths)), columns=[&quot;Country Name&quot;,&quot;Last 24 Hours Confirmed&quot;,&quot;Last 24 Hours Recovered&quot;,&quot;Last 24 Hours Deaths&quot;]) . Top_15_Confirmed_24hr=Last_24_Hours_country.sort_values([&quot;Last 24 Hours Confirmed&quot;],ascending=False).head(15) Top_15_Recoverd_24hr=Last_24_Hours_country.sort_values([&quot;Last 24 Hours Recovered&quot;],ascending=False).head(15) Top_15_Deaths_24hr=Last_24_Hours_country.sort_values([&quot;Last 24 Hours Deaths&quot;],ascending=False).head(15) fig, (ax1, ax2, ax3) = plt.subplots(3, 1,figsize=(10,20)) sns.barplot(x=Top_15_Confirmed_24hr[&quot;Last 24 Hours Confirmed&quot;],y=Top_15_Confirmed_24hr[&quot;Country Name&quot;],ax=ax1) ax1.set_title(&quot;Top 15 Countries with Highest Number of Confirmed Cases in Last 24 Hours&quot;) sns.barplot(x=Top_15_Recoverd_24hr[&quot;Last 24 Hours Recovered&quot;],y=Top_15_Recoverd_24hr[&quot;Country Name&quot;],ax=ax2) ax2.set_title(&quot;Top 15 Countries with Highest Number of Recovered Cases in Last 24 Hours&quot;) sns.barplot(x=Top_15_Deaths_24hr[&quot;Last 24 Hours Deaths&quot;],y=Top_15_Deaths_24hr[&quot;Country Name&quot;],ax=ax3) ax3.set_title(&quot;Top 15 Countries with Highest Number of Death Cases in Last 24 Hours&quot;) . Text(0.5, 1.0, &#39;Top 15 Countries with Highest Number of Death Cases in Last 24 Hours&#39;) . Last_24_Hours_country[&quot;Proportion of Confirmed&quot;]=(Last_24_Hours_country[&quot;Last 24 Hours Confirmed&quot;]/(datewise[&quot;Confirmed&quot;].iloc[-1]-datewise[&quot;Confirmed&quot;].iloc[-2]))*100 Last_24_Hours_country[&quot;Proportion of Recovered&quot;]=(Last_24_Hours_country[&quot;Last 24 Hours Recovered&quot;]/(datewise[&quot;Recovered&quot;].iloc[-1]-datewise[&quot;Recovered&quot;].iloc[-2]))*100 Last_24_Hours_country[&quot;Proportion of Deaths&quot;]=(Last_24_Hours_country[&quot;Last 24 Hours Deaths&quot;]/(datewise[&quot;Deaths&quot;].iloc[-1]-datewise[&quot;Deaths&quot;].iloc[-2]))*100 . Proportion of Countries in Confirmed, Recovered and Death Cases . Last_24_Hours_country[[&quot;Country Name&quot;,&quot;Proportion of Confirmed&quot;,&quot;Proportion of Recovered&quot;,&quot;Proportion of Deaths&quot;]].sort_values([&quot;Proportion of Confirmed&quot;],ascending=False).style.background_gradient(cmap=&quot;Reds&quot;) . &nbsp; Country Name Proportion of Confirmed Proportion of Recovered Proportion of Deaths . 1 India | 34.430314 | 54.434397 | 32.946106 | . 2 Brazil | 16.569093 | 0.694050 | 19.158256 | . 8 Argentina | 6.206079 | 7.018125 | 3.951628 | . 11 Colombia | 4.262169 | 3.873325 | 5.141878 | . 0 US | 2.490667 | 0.000000 | 3.266045 | . 3 France | 2.397288 | 0.134358 | 0.628452 | . 5 Russia | 1.903980 | 1.801812 | 3.761188 | . 39 Malaysia | 1.875903 | 1.088849 | 0.933156 | . 22 Chile | 1.708694 | 1.515957 | 1.133118 | . 4 Turkey | 1.592230 | 2.202522 | 1.304513 | . 23 Philippines | 1.544813 | 1.463357 | 1.485431 | . 12 Iran | 1.478054 | 2.911939 | 1.647305 | . 17 Indonesia | 1.365333 | 1.067179 | 1.542563 | . 61 Uruguay | 1.242422 | 0.721040 | 0.552276 | . 16 Peru | 1.144051 | 0.931442 | 1.542563 | . 82 Thailand | 0.998887 | 0.000000 | 0.323748 | . 20 South Africa | 0.939823 | 0.524823 | 0.666540 | . 9 Germany | 0.938576 | 1.388889 | 0.504666 | . 40 Nepal | 0.896565 | 1.197794 | 1.104552 | . 33 Japan | 0.749529 | 1.420410 | 0.866502 | . 18 Netherlands | 0.704192 | 0.005910 | 0.076176 | . 7 Italy | 0.696289 | 1.491135 | 0.790326 | . 71 Bahrain | 0.680691 | 0.499015 | 0.152352 | . 24 Iraq | 0.677363 | 1.068755 | 0.219006 | . 6 UK | 0.674036 | 0.000197 | 0.066654 | . 15 Ukraine | 0.671748 | 2.029748 | 1.542563 | . 78 Sri Lanka | 0.599374 | 0.399724 | 0.000000 | . 14 Mexico | 0.566722 | 0.220843 | 3.646924 | . 28 Pakistan | 0.560899 | 0.516154 | 0.533232 | . 53 Paraguay | 0.505163 | 0.438731 | 0.904590 | . 21 Canada | 0.471264 | 0.764972 | 0.257094 | . 51 Bolivia | 0.445891 | 0.298660 | 0.552276 | . 27 Belgium | 0.386827 | 0.000000 | 0.104742 | . 38 United Arab Emirates | 0.376844 | 0.350473 | 0.047610 | . 45 Ecuador | 0.327763 | 0.000000 | 0.733194 | . 47 Greece | 0.311333 | 0.000000 | 0.276138 | . 54 Tunisia | 0.295736 | 0.295902 | 0.580842 | . 72 Venezuela | 0.269531 | 0.229708 | 0.190440 | . 60 Dominican Republic | 0.257469 | 0.038416 | 0.047610 | . 84 Cuba | 0.247070 | 0.285461 | 0.095220 | . 59 Kuwait | 0.235840 | 0.225374 | 0.038088 | . 65 Egypt | 0.232720 | 0.240544 | 0.485622 | . 43 Saudi Arabia | 0.230017 | 0.250985 | 0.133308 | . 105 Maldives | 0.220866 | 0.270095 | 0.019044 | . 62 Denmark | 0.217122 | 0.171395 | 0.038088 | . 32 Bangladesh | 0.216914 | 0.233846 | 0.361836 | . 48 Belarus | 0.206308 | 0.238968 | 0.095220 | . 55 Georgia | 0.205060 | 0.116233 | 0.114264 | . 103 Afghanistan | 0.204020 | 0.031915 | 0.171396 | . 99 Cameroon | 0.196949 | 0.000000 | 0.047610 | . 68 Guatemala | 0.188838 | 0.150709 | 0.257094 | . 13 Poland | 0.161386 | 0.386919 | 1.190249 | . 112 Uganda | 0.145580 | 0.000000 | 0.009522 | . 29 Portugal | 0.126655 | 0.100473 | 0.000000 | . 123 Cambodia | 0.122287 | 0.081757 | 0.066654 | . 50 Panama | 0.119376 | 0.095745 | 0.038088 | . 63 Lithuania | 0.108145 | 0.342790 | 0.095220 | . 154 Vietnam | 0.106481 | 0.000000 | 0.000000 | . 70 Honduras | 0.104194 | 0.032703 | 0.114264 | . 37 Austria | 0.103570 | 0.168834 | 0.038088 | . 151 Taiwan | 0.102114 | 0.000000 | 0.199962 | . 85 South Korea | 0.099618 | 0.126478 | 0.057132 | . 31 Hungary | 0.096707 | 0.607171 | 0.257094 | . 19 Czech Republic | 0.094419 | 0.490544 | 0.123786 | . 79 Kenya | 0.092339 | 0.022656 | 0.161874 | . 42 Morocco | 0.085268 | 0.058511 | 0.028566 | . 108 Namibia | 0.081317 | 0.037037 | 0.104742 | . 128 Trinidad and Tobago | 0.079237 | 0.061269 | 0.114264 | . 92 Kyrgyzstan | 0.074038 | 0.090229 | 0.085698 | . 69 Slovenia | 0.073414 | 0.097124 | 0.019044 | . 52 Croatia | 0.070294 | 0.097912 | 0.095220 | . 95 Zambia | 0.066759 | 0.029748 | 0.009522 | . 86 Latvia | 0.063639 | 0.235028 | 0.104742 | . 35 Serbia | 0.056984 | 0.000000 | 0.095220 | . 58 West Bank and Gaza | 0.055944 | 0.078605 | 0.028566 | . 25 Romania | 0.055112 | 0.159574 | 0.695106 | . 93 Uzbekistan | 0.054904 | 0.054965 | 0.009522 | . 89 Algeria | 0.053657 | 0.036840 | 0.047610 | . 64 Ethiopia | 0.053241 | 0.182033 | 0.038088 | . 90 Norway | 0.050953 | 0.000000 | 0.000000 | . 41 Lebanon | 0.050745 | 0.261820 | 0.066654 | . 118 Angola | 0.049081 | 0.013593 | 0.076176 | . 134 Suriname | 0.043674 | 0.024823 | 0.057132 | . 56 Azerbaijan | 0.034107 | 0.082545 | 0.066654 | . 74 Qatar | 0.032444 | 0.066391 | 0.019044 | . 121 Cabo Verde | 0.031404 | 0.030930 | 0.019044 | . 155 Timor-Leste | 0.027036 | 0.000000 | 0.009522 | . 88 Estonia | 0.026412 | 0.074074 | 0.028566 | . 46 Bulgaria | 0.023917 | 0.020686 | 0.047610 | . 176 Bhutan | 0.020797 | 0.000000 | 0.000000 | . 131 Guyana | 0.020381 | 0.018125 | 0.019044 | . 44 Kazakhstan | 0.019965 | 0.017730 | 0.066654 | . 136 Haiti | 0.018925 | 0.003349 | 0.095220 | . 110 Jamaica | 0.017886 | 0.032112 | 0.057132 | . 127 Guinea | 0.012894 | 0.025808 | 0.000000 | . 143 Bahamas | 0.012686 | 0.027384 | 0.000000 | . 101 Cyprus | 0.011646 | 0.000000 | 0.000000 | . 129 Mauritania | 0.011646 | 0.005516 | 0.009522 | . 149 Equatorial Guinea | 0.011022 | 0.000000 | 0.047610 | . 114 Madagascar | 0.010399 | 0.029157 | 0.019044 | . 94 Montenegro | 0.009983 | 0.015760 | 0.028566 | . 163 Burundi | 0.009567 | 0.000000 | 0.000000 | . 66 Moldova | 0.009359 | 0.021474 | 0.028566 | . 73 Armenia | 0.008735 | 0.036643 | 0.047610 | . 83 Burma | 0.008319 | 0.003546 | 0.000000 | . 165 Eritrea | 0.008111 | 0.000000 | 0.000000 | . 111 Ivory Coast | 0.007903 | 0.010047 | 0.019044 | . 113 Senegal | 0.007279 | 0.007880 | 0.000000 | . 106 Singapore | 0.006863 | 0.003152 | 0.000000 | . 80 Nigeria | 0.006447 | 0.002167 | 0.000000 | . 119 Congo (Kinshasa) | 0.006239 | 0.002167 | 0.000000 | . 125 Syria | 0.006239 | 0.000985 | 0.047610 | . 81 North Macedonia | 0.006031 | 0.018125 | 0.104742 | . 102 Mozambique | 0.005615 | 0.052600 | 0.009522 | . 30 Israel | 0.004159 | 0.002758 | 0.000000 | . 115 Zimbabwe | 0.003120 | 0.002955 | 0.019044 | . 124 Rwanda | 0.002912 | 0.011820 | 0.000000 | . 174 Saint Vincent and the Grenadines | 0.002704 | 0.000000 | 0.000000 | . 122 Australia | 0.002704 | 0.000788 | 0.000000 | . 87 Albania | 0.002496 | 0.023247 | 0.009522 | . 98 Mainland China | 0.002288 | 0.000985 | 0.000000 | . 156 Yemen | 0.001664 | 0.004728 | 0.028566 | . 177 Mauritius | 0.001456 | 0.000394 | 0.000000 | . 133 Somalia | 0.001248 | 0.001773 | 0.000000 | . 139 Burkina Faso | 0.001248 | 0.000000 | 0.000000 | . 117 Malawi | 0.001248 | 0.000985 | 0.000000 | . 159 Niger | 0.000832 | 0.002364 | 0.000000 | . 166 Barbados | 0.000624 | 0.000000 | 0.000000 | . 175 Laos | 0.000624 | 0.023838 | 0.000000 | . 135 Mali | 0.000624 | 0.025808 | 0.009522 | . 120 Malta | 0.000624 | 0.000394 | 0.000000 | . 130 Eswatini | 0.000416 | 0.001182 | 0.000000 | . 170 New Zealand | 0.000416 | 0.001379 | 0.000000 | . 167 Comoros | 0.000416 | 0.000591 | 0.000000 | . 169 Liechtenstein | 0.000416 | 0.000000 | 0.000000 | . 162 Chad | 0.000208 | 0.000000 | 0.000000 | . 172 Sao Tome and Principe | 0.000208 | 0.000591 | 0.000000 | . 142 Hong Kong | 0.000208 | 0.001182 | 0.000000 | . 164 Sierra Leone | 0.000208 | 0.000591 | 0.000000 | . 168 Guinea-Bissau | 0.000208 | 0.000985 | 0.000000 | . 179 Diamond Princess | 0.000000 | 0.000000 | 0.000000 | . 189 MS Zaandam | 0.000000 | 0.000000 | 0.000000 | . 161 Saint Lucia | 0.000000 | 0.000000 | 0.000000 | . 180 Tanzania | 0.000000 | 0.000000 | 0.000000 | . 193 Kiribati | 0.000000 | 0.000000 | 0.000000 | . 192 Samoa | 0.000000 | 0.000000 | 0.000000 | . 181 Fiji | 0.000000 | 0.000000 | 0.000000 | . 191 Marshall Islands | 0.000000 | 0.000000 | 0.000000 | . 190 Vanuatu | 0.000000 | 0.000000 | 0.000000 | . 160 San Marino | 0.000000 | 0.000000 | 0.000000 | . 184 Grenada | 0.000000 | 0.000000 | 0.000000 | . 187 Holy See | 0.000000 | 0.000000 | 0.000000 | . 185 Saint Kitts and Nevis | 0.000000 | 0.000000 | 0.000000 | . 186 Macau | 0.000000 | 0.000000 | 0.000000 | . 188 Solomon Islands | 0.000000 | 0.000000 | 0.000000 | . 171 Monaco | 0.000000 | 0.000000 | 0.000000 | . 182 Brunei | 0.000000 | 0.000000 | 0.000000 | . 173 Liberia | 0.000000 | 0.000000 | 0.000000 | . 183 Dominica | 0.000000 | 0.000000 | 0.000000 | . 178 Antigua and Barbuda | 0.000000 | 0.000000 | 0.000000 | . 97 Finland | 0.000000 | 0.000000 | 0.000000 | . 158 Gambia | 0.000000 | 0.000000 | 0.000000 | . 75 Oman | 0.000000 | 0.000000 | 0.000000 | . 104 Luxembourg | 0.000000 | 0.000000 | 0.000000 | . 100 El Salvador | 0.000000 | 0.000000 | 0.038088 | . 96 Ghana | 0.000000 | 0.000000 | 0.000000 | . 91 Kosovo | 0.000000 | 0.000000 | 0.000000 | . 77 Libya | 0.000000 | 0.000000 | 0.000000 | . 76 Bosnia and Herzegovina | 0.000000 | 0.000000 | 0.000000 | . 67 Ireland | 0.000000 | 0.000000 | 0.000000 | . 157 Iceland | 0.000000 | 0.000000 | 0.000000 | . 57 Costa Rica | 0.000000 | 0.000000 | 0.000000 | . 49 Slovakia | 0.000000 | 0.000000 | 0.000000 | . 36 Switzerland | 0.000000 | 0.000000 | 0.038088 | . 34 Jordan | 0.000000 | 0.052994 | 0.000000 | . 26 Sweden | 0.000000 | 0.000000 | 0.000000 | . 10 Spain | 0.000000 | 0.000000 | 0.000000 | . 107 Mongolia | 0.000000 | 0.000000 | 0.000000 | . 109 Botswana | 0.000000 | 0.000000 | 0.000000 | . 116 Sudan | 0.000000 | 0.000000 | 0.000000 | . 126 Gabon | 0.000000 | 0.000000 | 0.000000 | . 132 Papua New Guinea | 0.000000 | 0.000000 | 0.000000 | . 137 Andorra | 0.000000 | 0.000000 | 0.000000 | . 138 Togo | 0.000000 | 0.000000 | 0.000000 | . 140 Tajikistan | 0.000000 | 0.000000 | 0.000000 | . 141 Belize | 0.000000 | 0.000000 | 0.000000 | . 144 Congo (Brazzaville) | 0.000000 | 0.000000 | 0.000000 | . 145 Djibouti | 0.000000 | 0.000000 | 0.000000 | . 146 Seychelles | 0.000000 | 0.000000 | 0.000000 | . 147 Lesotho | 0.000000 | 0.000000 | 0.000000 | . 148 South Sudan | 0.000000 | 0.000000 | 0.000000 | . 150 Benin | 0.000000 | 0.000000 | 0.000000 | . 152 Nicaragua | 0.000000 | 0.000000 | 0.000000 | . 153 Central African Republic | 0.000000 | 0.000000 | 0.000000 | . 194 Micronesia | 0.000000 | 0.000000 | 0.000000 | . fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,12)) top_15_confirmed=countrywise.sort_values([&quot;Confirmed&quot;],ascending=False).head(15) top_15_deaths=countrywise.sort_values([&quot;Deaths&quot;],ascending=False).head(15) sns.barplot(x=top_15_confirmed[&quot;Confirmed&quot;],y=top_15_confirmed.index,ax=ax1) ax1.set_title(&quot;Top 15 countries as per Number of Confirmed Cases&quot;) sns.barplot(x=top_15_deaths[&quot;Deaths&quot;],y=top_15_deaths.index,ax=ax2) ax2.set_title(&quot;Top 15 countries as per Number of Death Cases&quot;) . Text(0.5, 1.0, &#39;Top 15 countries as per Number of Death Cases&#39;) . If we check the list of countries in accordance to number tourists visiters from link mentioned above, Top countries are mainly France, Spain, USA, China, Italy, Mexico, UK, Turkey, Germany, Thailand. Another thing to take into account most of the countries mentioned above also have highest number of International Students. All of the them are the most affected countries because of COVID-19 . Another interesting thing to see is the median age of worst affected countries. . Top 25 Countries as per Mortatlity Rate and Recovery Rate with more than 500 Confirmed Cases . fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,15)) countrywise_plot_mortal=countrywise[countrywise[&quot;Confirmed&quot;]&gt;500].sort_values([&quot;Mortality&quot;],ascending=False).head(15) sns.barplot(x=countrywise_plot_mortal[&quot;Mortality&quot;],y=countrywise_plot_mortal.index,ax=ax1) ax1.set_title(&quot;Top 15 Countries according High Mortatlity Rate&quot;) ax1.set_xlabel(&quot;Mortality (in Percentage)&quot;) countrywise_plot_recover=countrywise[countrywise[&quot;Confirmed&quot;]&gt;500].sort_values([&quot;Recovery&quot;],ascending=False).head(15) sns.barplot(x=countrywise_plot_recover[&quot;Recovery&quot;],y=countrywise_plot_recover.index, ax=ax2) ax2.set_title(&quot;Top 15 Countries according High Recovery Rate&quot;) ax2.set_xlabel(&quot;Recovery (in Percentage)&quot;) . Text(0.5, 0, &#39;Recovery (in Percentage)&#39;) . fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,15)) countrywise_plot_mortal=countrywise[countrywise[&quot;Confirmed&quot;]&gt;500].sort_values([&quot;Mortality&quot;],ascending=False).tail(15) sns.barplot(x=countrywise_plot_mortal[&quot;Mortality&quot;],y=countrywise_plot_mortal.index,ax=ax1) ax1.set_title(&quot;Top 15 Countries according Low Mortatlity Rate&quot;) ax1.set_xlabel(&quot;Mortality (in Percentage)&quot;) countrywise_plot_recover=countrywise[countrywise[&quot;Confirmed&quot;]&gt;500].sort_values([&quot;Recovery&quot;],ascending=False).tail(15) sns.barplot(x=countrywise_plot_recover[&quot;Recovery&quot;],y=countrywise_plot_recover.index, ax=ax2) ax2.set_title(&quot;Top 15 Countries according Low Recovery Rate&quot;) ax2.set_xlabel(&quot;Recovery (in Percentage)&quot;) . Text(0.5, 0, &#39;Recovery (in Percentage)&#39;) . No Recovered Patients with considerable Mortality Rate . no_recovered_countries=countrywise[(countrywise[&quot;Recovered&quot;]==0)][[&quot;Confirmed&quot;,&quot;Deaths&quot;]] no_recovered_countries[&quot;Mortality Rate&quot;]=(no_recovered_countries[&quot;Deaths&quot;]/no_recovered_countries[&quot;Confirmed&quot;])*100 no_recovered_countries=no_recovered_countries[no_recovered_countries[&quot;Mortality Rate&quot;]&gt;0].sort_values([&quot;Mortality Rate&quot;],ascending=False) no_recovered_countries.style.background_gradient(&#39;Reds&#39;) . &nbsp; Confirmed Deaths Mortality Rate . Country/Region &nbsp; &nbsp; &nbsp; . Belgium 1059763.000000 | 24921.000000 | 2.351564 | . US 33251939.000000 | 594306.000000 | 1.787282 | . Sweden 1068473.000000 | 14451.000000 | 1.352491 | . Serbia 712046.000000 | 6844.000000 | 0.961174 | . Sweden currently has maximum number of Confirmed Cases, with no Recovered patient being recorded, it also has hihgt comparitively has high mortality rate compared to overall mortality rate of the World. . Countries with more than 100 Confirmed Cases and No Deaths with considerably high Recovery Rate . no_deaths=countrywise[(countrywise[&quot;Confirmed&quot;]&gt;100)&amp;(countrywise[&quot;Deaths&quot;]==0)] no_deaths=no_deaths[no_deaths[&quot;Recovery&quot;]&gt;0].sort_values([&quot;Recovery&quot;],ascending=False).drop([&quot;Mortality&quot;],1) no_deaths.style.background_gradient(cmap=&quot;Reds&quot;) . &nbsp; Confirmed Recovered Deaths Recovery . Country/Region &nbsp; &nbsp; &nbsp; &nbsp; . Dominica 188.000000 | 182.000000 | 0.000000 | 96.808511 | . Vietnam has able to contain COVID-19 pretty well with no Deaths recorded so far with pretty healthy Recovery Rate. Just for information Vietnam was the first country to inform World Health Organization about Human to Human Transmission of COVID-19. . Vietnam and Cambodia will soon be free from COVID-19. . fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,15)) countrywise[&quot;Active Cases&quot;]=(countrywise[&quot;Confirmed&quot;]-countrywise[&quot;Recovered&quot;]-countrywise[&quot;Deaths&quot;]) countrywise[&quot;Outcome Cases&quot;]=(countrywise[&quot;Recovered&quot;]+countrywise[&quot;Deaths&quot;]) top_15_active=countrywise.sort_values([&quot;Active Cases&quot;],ascending=False).head(15) top_15_outcome=countrywise.sort_values([&quot;Outcome Cases&quot;],ascending=False).head(15) sns.barplot(x=top_15_active[&quot;Active Cases&quot;],y=top_15_active.index,ax=ax1) sns.barplot(x=top_15_outcome[&quot;Outcome Cases&quot;],y=top_15_outcome.index,ax=ax2) ax1.set_title(&quot;Top 15 Countries with Most Number of Active Cases&quot;) ax2.set_title(&quot;Top 15 Countries with Most Number of Closed Cases&quot;) . Text(0.5, 1.0, &#39;Top 15 Countries with Most Number of Closed Cases&#39;) . # confirm_rate=[] # for country in countrywise.index: # days=country_date.ix[country].shape[0] # confirm_rate.append((countrywise.ix[country][&quot;Confirmed&quot;])/days) # countrywise[&quot;Confirm Cases/Day&quot;]=confirm_rate . # top_15_ccpd=countrywise.sort_values([&quot;Confirm Cases/Day&quot;],ascending=False).head(15) # sns.barplot(y=top_15_ccpd.index,x=top_15_ccpd[&quot;Confirm Cases/Day&quot;],ax=ax1) # ax1.set_title(&quot;Top 15 countries as per high number Confirmed Cases per Day&quot;) # bottom_15_ccpd=countrywise[countrywise[&quot;Confirmed&quot;]&gt;1000].sort_values([&quot;Confirm Cases/Day&quot;],ascending=False).tail(15) # sns.barplot(y=bottom_15_ccpd.index,x=bottom_15_ccpd[&quot;Confirm Cases/Day&quot;],ax=ax2) # ax2.set_title(&quot;Top 15 countries as per Lowest Confirmed Cases per Day having more than 1000 Confirmed Cases&quot;) . Mainland China has recorded highest number of Closed cases as thier Recovery Rate is staggering recording 85%+ . Confirmed Cases/Day is clear indication of why US has highest number of Active Cases currently. The rate is 11000+ cases per day. Showing increase in that value every day. . fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,15)) countrywise[&quot;Survival Probability&quot;]=(1-(countrywise[&quot;Deaths&quot;]/countrywise[&quot;Confirmed&quot;]))*100 top_25_survival=countrywise[countrywise[&quot;Confirmed&quot;]&gt;1000].sort_values([&quot;Survival Probability&quot;],ascending=False).head(15) sns.barplot(x=top_25_survival[&quot;Survival Probability&quot;],y=top_25_survival.index,ax=ax1) ax1.set_title(&quot;Top 25 Countries with Maximum Survival Probability having more than 1000 Confiremed Cases&quot;) print(&#39;Mean Survival Probability across all countries&#39;,countrywise[&quot;Survival Probability&quot;].mean()) print(&#39;Median Survival Probability across all countries&#39;,countrywise[&quot;Survival Probability&quot;].median()) print(&#39;Mean Death Probability across all countries&#39;,100-countrywise[&quot;Survival Probability&quot;].mean()) print(&#39;Median Death Probability across all countries&#39;,100-countrywise[&quot;Survival Probability&quot;].median()) Bottom_5_countries=countrywise[countrywise[&quot;Confirmed&quot;]&gt;100].sort_values([&quot;Survival Probability&quot;],ascending=True).head(15) sns.barplot(x=Bottom_5_countries[&quot;Survival Probability&quot;],y=Bottom_5_countries.index,ax=ax2) plt.title(&quot;Bottom 15 Countries as per Survival Probability&quot;) . Mean Survival Probability across all countries 97.83094336678441 Median Survival Probability across all countries 98.38462415588694 Mean Death Probability across all countries 2.1690566332155896 Median Death Probability across all countries 1.6153758441130606 . Text(0.5, 1.0, &#39;Bottom 15 Countries as per Survival Probability&#39;) . Survival Probability is the only graph that looks the most promising! Having average survival probability of 95%+ across all countries. The difference between Mean and Median Death Probability is an clear indication that there few countries with really high mortality rate e.g. Italy, Algeria, UK etc. . Journey of different Countries in COVID-19 . When we see daily news reports on COVID-19 it&#39;s really hard to interpret what&#39;s actually happening, since the numbers are changing so rapidly but that&#39;s something expected from Exponential growth. Since almost all the pandemics tend to grow exponentially it&#39;s really hard to understand for someone from a non-mathematical or non-statistical background. . We are more concerned about how we are doing and where we are heading in this pandemic rather than just looking at those exponentially growing numbers. The growth won&#39;t be exponentially forever, at some point of time the curve will become flat because probably all the people on the planet are infected or we human have been able to control the disease. . When we are in the middle of the exponential growth it&#39;s almost impossible to tell where are we heading. . Here, I am trying to show how we can interpret the exponential growth which is the common trend among all the countries . fig=go.Figure() for country in countrywise.head(10).index: fig.add_trace(go.Scatter(x=grouped_country.loc[country][&quot;log_confirmed&quot;], y=grouped_country.loc[country][&quot;log_active&quot;], mode=&#39;lines&#39;,name=country)) fig.update_layout(height=600,title=&quot;COVID-19 Journey of Top 15 Worst Affected Countries&quot;, xaxis_title=&quot;Confirmed Cases (Logrithmic Scale)&quot;,yaxis_title=&quot;Active Cases (Logarithmic Scale)&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . It&#39;s pretty evident that the disease is spreading in same manner everywhere, but if particular country is following pandemic controlling practices rigrously the results are evident in the graph. . Most of the countries will follow the same trajectory as that USA, which is &quot;Uncontrolled Exponential Growth&quot; . There are few countries where the pandemic controlling practices seems to be working accurately, few classic examples are China, Germany, Italy, Spain, Turkey has started showing that dip indicating there are somehow got control over COVID-19 . Countries like United Kingdom, Russia are following similar lines as that of United States, indicating the growth is still exponential among those countries. . Iran is showing some occasional drops. . fig=go.Figure() for country in countrywise.head(10).index: fig.add_trace(go.Scatter(x=grouped_country.loc[country].index, y=grouped_country.loc[country][&quot;Confirmed&quot;].rolling(window=7).mean().diff(), mode=&#39;lines&#39;,name=country)) fig.update_layout(height=600,title=&quot;7 Days Rolling Average of Daily increase of Confirmed Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() for country in countrywise.head(10).index: fig.add_trace(go.Scatter(x=grouped_country.loc[country].index, y=grouped_country.loc[country][&quot;Deaths&quot;].rolling(window=7).mean().diff(), mode=&#39;lines&#39;,name=country)) fig.update_layout(height=600,title=&quot;7 Days Rolling Average of Daily increase of Death Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Death Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() for country in countrywise.head(10).index: fig.add_trace(go.Scatter(x=grouped_country.loc[country].index, y=grouped_country.loc[country][&quot;Recovered&quot;].rolling(window=7).mean().diff(), mode=&#39;lines&#39;,name=country)) fig.update_layout(height=600,title=&quot;7 Days Rolling Average of Daily increase of Recovered Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Recovered Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . Clustering of Countries . The clustering of countries can be done considering different features. Here I&#39;m trying to cluster different countries based on the Mortality and Recovery rate of indivisual country. . As we all are well aware that COVID-19 has different Mortality Rate among different countries based on different factors and so is the Recovery Rate because of pandemic controlling practices followed by the individual country. Also Mortality Rate and Recovery Rate both togther takes into account all types of cases Confirmed, Recoverd and Deaths. . Let&#39;s checkout how these clusters look like! . X=countrywise[[&quot;Mortality&quot;,&quot;Recovery&quot;]] #Standard Scaling since K-Means Clustering is a distance based alogrithm X=std.fit_transform(X) . wcss=[] sil=[] for i in range(2,11): clf=KMeans(n_clusters=i,init=&#39;k-means++&#39;,random_state=42) clf.fit(X) labels=clf.labels_ centroids=clf.cluster_centers_ sil.append(silhouette_score(X, labels, metric=&#39;euclidean&#39;)) wcss.append(clf.inertia_) . x=np.arange(2,11) plt.figure(figsize=(10,5)) plt.plot(x,wcss,marker=&#39;o&#39;) plt.xlabel(&quot;Number of Clusters&quot;) plt.ylabel(&quot;Within Cluster Sum of Squares (WCSS)&quot;) plt.title(&quot;Elbow Method&quot;) . Text(0.5, 1.0, &#39;Elbow Method&#39;) . import scipy.cluster.hierarchy as sch plt.figure(figsize=(20,15)) dendogram=sch.dendrogram(sch.linkage(X, method = &quot;ward&quot;)) . All methods namely Elbow Method and Hierarchical Clustering shows K=3 will correct number of clusters. . clf_final=KMeans(n_clusters=3,init=&#39;k-means++&#39;,random_state=6) clf_final.fit(X) . KMeans(n_clusters=3, random_state=6) . countrywise[&quot;Clusters&quot;]=clf_final.predict(X) . Summary of Clusters . cluster_summary=pd.concat([countrywise[countrywise[&quot;Clusters&quot;]==1].head(15),countrywise[countrywise[&quot;Clusters&quot;]==2].head(15),countrywise[countrywise[&quot;Clusters&quot;]==0].head(15)]) cluster_summary.style.background_gradient(cmap=&#39;Reds&#39;).format(&quot;{:.2f}&quot;) . &nbsp; Confirmed Recovered Deaths Mortality Recovery Active Cases Outcome Cases Survival Probability Clusters . Country/Region &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . US 33251939.00 | 0.00 | 594306.00 | 1.79 | 0.00 | 32657633.00 | 594306.00 | 98.21 | 1.00 | . France 5719877.00 | 390878.00 | 109518.00 | 1.91 | 6.83 | 5219481.00 | 500396.00 | 98.09 | 1.00 | . UK 4496823.00 | 15481.00 | 128037.00 | 2.85 | 0.34 | 4353305.00 | 143518.00 | 97.15 | 1.00 | . Spain 3668658.00 | 150376.00 | 79905.00 | 2.18 | 4.10 | 3438377.00 | 230281.00 | 97.82 | 1.00 | . Netherlands 1671967.00 | 26810.00 | 17889.00 | 1.07 | 1.60 | 1627268.00 | 44699.00 | 98.93 | 1.00 | . Sweden 1068473.00 | 0.00 | 14451.00 | 1.35 | 0.00 | 1054022.00 | 14451.00 | 98.65 | 1.00 | . Belgium 1059763.00 | 0.00 | 24921.00 | 2.35 | 0.00 | 1034842.00 | 24921.00 | 97.65 | 1.00 | . Serbia 712046.00 | 0.00 | 6844.00 | 0.96 | 0.00 | 705202.00 | 6844.00 | 99.04 | 1.00 | . Switzerland 693023.00 | 317600.00 | 10805.00 | 1.56 | 45.83 | 364618.00 | 328405.00 | 98.44 | 1.00 | . Greece 400395.00 | 93764.00 | 12024.00 | 3.00 | 23.42 | 294607.00 | 105788.00 | 97.00 | 1.00 | . Ireland 254870.00 | 23364.00 | 4941.00 | 1.94 | 9.17 | 226565.00 | 28305.00 | 98.06 | 1.00 | . Honduras 236952.00 | 84389.00 | 6296.00 | 2.66 | 35.61 | 146267.00 | 90685.00 | 97.34 | 1.00 | . Thailand 151842.00 | 26873.00 | 988.00 | 0.65 | 17.70 | 123981.00 | 27861.00 | 99.35 | 1.00 | . Norway 124655.00 | 17998.00 | 783.00 | 0.63 | 14.44 | 105874.00 | 18781.00 | 99.37 | 1.00 | . Finland 92244.00 | 46000.00 | 948.00 | 1.03 | 49.87 | 45296.00 | 46948.00 | 98.97 | 1.00 | . Yemen 6731.00 | 3399.00 | 1319.00 | 19.60 | 50.50 | 2013.00 | 4718.00 | 80.40 | 2.00 | . MS Zaandam 9.00 | 7.00 | 2.00 | 22.22 | 77.78 | 0.00 | 9.00 | 77.78 | 2.00 | . Vanuatu 4.00 | 3.00 | 1.00 | 25.00 | 75.00 | 0.00 | 4.00 | 75.00 | 2.00 | . India 27894800.00 | 25454320.00 | 325972.00 | 1.17 | 91.25 | 2114508.00 | 25780292.00 | 98.83 | 0.00 | . Brazil 16471600.00 | 14496224.00 | 461057.00 | 2.80 | 88.01 | 1514319.00 | 14957281.00 | 97.20 | 0.00 | . Turkey 5235978.00 | 5094279.00 | 47271.00 | 0.90 | 97.29 | 94428.00 | 5141550.00 | 99.10 | 0.00 | . Russia 4995613.00 | 4616422.00 | 118781.00 | 2.38 | 92.41 | 260410.00 | 4735203.00 | 97.62 | 0.00 | . Italy 4213055.00 | 3845087.00 | 126002.00 | 2.99 | 91.27 | 241966.00 | 3971089.00 | 97.01 | 0.00 | . Argentina 3732263.00 | 3288467.00 | 77108.00 | 2.07 | 88.11 | 366688.00 | 3365575.00 | 97.93 | 0.00 | . Germany 3684672.00 | 3479700.00 | 88413.00 | 2.40 | 94.44 | 116559.00 | 3568113.00 | 97.60 | 0.00 | . Colombia 3363061.00 | 3141549.00 | 87747.00 | 2.61 | 93.41 | 133765.00 | 3229296.00 | 97.39 | 0.00 | . Iran 2893218.00 | 2425033.00 | 79741.00 | 2.76 | 83.82 | 388444.00 | 2504774.00 | 97.24 | 0.00 | . Poland 2871371.00 | 2636675.00 | 73682.00 | 2.57 | 91.83 | 161014.00 | 2710357.00 | 97.43 | 0.00 | . Mexico 2411503.00 | 1924865.00 | 223455.00 | 9.27 | 79.82 | 263183.00 | 2148320.00 | 90.73 | 0.00 | . Ukraine 2257904.00 | 2084477.00 | 52414.00 | 2.32 | 92.32 | 121013.00 | 2136891.00 | 97.68 | 0.00 | . Peru 1947555.00 | 1897522.00 | 68978.00 | 3.54 | 97.43 | -18945.00 | 1966500.00 | 96.46 | 0.00 | . Indonesia 1809926.00 | 1659974.00 | 50262.00 | 2.78 | 91.72 | 99690.00 | 1710236.00 | 97.22 | 0.00 | . Czech Republic 1660935.00 | 1617498.00 | 30101.00 | 1.81 | 97.38 | 13336.00 | 1647599.00 | 98.19 | 0.00 | . print(&quot;Avergae Mortality Rate of Cluster 0: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==0][&quot;Mortality&quot;].mean()) print(&quot;Avergae Recovery Rate of Cluster 0: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==0][&quot;Recovery&quot;].mean()) print(&quot;Avergae Mortality Rate of Cluster 1: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==1][&quot;Mortality&quot;].mean()) print(&quot;Avergae Recovery Rate of Cluster 1: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==1][&quot;Recovery&quot;].mean()) print(&quot;Avergae Mortality Rate of Cluster 2: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==2][&quot;Mortality&quot;].mean()) print(&quot;Avergae Recovery Rate of Cluster 2: &quot;,countrywise[countrywise[&quot;Clusters&quot;]==2][&quot;Recovery&quot;].mean()) . Avergae Mortality Rate of Cluster 0: 1.877885707521883 Avergae Recovery Rate of Cluster 0: 90.74888221767141 Avergae Mortality Rate of Cluster 1: 1.701640341180339 Avergae Recovery Rate of Cluster 1: 22.335095583921884 Avergae Mortality Rate of Cluster 2: 22.272707263793283 Avergae Recovery Rate of Cluster 2: 67.75849166652911 . plt.figure(figsize=(10,5)) sns.scatterplot(x=countrywise[&quot;Recovery&quot;],y=countrywise[&quot;Mortality&quot;],hue=countrywise[&quot;Clusters&quot;],s=100) plt.axvline(((datewise[&quot;Recovered&quot;]/datewise[&quot;Confirmed&quot;])*100).mean(), color=&#39;red&#39;,linestyle=&quot;--&quot;,label=&quot;Mean Recovery Rate around the World&quot;) plt.axhline(((datewise[&quot;Deaths&quot;]/datewise[&quot;Confirmed&quot;])*100).mean(), color=&#39;black&#39;,linestyle=&quot;--&quot;,label=&quot;Mean Mortality Rate around the World&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f19bf8e4e50&gt; . print(&quot;Few Countries belonging to Cluster 0: &quot;,list(countrywise[countrywise[&quot;Clusters&quot;]==0].head(10).index)) print(&quot;Few Countries belonging to Cluster 1: &quot;,list(countrywise[countrywise[&quot;Clusters&quot;]==1].head(10).index)) print(&quot;Few Countries belonging to Cluster 2: &quot;,list(countrywise[countrywise[&quot;Clusters&quot;]==2].head(10).index)) . Few Countries belonging to Cluster 0: [&#39;India&#39;, &#39;Brazil&#39;, &#39;Turkey&#39;, &#39;Russia&#39;, &#39;Italy&#39;, &#39;Argentina&#39;, &#39;Germany&#39;, &#39;Colombia&#39;, &#39;Iran&#39;, &#39;Poland&#39;] Few Countries belonging to Cluster 1: [&#39;US&#39;, &#39;France&#39;, &#39;UK&#39;, &#39;Spain&#39;, &#39;Netherlands&#39;, &#39;Sweden&#39;, &#39;Belgium&#39;, &#39;Serbia&#39;, &#39;Switzerland&#39;, &#39;Greece&#39;] Few Countries belonging to Cluster 2: [&#39;Yemen&#39;, &#39;MS Zaandam&#39;, &#39;Vanuatu&#39;] . Cluster 2 is a set of countries which have really High Mortality Rate and consdierably Good Recovery Rate. Basically few countries among these clusters have seen already the worst of this pandemic but are now recovering with healty Recovery Rate. . Cluster 0 is set of countries which have Low Mortality Rate and really High Recovery Rate. These are the set of countries who has been able to control the COVID-19 by following pandemic controlling practices rigorously. . Cluster 1 is set of countries which have Low Mortality Rate and really Low Recovery Rate. These countries need to pace up their Revovery Rate to get out it, Some thses countries have really high number of Infected Cases but Low Mortality is positive sign out of it. . Comparison of China, Italy, US, Spain, Brazil and Rest of the World . china_data=covid[covid[&quot;Country/Region&quot;]==&quot;Mainland China&quot;] Italy_data=covid[covid[&quot;Country/Region&quot;]==&quot;Italy&quot;] US_data=covid[covid[&quot;Country/Region&quot;]==&quot;US&quot;] spain_data=covid[covid[&quot;Country/Region&quot;]==&quot;Spain&quot;] brazil_data=covid[covid[&quot;Country/Region&quot;]==&quot;Brazil&quot;] rest_of_world=covid[(covid[&quot;Country/Region&quot;]!=&quot;Mainland China&quot;)&amp;(covid[&quot;Country/Region&quot;]!=&quot;Italy&quot;)&amp;(covid[&quot;Country/Region&quot;]!=&quot;US&quot;)&amp;(covid[&quot;Country/Region&quot;]!=&quot;Spain&quot;)&amp;(covid[&quot;Country/Region&quot;]!=&quot;Brazil&quot;)] datewise_china=china_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise_Italy=Italy_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise_US=US_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise_Spain=spain_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise_Brazil=brazil_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) datewise_restofworld=rest_of_world.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Confirmed&quot;]), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Confirmed Cases plot&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Recovered&quot;]), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Recovered Cases plot&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Deaths&quot;]), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Death Cases plot&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . China has been able to &quot;flatten the curve&quot; looking at their graphs of Confirmed and Death Cases. With staggering Recovery Rate. . US seems to have good control on Deaths, but number of people getting affected is going way out of hand. . datewise_china[&quot;Mortality&quot;]=(datewise_china[&quot;Deaths&quot;]/datewise_china[&quot;Confirmed&quot;])*100 datewise_Italy[&quot;Mortality&quot;]=(datewise_Italy[&quot;Deaths&quot;]/datewise_Italy[&quot;Confirmed&quot;])*100 datewise_US[&quot;Mortality&quot;]=(datewise_US[&quot;Deaths&quot;]/datewise_US[&quot;Confirmed&quot;])*100 datewise_Spain[&quot;Mortality&quot;]=(datewise_Spain[&quot;Deaths&quot;]/datewise_Spain[&quot;Confirmed&quot;])*100 datewise_Brazil[&quot;Mortality&quot;]=(datewise_Brazil[&quot;Deaths&quot;]/datewise_Brazil[&quot;Confirmed&quot;])*100 datewise_restofworld[&quot;Mortality&quot;]=(datewise_restofworld[&quot;Deaths&quot;]/datewise_restofworld[&quot;Confirmed&quot;])*100 datewise_china[&quot;Recovery&quot;]=(datewise_china[&quot;Recovered&quot;]/datewise_china[&quot;Confirmed&quot;])*100 datewise_Italy[&quot;Recovery&quot;]=(datewise_Italy[&quot;Recovered&quot;]/datewise_Italy[&quot;Confirmed&quot;])*100 datewise_US[&quot;Recovery&quot;]=(datewise_US[&quot;Recovered&quot;]/datewise_US[&quot;Confirmed&quot;])*100 datewise_Spain[&quot;Recovery&quot;]=(datewise_Spain[&quot;Recovered&quot;]/datewise_Spain[&quot;Confirmed&quot;])*100 datewise_Brazil[&quot;Recovery&quot;]=(datewise_Brazil[&quot;Recovered&quot;]/datewise_Brazil[&quot;Confirmed&quot;])*100 datewise_restofworld[&quot;Recovery&quot;]=(datewise_restofworld[&quot;Recovered&quot;]/datewise_restofworld[&quot;Confirmed&quot;])*100 . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Mortality&quot;]), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Mortality Rate comparison plot&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Mortality Rate&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Recovery&quot;]), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Recovery Rate comparison plot&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Recovery Rate&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . Taking off Recovery Rate of Spain is a good sign but it&#39;s nowhere in comparison to the Moratality Rate. . Its alarming sign for USA and Brazil as their Recovery Rate is improving considerably as compared to other severly affected countries. . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Confirmed&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Daily increase in Number of Confirmed Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_china.index, y=(datewise_china[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;China&quot;)) fig.add_trace(go.Scatter(x=datewise_Italy.index, y=(datewise_Italy[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US.index, y=(datewise_US[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;United States&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain.index, y=(datewise_Spain[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_Brazil.index, y=(datewise_Brazil[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Brazil&quot;)) fig.add_trace(go.Scatter(x=datewise_restofworld.index, y=(datewise_restofworld[&quot;Deaths&quot;]).diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Rest of the World&quot;)) fig.update_layout(title=&quot;Daily increase in Number of Death Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . We can clearly notice the decreasing trend in the number of Daily Confirmed and Death Cases of Spain and Italy. That&#39;s really positive sign for both the countries. . Data Analysis for India . For detailed Data analysis and Forecasting specific to India . Click Here: COVID-19 Data Analysis &amp; Forecasting for India . The notebook consists of detailed data analysis specific to India, Comparison of India with the neighboring countries, Comparison with worst affected countries in this pandemic and try and build Machine Learnig Prediction and Time Series and Forecasting models to try and understand the how the numbers are going to be in near future. . india_data=covid[covid[&quot;Country/Region&quot;]==&quot;India&quot;] datewise_india=india_data.groupby([&quot;ObservationDate&quot;]).agg({&quot;Confirmed&quot;:&#39;sum&#39;,&quot;Recovered&quot;:&#39;sum&#39;,&quot;Deaths&quot;:&#39;sum&#39;}) print(datewise_india.iloc[-1]) print(&quot;Total Active Cases: &quot;,datewise_india[&quot;Confirmed&quot;].iloc[-1]-datewise_india[&quot;Recovered&quot;].iloc[-1]-datewise_india[&quot;Deaths&quot;].iloc[-1]) print(&quot;Total Closed Cases: &quot;,datewise_india[&quot;Recovered&quot;].iloc[-1]+datewise_india[&quot;Deaths&quot;].iloc[-1]) . Confirmed 27894800.0 Recovered 25454320.0 Deaths 325972.0 Name: 2021-05-29 00:00:00, dtype: float64 Total Active Cases: 2114508.0 Total Closed Cases: 25780292.0 . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;, name=&#39;Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Recovered&quot;], mode=&#39;lines+markers&#39;, name=&#39;Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Deaths&quot;], mode=&#39;lines+markers&#39;, name=&#39;Death Cases&#39;)) fig.update_layout(title=&quot;Growth of different types of cases in India&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=px.bar(x=datewise_india.index,y=datewise_india[&quot;Confirmed&quot;]-datewise_india[&quot;Recovered&quot;]-datewise_india[&quot;Deaths&quot;]) fig.update_layout(title=&quot;Distribution of Number of Active Cases in India&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,) fig.show() . . . india_increase_confirm=[] india_increase_recover=[] india_increase_deaths=[] for i in range(datewise_india.shape[0]-1): india_increase_confirm.append(((datewise_india[&quot;Confirmed&quot;].iloc[i+1])/datewise_india[&quot;Confirmed&quot;].iloc[i])) india_increase_recover.append(((datewise_india[&quot;Recovered&quot;].iloc[i+1])/datewise_india[&quot;Recovered&quot;].iloc[i])) india_increase_deaths.append(((datewise_india[&quot;Deaths&quot;].iloc[i+1])/datewise_india[&quot;Deaths&quot;].iloc[i])) india_increase_confirm.insert(0,1) india_increase_recover.insert(0,1) india_increase_deaths.insert(0,1) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_india.index, y=india_increase_confirm, mode=&#39;lines&#39;, name=&#39;Growth Factor of Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=india_increase_recover, mode=&#39;lines&#39;, name=&#39;Growth Factor of Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=india_increase_deaths, mode=&#39;lines&#39;, name=&#39;Growth Factor of Death Cases&#39;)) fig.update_layout(title=&quot;Datewise Growth Factor of Active and Closed cases in India&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Growth Factor&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Confirmed&quot;].diff().fillna(0), mode=&#39;lines+markers&#39;, name=&#39;Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Recovered&quot;].diff().fillna(0), mode=&#39;lines+markers&#39;, name=&#39;Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Deaths&quot;].diff().fillna(0), mode=&#39;lines+markers&#39;, name=&#39;Death Cases&#39;)) fig.update_layout(title=&quot;Daily increase in different types of cases in India&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . datewise_india[&quot;WeekOfYear&quot;]=datewise_india.index.weekofyear week_num_india=[] india_weekwise_confirmed=[] india_weekwise_recovered=[] india_weekwise_deaths=[] w=1 for i in list(datewise_india[&quot;WeekOfYear&quot;].unique()): india_weekwise_confirmed.append(datewise_india[datewise_india[&quot;WeekOfYear&quot;]==i][&quot;Confirmed&quot;].iloc[-1]) india_weekwise_recovered.append(datewise_india[datewise_india[&quot;WeekOfYear&quot;]==i][&quot;Recovered&quot;].iloc[-1]) india_weekwise_deaths.append(datewise_india[datewise_india[&quot;WeekOfYear&quot;]==i][&quot;Deaths&quot;].iloc[-1]) week_num_india.append(w) w=w+1 fig=go.Figure() fig.add_trace(go.Scatter(x=week_num_india, y=india_weekwise_confirmed, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Confirmed Cases&#39;)) fig.add_trace(go.Scatter(x=week_num_india, y=india_weekwise_recovered, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Recovered Cases&#39;)) fig.add_trace(go.Scatter(x=week_num_india, y=india_weekwise_deaths, mode=&#39;lines+markers&#39;, name=&#39;Weekly Growth of Death Cases&#39;)) fig.update_layout(title=&quot;Weekly Growth of different types of Cases in India&quot;, xaxis_title=&quot;Week Number&quot;,yaxis_title=&quot;Number of Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(15,5)) sns.barplot(x=week_num_india,y=pd.Series(india_weekwise_confirmed).diff().fillna(0),ax=ax1) sns.barplot(x=week_num_india,y=pd.Series(india_weekwise_deaths).diff().fillna(0),ax=ax2) ax1.set_xlabel(&quot;Week Number&quot;) ax2.set_xlabel(&quot;Week Number&quot;) ax1.set_ylabel(&quot;Number of Confirmed Cases&quot;) ax2.set_ylabel(&quot;Number of Death Cases&quot;) ax1.set_title(&quot;India&#39;s Weekwise increase in Number of Confirmed Cases&quot;) ax2.set_title(&quot;India&#39;s Weekwise increase in Number of Death Cases&quot;) . Text(0.5, 1.0, &#34;India&#39;s Weekwise increase in Number of Death Cases&#34;) . max_ind=datewise_india[&quot;Confirmed&quot;].max() print(&quot;It took&quot;,datewise_Italy[(datewise_Italy[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Italy[&quot;Confirmed&quot;]&lt;=max_ind)].shape[0],&quot;days in Italy to reach number of Confirmed Cases equivalent to India&quot;) print(&quot;It took&quot;,datewise_US[(datewise_US[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_US[&quot;Confirmed&quot;]&lt;=max_ind)].shape[0],&quot;days in USA to reach number of Confirmed Cases equivalent to India&quot;) print(&quot;It took&quot;,datewise_Spain[(datewise_Spain[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Spain[&quot;Confirmed&quot;]&lt;=max_ind)].shape[0],&quot;days in Spain to reach number of Confirmed Cases equivalent to India&quot;) print(&quot;It took&quot;,datewise_india[datewise_india[&quot;Confirmed&quot;]&gt;0].shape[0],&quot;days in India to reach&quot;,max_ind,&quot;Confirmed Cases&quot;) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_Italy[(datewise_Italy[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Italy[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_Italy[(datewise_Italy[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Italy[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;], mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US[(datewise_US[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_US[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_US[(datewise_US[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_US[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;], mode=&#39;lines&#39;,name=&quot;USA&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain[(datewise_Spain[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Spain[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_Spain[(datewise_Spain[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Spain[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;], mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Confirmed&quot;], mode=&#39;lines&#39;,name=&quot;India&quot;)) fig.update_layout(title=&quot;Growth of Recovered Cases with respect to India&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Confirmed Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . It took 485 days in Italy to reach number of Confirmed Cases equivalent to India It took 392 days in USA to reach number of Confirmed Cases equivalent to India It took 484 days in Spain to reach number of Confirmed Cases equivalent to India It took 486 days in India to reach 27894800.0 Confirmed Cases . . . Comparison of Daily Increase in Number of Cases of Italy, Spain, USA and India, where maximum number of Confirmed Cases are equivalent to maximum number of Confirmed Cases in India . fig=go.Figure() fig.add_trace(go.Scatter(x=datewise_Italy[(datewise_Italy[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Italy[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_Italy[(datewise_Italy[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Italy[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;].diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Italy&quot;)) fig.add_trace(go.Scatter(x=datewise_US[(datewise_US[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_US[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_US[(datewise_US[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_US[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;].diff().fillna(0), mode=&#39;lines&#39;,name=&quot;USA&quot;)) fig.add_trace(go.Scatter(x=datewise_Spain[(datewise_Spain[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Spain[&quot;Confirmed&quot;]&lt;=max_ind)].index, y=datewise_Spain[(datewise_Spain[&quot;Confirmed&quot;]&gt;0)&amp;(datewise_Spain[&quot;Confirmed&quot;]&lt;=max_ind)][&quot;Confirmed&quot;].diff().fillna(0), mode=&#39;lines&#39;,name=&quot;Spain&quot;)) fig.add_trace(go.Scatter(x=datewise_india.index, y=datewise_india[&quot;Confirmed&quot;].diff().fillna(0), mode=&#39;lines&#39;,name=&quot;India&quot;)) fig.update_layout(title=&quot;Daily increase in Confirmed Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Confirmed Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . Prediction using Machine Learning Models . Linear Regression Model for Confirm Cases Prediction . datewise[&quot;Days Since&quot;]=datewise.index-datewise.index[0] datewise[&quot;Days Since&quot;]=datewise[&quot;Days Since&quot;].dt.days . train_ml=datewise.iloc[:int(datewise.shape[0]*0.95)] valid_ml=datewise.iloc[int(datewise.shape[0]*0.95):] model_scores=[] . lin_reg=LinearRegression(normalize=True) . lin_reg.fit(np.array(train_ml[&quot;Days Since&quot;]).reshape(-1,1),np.array(train_ml[&quot;Confirmed&quot;]).reshape(-1,1)) . LinearRegression(normalize=True) . prediction_valid_linreg=lin_reg.predict(np.array(valid_ml[&quot;Days Since&quot;]).reshape(-1,1)) . model_scores.append(np.sqrt(mean_squared_error(valid_ml[&quot;Confirmed&quot;],prediction_valid_linreg))) print(&quot;Root Mean Square Error for Linear Regression: &quot;,np.sqrt(mean_squared_error(valid_ml[&quot;Confirmed&quot;],prediction_valid_linreg))) . Root Mean Square Error for Linear Regression: 33541511.296706144 . plt.figure(figsize=(11,6)) prediction_linreg=lin_reg.predict(np.array(datewise[&quot;Days Since&quot;]).reshape(-1,1)) linreg_output=[] for i in range(prediction_linreg.shape[0]): linreg_output.append(prediction_linreg[i][0]) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=datewise.index, y=linreg_output, mode=&#39;lines&#39;,name=&quot;Linear Regression Best Fit Line&quot;, line=dict(color=&#39;black&#39;, dash=&#39;dot&#39;))) fig.update_layout(title=&quot;Confirmed Cases Linear Regression Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . &lt;Figure size 792x432 with 0 Axes&gt; . The Linear Regression Model is absolutely falling aprat. As it is clearly visible that the trend of Confirmed Cases in absolutely not Linear. . Polynomial Regression for Prediction of Confirmed Cases . train_ml=datewise.iloc[:int(datewise.shape[0]*0.95)] valid_ml=datewise.iloc[int(datewise.shape[0]*0.95):] . poly = PolynomialFeatures(degree = 8) . train_poly=poly.fit_transform(np.array(train_ml[&quot;Days Since&quot;]).reshape(-1,1)) valid_poly=poly.fit_transform(np.array(valid_ml[&quot;Days Since&quot;]).reshape(-1,1)) y=train_ml[&quot;Confirmed&quot;] . linreg=LinearRegression(normalize=True) linreg.fit(train_poly,y) . LinearRegression(normalize=True) . prediction_poly=linreg.predict(valid_poly) rmse_poly=np.sqrt(mean_squared_error(valid_ml[&quot;Confirmed&quot;],prediction_poly)) model_scores.append(rmse_poly) print(&quot;Root Mean Squared Error for Polynomial Regression: &quot;,rmse_poly) . Root Mean Squared Error for Polynomial Regression: 27362958.416571088 . comp_data=poly.fit_transform(np.array(datewise[&quot;Days Since&quot;]).reshape(-1,1)) plt.figure(figsize=(11,6)) predictions_poly=linreg.predict(comp_data) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=datewise.index, y=predictions_poly, mode=&#39;lines&#39;,name=&quot;Polynomial Regression Best Fit&quot;, line=dict(color=&#39;black&#39;, dash=&#39;dot&#39;))) fig.update_layout(title=&quot;Confirmed Cases Polynomial Regression Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;, legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . &lt;Figure size 792x432 with 0 Axes&gt; . new_prediction_poly=[] for i in range(1,18): new_date_poly=poly.fit_transform(np.array(datewise[&quot;Days Since&quot;].max()+i).reshape(-1,1)) new_prediction_poly.append(linreg.predict(new_date_poly)[0]) . Support Vector Machine ModelRegressor for Prediction of Confirmed Cases . train_ml=datewise.iloc[:int(datewise.shape[0]*0.95)] valid_ml=datewise.iloc[int(datewise.shape[0]*0.95):] . svm=SVR(C=1,degree=6,kernel=&#39;poly&#39;,epsilon=0.01) . svm.fit(np.array(train_ml[&quot;Days Since&quot;]).reshape(-1,1),np.array(train_ml[&quot;Confirmed&quot;]).reshape(-1,1)) . SVR(C=1, degree=6, epsilon=0.01, kernel=&#39;poly&#39;) . prediction_valid_svm=svm.predict(np.array(valid_ml[&quot;Days Since&quot;]).reshape(-1,1)) . model_scores.append(np.sqrt(mean_squared_error(valid_ml[&quot;Confirmed&quot;],prediction_valid_svm))) print(&quot;Root Mean Square Error for Support Vectore Machine: &quot;,np.sqrt(mean_squared_error(valid_ml[&quot;Confirmed&quot;],prediction_valid_svm))) . Root Mean Square Error for Support Vectore Machine: 27435923.21693116 . plt.figure(figsize=(11,6)) prediction_svm=svm.predict(np.array(datewise[&quot;Days Since&quot;]).reshape(-1,1)) fig=go.Figure() fig.add_trace(go.Scatter(x=datewise.index, y=datewise[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=datewise.index, y=prediction_svm, mode=&#39;lines&#39;,name=&quot;Support Vector Machine Best fit Kernel&quot;, line=dict(color=&#39;black&#39;, dash=&#39;dot&#39;))) fig.update_layout(title=&quot;Confirmed Cases Support Vectore Machine Regressor Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . &lt;Figure size 792x432 with 0 Axes&gt; . Support Vector Machine model isn&#39;t providing great results now, the predictions are either overshooting or really lower than what&#39;s expected. . new_date=[] new_prediction_lr=[] new_prediction_svm=[] for i in range(1,18): new_date.append(datewise.index[-1]+timedelta(days=i)) new_prediction_lr.append(lin_reg.predict(np.array(datewise[&quot;Days Since&quot;].max()+i).reshape(-1,1))[0][0]) new_prediction_svm.append(svm.predict(np.array(datewise[&quot;Days Since&quot;].max()+i).reshape(-1,1))[0]) . pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.6f&#39; % x) model_predictions=pd.DataFrame(zip(new_date,new_prediction_lr,new_prediction_poly,new_prediction_svm), columns=[&quot;Dates&quot;,&quot;Linear Regression Prediction&quot;,&quot;Polynonmial Regression Prediction&quot;,&quot;SVM Prediction&quot;]) model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Predictions of Linear Regression are nowhere close to actual values. . Time Series Forecasting . Holt&#39;s Linear Model . model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . holt=Holt(np.asarray(model_train[&quot;Confirmed&quot;])).fit(smoothing_level=0.4, smoothing_slope=0.4,optimized=False) . y_pred[&quot;Holt&quot;]=holt.forecast(len(valid)) model_scores.append(np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;Holt&quot;]))) print(&quot;Root Mean Square Error Holt&#39;s Linear Model: &quot;,np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;Holt&quot;]))) . Root Mean Square Error Holt&#39;s Linear Model: 1696111.7924457418 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;Holt&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction of Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases Holt&#39;s Linear Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . holt_new_date=[] holt_new_prediction=[] for i in range(1,18): holt_new_date.append(datewise.index[-1]+timedelta(days=i)) holt_new_prediction.append(holt.forecast((len(valid)+i))[-1]) model_predictions[&quot;Holt&#39;s Linear Model Prediction&quot;]=holt_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Holt&#39;s Winter Model for Daily Time Series . model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . es=ExponentialSmoothing(np.asarray(model_train[&#39;Confirmed&#39;]),seasonal_periods=14,trend=&#39;add&#39;, seasonal=&#39;mul&#39;).fit() . y_pred[&quot;Holt&#39;s Winter Model&quot;]=es.forecast(len(valid)) model_scores.append(np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;Holt&#39;s Winter Model&quot;]))) print(&quot;Root Mean Square Error for Holt&#39;s Winter Model: &quot;,np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;Holt&#39;s Winter Model&quot;]))) . Root Mean Square Error for Holt&#39;s Winter Model: 2594639.6682255697 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;Holt &#39;s Winter Model&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction of Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases Holt&#39;s Winter Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . holt_winter_new_prediction=[] for i in range(1,18): holt_winter_new_prediction.append(es.forecast((len(valid)+i))[-1]) model_predictions[&quot;Holt&#39;s Winter Model Prediction&quot;]=holt_winter_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . AR Model (using AUTO ARIMA) . model_ar= auto_arima(model_train[&quot;Confirmed&quot;],trace=True, error_action=&#39;ignore&#39;, start_p=0,start_q=0,max_p=4,max_q=0, suppress_warnings=True,stepwise=False,seasonal=False) model_ar.fit(model_train[&quot;Confirmed&quot;]) . ARIMA(0,2,0)(0,0,0)[0] intercept : AIC=11809.478, Time=0.05 sec ARIMA(1,2,0)(0,0,0)[0] intercept : AIC=11798.602, Time=0.03 sec ARIMA(2,2,0)(0,0,0)[0] intercept : AIC=11798.849, Time=0.06 sec ARIMA(3,2,0)(0,0,0)[0] intercept : AIC=11750.641, Time=0.14 sec ARIMA(4,2,0)(0,0,0)[0] intercept : AIC=11660.908, Time=0.19 sec Best model: ARIMA(4,2,0)(0,0,0)[0] intercept Total fit time: 0.489 seconds . ARIMA(order=(4, 2, 0), scoring_args={}, suppress_warnings=True) . prediction_ar=model_ar.predict(len(valid)) y_pred[&quot;AR Model Prediction&quot;]=prediction_ar . model_scores.append(np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;AR Model Prediction&quot;]))) print(&quot;Root Mean Square Error for AR Model: &quot;,np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;AR Model Prediction&quot;]))) . Root Mean Square Error for AR Model: 2350964.490321815 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;AR Model Prediction&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction of Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases AR Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . AR_model_new_prediction=[] for i in range(1,18): AR_model_new_prediction.append(model_ar.predict(len(valid)+i)[-1]) model_predictions[&quot;AR Model Prediction&quot;]=AR_model_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction AR Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | 175686390.526716 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | 176528318.654233 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | 177370455.177831 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | 178213091.415752 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | 179057425.154646 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; MA Model (using AUTO ARIMA) . model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . model_ma= auto_arima(model_train[&quot;Confirmed&quot;],trace=True, error_action=&#39;ignore&#39;, start_p=0,start_q=0,max_p=0,max_q=2, suppress_warnings=True,stepwise=False,seasonal=False) model_ma.fit(model_train[&quot;Confirmed&quot;]) . ARIMA(0,2,0)(0,0,0)[0] intercept : AIC=11809.478, Time=0.03 sec ARIMA(0,2,1)(0,0,0)[0] intercept : AIC=11785.886, Time=0.09 sec ARIMA(0,2,2)(0,0,0)[0] intercept : AIC=11738.463, Time=0.15 sec Best model: ARIMA(0,2,2)(0,0,0)[0] intercept Total fit time: 0.278 seconds . ARIMA(order=(0, 2, 2), scoring_args={}, suppress_warnings=True) . prediction_ma=model_ma.predict(len(valid)) y_pred[&quot;MA Model Prediction&quot;]=prediction_ma . model_scores.append(np.sqrt(mean_squared_error(valid[&quot;Confirmed&quot;],prediction_ma))) print(&quot;Root Mean Square Error for MA Model: &quot;,np.sqrt(mean_squared_error(valid[&quot;Confirmed&quot;],prediction_ma))) . Root Mean Square Error for MA Model: 2901478.9273606585 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;MA Model Prediction&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction for Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases MA Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . MA_model_new_prediction=[] for i in range(1,18): MA_model_new_prediction.append(model_ma.predict(len(valid)+i)[-1]) model_predictions[&quot;MA Model Prediction&quot;]=MA_model_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction AR Model Prediction MA Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | 175686390.526716 | 176938616.913241 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | 176528318.654233 | 177872887.185443 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | 177370455.177831 | 178812067.494476 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | 178213091.415752 | 179756157.840340 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | 179057425.154646 | 180705158.223035 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; ARIMA Model (using AUTOARIMA) . model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . model_arima= auto_arima(model_train[&quot;Confirmed&quot;],trace=True, error_action=&#39;ignore&#39;, start_p=1,start_q=1,max_p=3,max_q=3, suppress_warnings=True,stepwise=False,seasonal=False) model_arima.fit(model_train[&quot;Confirmed&quot;]) . ARIMA(0,2,0)(0,0,0)[0] intercept : AIC=11809.478, Time=0.03 sec ARIMA(0,2,1)(0,0,0)[0] intercept : AIC=11785.886, Time=0.09 sec ARIMA(0,2,2)(0,0,0)[0] intercept : AIC=11738.463, Time=0.15 sec ARIMA(0,2,3)(0,0,0)[0] intercept : AIC=11730.267, Time=0.22 sec ARIMA(1,2,0)(0,0,0)[0] intercept : AIC=11798.602, Time=0.04 sec ARIMA(1,2,1)(0,0,0)[0] intercept : AIC=11730.635, Time=0.14 sec ARIMA(1,2,2)(0,0,0)[0] intercept : AIC=11753.103, Time=0.25 sec ARIMA(1,2,3)(0,0,0)[0] intercept : AIC=11835.814, Time=0.40 sec ARIMA(2,2,0)(0,0,0)[0] intercept : AIC=11798.849, Time=0.05 sec ARIMA(2,2,1)(0,0,0)[0] intercept : AIC=11716.037, Time=0.39 sec ARIMA(2,2,2)(0,0,0)[0] intercept : AIC=inf, Time=1.33 sec ARIMA(2,2,3)(0,0,0)[0] intercept : AIC=11543.789, Time=1.30 sec ARIMA(3,2,0)(0,0,0)[0] intercept : AIC=11750.641, Time=0.30 sec ARIMA(3,2,1)(0,0,0)[0] intercept : AIC=11650.242, Time=0.75 sec ARIMA(3,2,2)(0,0,0)[0] intercept : AIC=11534.633, Time=2.28 sec Best model: ARIMA(3,2,2)(0,0,0)[0] intercept Total fit time: 7.778 seconds . ARIMA(order=(3, 2, 2), scoring_args={}, suppress_warnings=True) . prediction_arima=model_arima.predict(len(valid)) y_pred[&quot;ARIMA Model Prediction&quot;]=prediction_arima . model_scores.append(np.sqrt(mean_squared_error(valid[&quot;Confirmed&quot;],prediction_arima))) print(&quot;Root Mean Square Error for ARIMA Model: &quot;,np.sqrt(mean_squared_error(valid[&quot;Confirmed&quot;],prediction_arima))) . Root Mean Square Error for ARIMA Model: 3160128.216756515 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;ARIMA Model Prediction&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction for Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases ARIMA Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . ARIMA_model_new_prediction=[] for i in range(1,18): ARIMA_model_new_prediction.append(model_arima.predict(len(valid)+i)[-1]) model_predictions[&quot;ARIMA Model Prediction&quot;]=ARIMA_model_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction AR Model Prediction MA Model Prediction ARIMA Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | 175686390.526716 | 176938616.913241 | 177416042.776341 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | 176528318.654233 | 177872887.185443 | 178299238.605094 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | 177370455.177831 | 178812067.494476 | 179262949.471225 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | 178213091.415752 | 179756157.840340 | 180311831.173595 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | 179057425.154646 | 180705158.223035 | 181391134.015959 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; SARIMA Model (using AUTO ARIMA) . model_sarima= auto_arima(model_train[&quot;Confirmed&quot;],trace=True, error_action=&#39;ignore&#39;, start_p=0,start_q=0,max_p=2,max_q=2,m=7, suppress_warnings=True,stepwise=True,seasonal=True) model_sarima.fit(model_train[&quot;Confirmed&quot;]) . Performing stepwise search to minimize aic ARIMA(0,2,0)(1,0,1)[7] : AIC=11695.173, Time=0.65 sec ARIMA(0,2,0)(0,0,0)[7] : AIC=11807.732, Time=0.06 sec ARIMA(1,2,0)(1,0,0)[7] : AIC=11687.019, Time=0.24 sec ARIMA(0,2,1)(0,0,1)[7] : AIC=11682.480, Time=0.38 sec ARIMA(0,2,1)(0,0,0)[7] : AIC=11786.601, Time=0.15 sec ARIMA(0,2,1)(1,0,1)[7] : AIC=11593.360, Time=0.96 sec ARIMA(0,2,1)(1,0,0)[7] : AIC=11635.387, Time=0.29 sec ARIMA(0,2,1)(2,0,1)[7] : AIC=11592.530, Time=1.54 sec ARIMA(0,2,1)(2,0,0)[7] : AIC=11623.862, Time=0.63 sec ARIMA(0,2,1)(2,0,2)[7] : AIC=11593.700, Time=3.03 sec ARIMA(0,2,1)(1,0,2)[7] : AIC=11592.018, Time=0.94 sec ARIMA(0,2,1)(0,0,2)[7] : AIC=11661.146, Time=0.49 sec ARIMA(0,2,0)(1,0,2)[7] : AIC=11698.470, Time=0.93 sec ARIMA(1,2,1)(1,0,2)[7] : AIC=11589.999, Time=1.60 sec ARIMA(1,2,1)(0,0,2)[7] : AIC=11649.655, Time=0.86 sec ARIMA(1,2,1)(1,0,1)[7] : AIC=11591.366, Time=0.76 sec ARIMA(1,2,1)(2,0,2)[7] : AIC=11591.685, Time=2.60 sec ARIMA(1,2,1)(0,0,1)[7] : AIC=11664.029, Time=0.46 sec ARIMA(1,2,1)(2,0,1)[7] : AIC=11590.503, Time=1.76 sec ARIMA(1,2,0)(1,0,2)[7] : AIC=11629.360, Time=0.96 sec ARIMA(2,2,1)(1,0,2)[7] : AIC=11575.118, Time=1.45 sec ARIMA(2,2,1)(0,0,2)[7] : AIC=11640.735, Time=0.89 sec ARIMA(2,2,1)(1,0,1)[7] : AIC=11576.924, Time=0.84 sec ARIMA(2,2,1)(2,0,2)[7] : AIC=11576.807, Time=2.32 sec ARIMA(2,2,1)(0,0,1)[7] : AIC=11656.210, Time=0.55 sec ARIMA(2,2,1)(2,0,1)[7] : AIC=11575.667, Time=1.78 sec ARIMA(2,2,0)(1,0,2)[7] : AIC=11617.001, Time=1.11 sec ARIMA(2,2,2)(1,0,2)[7] : AIC=11530.862, Time=5.51 sec ARIMA(2,2,2)(0,0,2)[7] : AIC=11594.983, Time=2.46 sec ARIMA(2,2,2)(1,0,1)[7] : AIC=11529.930, Time=1.75 sec ARIMA(2,2,2)(0,0,1)[7] : AIC=11597.914, Time=1.58 sec ARIMA(2,2,2)(1,0,0)[7] : AIC=11593.771, Time=1.55 sec ARIMA(2,2,2)(2,0,1)[7] : AIC=11531.098, Time=4.12 sec ARIMA(2,2,2)(0,0,0)[7] : AIC=11602.758, Time=0.87 sec ARIMA(2,2,2)(2,0,0)[7] : AIC=11579.930, Time=2.70 sec ARIMA(2,2,2)(2,0,2)[7] : AIC=11532.498, Time=4.01 sec ARIMA(1,2,2)(1,0,1)[7] : AIC=11542.508, Time=1.09 sec ARIMA(2,2,2)(1,0,1)[7] intercept : AIC=inf, Time=2.32 sec Best model: ARIMA(2,2,2)(1,0,1)[7] Total fit time: 56.302 seconds . ARIMA(order=(2, 2, 2), scoring_args={}, seasonal_order=(1, 0, 1, 7), suppress_warnings=True, with_intercept=False) . prediction_sarima=model_sarima.predict(len(valid)) y_pred[&quot;SARIMA Model Prediction&quot;]=prediction_sarima . model_scores.append(np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;SARIMA Model Prediction&quot;]))) print(&quot;Root Mean Square Error for SARIMA Model: &quot;,np.sqrt(mean_squared_error(y_pred[&quot;Confirmed&quot;],y_pred[&quot;SARIMA Model Prediction&quot;]))) . Root Mean Square Error for SARIMA Model: 2357813.584875235 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Confirmed Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Confirmed&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Confirmed Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;SARIMA Model Prediction&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction for Confirmed Cases&quot;,)) fig.update_layout(title=&quot;Confirmed Cases SARIMA Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Confirmed Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . SARIMA_model_new_prediction=[] for i in range(1,18): SARIMA_model_new_prediction.append(model_sarima.predict(len(valid)+i)[-1]) model_predictions[&quot;SARIMA Model Prediction&quot;]=SARIMA_model_new_prediction model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction AR Model Prediction MA Model Prediction ARIMA Model Prediction SARIMA Model Prediction . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | 175686390.526716 | 176938616.913241 | 177416042.776341 | 175731193.635291 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | 176528318.654233 | 177872887.185443 | 178299238.605094 | 176486478.773622 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | 177370455.177831 | 178812067.494476 | 179262949.471225 | 177346177.932718 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | 178213091.415752 | 179756157.840340 | 180311831.173595 | 178259953.574374 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | 179057425.154646 | 180705158.223035 | 181391134.015959 | 179192135.442158 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Facebook&#39;s Prophet Model for forecasting . prophet_c=Prophet(interval_width=0.95,weekly_seasonality=True,) prophet_confirmed=pd.DataFrame(zip(list(datewise.index),list(datewise[&quot;Confirmed&quot;])),columns=[&#39;ds&#39;,&#39;y&#39;]) . prophet_c.fit(prophet_confirmed) . INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. DEBUG:cmdstanpy:input tempfile: /tmp/tmp1hdn0j_9/621fvi80.json DEBUG:cmdstanpy:input tempfile: /tmp/tmp1hdn0j_9/n1yc24i_.json DEBUG:cmdstanpy:idx 0 DEBUG:cmdstanpy:running CmdStan, num_threads: None DEBUG:cmdstanpy:CmdStan args: [&#39;/usr/local/lib/python3.7/dist-packages/prophet/stan_model/prophet_model.bin&#39;, &#39;random&#39;, &#39;seed=32513&#39;, &#39;data&#39;, &#39;file=/tmp/tmp1hdn0j_9/621fvi80.json&#39;, &#39;init=/tmp/tmp1hdn0j_9/n1yc24i_.json&#39;, &#39;output&#39;, &#39;file=/tmp/tmpgh2viftq/prophet_model-20220911120844.csv&#39;, &#39;method=optimize&#39;, &#39;algorithm=lbfgs&#39;, &#39;iter=10000&#39;] 12:08:44 - cmdstanpy - INFO - Chain [1] start processing INFO:cmdstanpy:Chain [1] start processing 12:08:44 - cmdstanpy - INFO - Chain [1] done processing INFO:cmdstanpy:Chain [1] done processing . &lt;prophet.forecaster.Prophet at 0x7f19be49f1d0&gt; . forecast_c=prophet_c.make_future_dataframe(periods=17) forecast_confirmed=forecast_c.copy() . confirmed_forecast=prophet_c.predict(forecast_c) #print(confirmed_forecast[[&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]]) . model_scores.append(np.sqrt(mean_squared_error(datewise[&quot;Confirmed&quot;],confirmed_forecast[&#39;yhat&#39;].head(datewise.shape[0])))) print(&quot;Root Mean Squared Error for Prophet Model: &quot;,np.sqrt(mean_squared_error(datewise[&quot;Confirmed&quot;],confirmed_forecast[&#39;yhat&#39;].head(datewise.shape[0])))) . Root Mean Squared Error for Prophet Model: 1027618.2849851169 . print(prophet_c.plot(confirmed_forecast)) . Figure(720x432) . print(prophet_c.plot_components(confirmed_forecast)) . Figure(648x432) . Summarization of Forecasts using different Models . model_names=[&quot;Linear Regression&quot;,&quot;Polynomial Regression&quot;,&quot;Support Vector Machine Regressor&quot;,&quot;Holt&#39;s Linear&quot;,&quot;Holt&#39;s Winter Model&quot;, &quot;Auto Regressive Model (AR)&quot;,&quot;Moving Average Model (MA)&quot;,&quot;ARIMA Model&quot;,&quot;SARIMA Model&quot;,&quot;Facebook&#39;s Prophet Model&quot;] model_summary=pd.DataFrame(zip(model_names,model_scores),columns=[&quot;Model Name&quot;,&quot;Root Mean Squared Error&quot;]).sort_values([&quot;Root Mean Squared Error&quot;]) model_summary . Model Name Root Mean Squared Error . 9 Facebook&#39;s Prophet Model | 1027618.284985 | . 3 Holt&#39;s Linear | 1696111.792446 | . 5 Auto Regressive Model (AR) | 2350964.490322 | . 8 SARIMA Model | 2357813.584875 | . 4 Holt&#39;s Winter Model | 2594639.668226 | . 6 Moving Average Model (MA) | 2901478.927361 | . 7 ARIMA Model | 3160128.216757 | . 1 Polynomial Regression | 27362958.416571 | . 2 Support Vector Machine Regressor | 27435923.216931 | . 0 Linear Regression | 33541511.296706 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; model_predictions[&quot;Prophet&#39;s Prediction&quot;]=list(confirmed_forecast[&quot;yhat&quot;].tail(17)) model_predictions[&quot;Prophet&#39;s Upper Bound&quot;]=list(confirmed_forecast[&quot;yhat_upper&quot;].tail(17)) model_predictions.head() . Dates Linear Regression Prediction Polynonmial Regression Prediction SVM Prediction Holt&#39;s Linear Model Prediction Holt&#39;s Winter Model Prediction AR Model Prediction MA Model Prediction ARIMA Model Prediction SARIMA Model Prediction Prophet&#39;s Prediction Prophet&#39;s Upper Bound . 0 2021-05-30 | 134093578.446173 | 226028087.475372 | 216458436.483116 | 174366504.894772 | 175873980.988871 | 175686390.526716 | 176938616.913241 | 177416042.776341 | 175731193.635291 | 169459809.090123 | 171483558.872668 | . 1 2021-05-31 | 134427502.855213 | 230137725.685058 | 218802891.161817 | 175133215.804870 | 176535478.925915 | 176528318.654233 | 177872887.185443 | 178299238.605094 | 176486478.773622 | 170030833.121070 | 172047244.682688 | . 2 2021-06-01 | 134761427.264252 | 234392252.930976 | 221171147.183054 | 175899926.714968 | 177370523.430426 | 177370455.177831 | 178812067.494476 | 179262949.471225 | 177346177.932718 | 170654717.260170 | 172588532.062770 | . 3 2021-06-02 | 135095351.673291 | 238795909.836299 | 223563397.464129 | 176666637.625065 | 178275543.880503 | 178213091.415752 | 179756157.840340 | 180311831.173595 | 178259953.574374 | 171315069.571070 | 173389186.329374 | . 4 2021-06-03 | 135429276.082331 | 243353027.074790 | 225979836.092718 | 177433348.535163 | 179241090.821188 | 179057425.154646 | 180705158.223035 | 181391134.015959 | 179192135.442158 | 171988246.771603 | 174029579.064819 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Time Series Forecasting for Death Cases . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Deaths&quot;], mode=&#39;lines+markers&#39;,name=&quot;Death Cases&quot;)) fig.update_layout(title=&quot;Death Cases&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Number of Death Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . model_train=datewise.iloc[:int(datewise.shape[0]*0.95)] valid=datewise.iloc[int(datewise.shape[0]*0.95):] y_pred=valid.copy() . model_arima_deaths=auto_arima(model_train[&quot;Deaths&quot;],trace=True, error_action=&#39;ignore&#39;, start_p=0,start_q=0, max_p=5,max_q=5,suppress_warnings=True,stepwise=False,seasonal=False) model_arima_deaths.fit(model_train[&quot;Deaths&quot;]) . ARIMA(0,2,0)(0,0,0)[0] intercept : AIC=8372.566, Time=0.03 sec ARIMA(0,2,1)(0,0,0)[0] intercept : AIC=8373.587, Time=0.08 sec ARIMA(0,2,2)(0,0,0)[0] intercept : AIC=8257.762, Time=0.42 sec ARIMA(0,2,3)(0,0,0)[0] intercept : AIC=8225.544, Time=1.01 sec ARIMA(0,2,4)(0,0,0)[0] intercept : AIC=8133.811, Time=1.16 sec ARIMA(0,2,5)(0,0,0)[0] intercept : AIC=8117.849, Time=1.32 sec ARIMA(1,2,0)(0,0,0)[0] intercept : AIC=8374.053, Time=0.04 sec ARIMA(1,2,1)(0,0,0)[0] intercept : AIC=8283.551, Time=0.89 sec ARIMA(1,2,2)(0,0,0)[0] intercept : AIC=8246.770, Time=0.95 sec ARIMA(1,2,3)(0,0,0)[0] intercept : AIC=8206.501, Time=1.20 sec ARIMA(1,2,4)(0,0,0)[0] intercept : AIC=8114.336, Time=1.41 sec ARIMA(2,2,0)(0,0,0)[0] intercept : AIC=8343.019, Time=0.08 sec ARIMA(2,2,1)(0,0,0)[0] intercept : AIC=8201.101, Time=0.83 sec ARIMA(2,2,2)(0,0,0)[0] intercept : AIC=8079.130, Time=1.17 sec ARIMA(2,2,3)(0,0,0)[0] intercept : AIC=8032.299, Time=1.31 sec ARIMA(3,2,0)(0,0,0)[0] intercept : AIC=8298.821, Time=0.14 sec ARIMA(3,2,1)(0,0,0)[0] intercept : AIC=8164.503, Time=1.03 sec ARIMA(3,2,2)(0,0,0)[0] intercept : AIC=8206.177, Time=1.27 sec ARIMA(4,2,0)(0,0,0)[0] intercept : AIC=8268.916, Time=0.21 sec ARIMA(4,2,1)(0,0,0)[0] intercept : AIC=8129.881, Time=0.88 sec ARIMA(5,2,0)(0,0,0)[0] intercept : AIC=7978.749, Time=0.30 sec Best model: ARIMA(5,2,0)(0,0,0)[0] intercept Total fit time: 15.803 seconds . ARIMA(order=(5, 2, 0), scoring_args={}, suppress_warnings=True) . predictions_deaths=model_arima_deaths.predict(len(valid)) y_pred[&quot;ARIMA Death Prediction&quot;]=predictions_deaths . print(&quot;Root Mean Square Error: &quot;,np.sqrt(mean_squared_error(valid[&quot;Deaths&quot;],predictions_deaths))) . Root Mean Square Error: 16995.356616234585 . fig=go.Figure() fig.add_trace(go.Scatter(x=model_train.index, y=model_train[&quot;Deaths&quot;], mode=&#39;lines+markers&#39;,name=&quot;Train Data for Death Cases&quot;)) fig.add_trace(go.Scatter(x=valid.index, y=valid[&quot;Deaths&quot;], mode=&#39;lines+markers&#39;,name=&quot;Validation Data for Death Cases&quot;,)) fig.add_trace(go.Scatter(x=valid.index, y=y_pred[&quot;ARIMA Death Prediction&quot;], mode=&#39;lines+markers&#39;,name=&quot;Prediction for Death Cases&quot;,)) fig.update_layout(title=&quot;Death Cases ARIMA Model Prediction&quot;, xaxis_title=&quot;Date&quot;,yaxis_title=&quot;Death Cases&quot;,legend=dict(x=0,y=1,traceorder=&quot;normal&quot;)) fig.show() . . . ARIMA_model_death_forecast=[] for i in range(1,18): ARIMA_model_death_forecast.append(model_arima_deaths.predict(len(valid)+i)[-1]) . pd.DataFrame(zip(new_date,ARIMA_model_death_forecast),columns=[&quot;Deaths&quot;,&quot;ARIMA Model Death Forecast&quot;]).head() . Deaths ARIMA Model Death Forecast . 0 2021-05-30 | 3581571.344507 | . 1 2021-05-31 | 3595243.649509 | . 2 2021-06-01 | 3609474.437173 | . 3 2021-06-02 | 3624071.061656 | . 4 2021-06-03 | 3638565.543238 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Conclusion . COVID-19 doesn&#39;t have very high mortatlity rate as we can see which is the most positive take away. Also the healthy Recovery Rate implies the disease is cureable. The only matter of concern is the exponential growth rate of infection. . Countries like USA, Spain, United Kingdom,and Italy are facing some serious trouble in containing the disease showing how deadly the neglegence can lead to. The need of the hour is to perform COVID-19 pendemic controlling practices like Testing, Contact Tracing and Quarantine with a speed greater than the speed of disease spread at each country level. . The reason of putting this graph in the conclusion, there is an interesting pattern to observe here, Everytime there has been drop in World&#39;s Carbon emission, the world economy crashed. A one classic example is 2008 recession. I think most of you must have already guessed what&#39;s ahead, probably COVID-19 is just a big wave with a Tsunami of Recession or Depression following it. . The growth of Confirmed and Death Cases seems to have slowed down since past few days. Which is really good sign. Hope this goes like that for a brief period. There should not be any new country emerging as the new epicenter of COVID-19 just like USA happened to be that epicenter for brief period. In case if any new country emerges as new epicenter, the Growth of Confirmed Cases will shoot up again. .",
            "url": "https://janmejaybhoi.github.io/w/time%20series%20analysis/survival%20analysis/autoregression/moving%20average/prophet/2022/01/06/Covid_EDA_Forecasting.html",
            "relUrl": "/time%20series%20analysis/survival%20analysis/autoregression/moving%20average/prophet/2022/01/06/Covid_EDA_Forecasting.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Scene Classification with GradCam Visualization",
            "content": ". Overview . This notebook will examine behaviors of a visual explanation methods of deep learning model. The model will train classifying to 6 classes (buildings, forest, glacier, mountain, sea, street) for each images using this datasets. The architecture of the model is a self prepared ResNet18_ . Visual explanation methods that will be examined are . - Grad-CAM https://arxiv.org/abs/1610.02391 . Why this is Useful? . To help deep learning practitioners visually debug their models and properly understand where it’s “looking” in an image, Selvaraju et al. created Gradient-weighted Class Activation Mapping, or more simply, Grad-CAM . | Grad-CAM uses the gradients of any target concept (say logits for “dog” or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. . | . Using Kaggle API to download dataset into Colab Environment . !pip install -q kaggle . from google.colab import files files.upload() . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;janmejaybhoi&#34;,&#34;key&#34;:&#34;6cdf252d44b3db77a67e30fff01bf8a0&#34;}&#39;} . !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ . ! chmod 600 ~/.kaggle/kaggle.json . !kaggle datasets download -d puneet6060/intel-image-classification . Downloading intel-image-classification.zip to /content 100% 345M/346M [00:02&lt;00:00, 131MB/s] 100% 346M/346M [00:02&lt;00:00, 131MB/s] . !unzip /content/intel-image-classification . Import necessary libraries . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import random import tensorflow as tf from tensorflow import keras from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 from tensorflow.keras.layers import * from tensorflow.keras.models import Model, load_model from tensorflow.keras.initializers import glorot_uniform from tensorflow.keras.utils import plot_model from tensorflow.keras import backend as K from tensorflow.keras.optimizers import SGD from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler from sklearn.metrics import confusion_matrix, classification_report, accuracy_score from random import sample from IPython.display import display import os import PIL . Read Data . train = {} test = {} path = &quot;/content/intel-image-classification&quot; # Make dictionary storing images for each category under train data. path_train = os.path.join(path, &quot;seg_train/seg_train&quot;) for i in os.listdir(path_train): train[i] = os.listdir(os.path.join(path_train, i)) # Make dictionary storing images for each category under test data. path_test = os.path.join(path, &quot;seg_test/seg_test&quot;) for i in os.listdir(path_test): test[i] = os.listdir(os.path.join(path_test, i)) . Explore data . len_train = np.concatenate(list(train.values())).shape[0] len_test = np.concatenate(list(test.values())).shape[0] print(&quot;Number of images in training data : {}&quot;.format(len_train)) print(&quot;Number of images in testing data : {}&quot;.format(len_test)) . Number of images in training data : 14034 Number of images in testing data : 3000 . # You will see different images each time. fig, axs = plt.subplots(6, 5, figsize = (15, 15)) for i, item in enumerate(os.listdir(path_train)): images = sample(train[item], 5) for j, image in enumerate(images): img = PIL.Image.open(os.path.join(path_train, item, image)) axs[i, j].imshow(img) axs[i, j].set(xlabel = item, xticks = [], yticks = []) fig.tight_layout() . for item in train.keys(): print(item, len(train[item])) . sea 2274 glacier 2404 buildings 2191 street 2382 forest 2271 mountain 2512 . # This is often useful when you want your dataset to be balanced. fig, ax = plt.subplots() ax.pie( [len(train[item]) for item in train], labels = train.keys(), autopct = &quot;%1.1f%%&quot; ) fig.show() . Augment data . # Here we go with zooming, flipping (horizontally and vertically), and rescaling. train_datagen = ImageDataGenerator( zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, rescale=1./255 ) # For test data we only rescale the data. # Never augment test data!!! test_datagen = ImageDataGenerator(rescale=1./255) . Create data generator . # This will make images (including augmented ones) start flowing from the directory to the model. # Note that augmented images are not stored along with the original images. The process happens in memory. # Train generator train_generator = train_datagen.flow_from_directory( path_train, target_size=(256, 256), batch_size=32, class_mode=&#39;categorical&#39; ) # Test generator test_generator = test_datagen.flow_from_directory( path_test, target_size=(256, 256), batch_size=32, class_mode=&#39;categorical&#39; ) . Found 14034 images belonging to 6 classes. Found 3000 images belonging to 6 classes. . Develop Neural Network Architecture . # You can use a different architecture if you like. def res_block(X, filter, stage): # Convolutional_block X_copy = X f1 , f2, f3 = filter # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_conv_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = MaxPool2D((2,2))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_conv_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_conv_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_c&#39;)(X) # Short path X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_conv_copy&#39;, kernel_initializer= glorot_uniform(seed = 0))(X_copy) X_copy = MaxPool2D((2,2))(X_copy) X_copy = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_conv_copy&#39;)(X_copy) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) # Identity Block 1 X_copy = X # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_identity_1_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_identity_1_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_identity_1_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis = 3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_1_c&#39;)(X) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) # Identity Block 2 X_copy = X # Main Path X = Conv2D(f1, (1,1),strides = (1,1), name =&#39;res_&#39;+str(stage)+&#39;_identity_2_a&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_a&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = &#39;same&#39;, name =&#39;res_&#39;+str(stage)+&#39;_identity_2_b&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_b&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name =&#39;res_&#39;+str(stage)+&#39;_identity_2_c&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_&#39;+str(stage)+&#39;_identity_2_c&#39;)(X) # ADD X = Add()([X, X_copy]) X = Activation(&#39;relu&#39;)(X) return X . input_shape = (256,256,3) # Input tensor shape X_input = Input(input_shape) # Zero-padding X = ZeroPadding2D((3,3))(X_input) # 1- stage X = Conv2D(64, (7,7), strides= (2,2), name = &#39;conv1&#39;, kernel_initializer= glorot_uniform(seed = 0))(X) X = BatchNormalization(axis =3, name = &#39;bn_conv1&#39;)(X) X = Activation(&#39;relu&#39;)(X) X = MaxPooling2D((3,3), strides= (2,2))(X) # 2- stage X = res_block(X, filter= [64,64,256], stage= 2) # 3- stage X = res_block(X, filter= [128,128,512], stage= 3) # 4- stage X = res_block(X, filter= [256,256,1024], stage= 4) # 5- stage X = res_block(X, filter= [512,512,2048], stage= 5) # Average Pooling X = AveragePooling2D((2,2), name = &#39;Averagea_Pooling&#39;)(X) # Final layer X = Flatten()(X) X = Dropout(0.4)(X) X = Dense(6, activation = &#39;softmax&#39;, name = &#39;Dense_final&#39;, kernel_initializer= glorot_uniform(seed=0))(X) # Build model. model = Model( inputs= X_input, outputs = X, name = &#39;Resnet18&#39; ) # Check out model summary. model.summary() . Model: &#34;Resnet18&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 256, 256, 3) 0 __________________________________________________________________________________________________ zero_padding2d (ZeroPadding2D) (None, 262, 262, 3) 0 input_1[0][0] __________________________________________________________________________________________________ conv1 (Conv2D) (None, 128, 128, 64) 9472 zero_padding2d[0][0] __________________________________________________________________________________________________ bn_conv1 (BatchNormalization) (None, 128, 128, 64) 256 conv1[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 128, 128, 64) 0 bn_conv1[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 63, 63, 64) 0 activation[0][0] __________________________________________________________________________________________________ res_2_conv_a (Conv2D) (None, 63, 63, 64) 4160 max_pooling2d[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 31, 31, 64) 0 res_2_conv_a[0][0] __________________________________________________________________________________________________ bn_2_conv_a (BatchNormalization (None, 31, 31, 64) 256 max_pooling2d_1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 31, 31, 64) 0 bn_2_conv_a[0][0] __________________________________________________________________________________________________ res_2_conv_b (Conv2D) (None, 31, 31, 64) 36928 activation_1[0][0] __________________________________________________________________________________________________ bn_2_conv_b (BatchNormalization (None, 31, 31, 64) 256 res_2_conv_b[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 31, 31, 64) 0 bn_2_conv_b[0][0] __________________________________________________________________________________________________ res_2_conv_copy (Conv2D) (None, 63, 63, 256) 16640 max_pooling2d[0][0] __________________________________________________________________________________________________ res_2_conv_c (Conv2D) (None, 31, 31, 256) 16640 activation_2[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 31, 31, 256) 0 res_2_conv_copy[0][0] __________________________________________________________________________________________________ bn_2_conv_c (BatchNormalization (None, 31, 31, 256) 1024 res_2_conv_c[0][0] __________________________________________________________________________________________________ bn_2_conv_copy (BatchNormalizat (None, 31, 31, 256) 1024 max_pooling2d_2[0][0] __________________________________________________________________________________________________ add (Add) (None, 31, 31, 256) 0 bn_2_conv_c[0][0] bn_2_conv_copy[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 31, 31, 256) 0 add[0][0] __________________________________________________________________________________________________ res_2_identity_1_a (Conv2D) (None, 31, 31, 64) 16448 activation_3[0][0] __________________________________________________________________________________________________ bn_2_identity_1_a (BatchNormali (None, 31, 31, 64) 256 res_2_identity_1_a[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 31, 31, 64) 0 bn_2_identity_1_a[0][0] __________________________________________________________________________________________________ res_2_identity_1_b (Conv2D) (None, 31, 31, 64) 36928 activation_4[0][0] __________________________________________________________________________________________________ bn_2_identity_1_b (BatchNormali (None, 31, 31, 64) 256 res_2_identity_1_b[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 31, 31, 64) 0 bn_2_identity_1_b[0][0] __________________________________________________________________________________________________ res_2_identity_1_c (Conv2D) (None, 31, 31, 256) 16640 activation_5[0][0] __________________________________________________________________________________________________ bn_2_identity_1_c (BatchNormali (None, 31, 31, 256) 1024 res_2_identity_1_c[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 31, 31, 256) 0 bn_2_identity_1_c[0][0] activation_3[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 31, 31, 256) 0 add_1[0][0] __________________________________________________________________________________________________ res_2_identity_2_a (Conv2D) (None, 31, 31, 64) 16448 activation_6[0][0] __________________________________________________________________________________________________ bn_2_identity_2_a (BatchNormali (None, 31, 31, 64) 256 res_2_identity_2_a[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 31, 31, 64) 0 bn_2_identity_2_a[0][0] __________________________________________________________________________________________________ res_2_identity_2_b (Conv2D) (None, 31, 31, 64) 36928 activation_7[0][0] __________________________________________________________________________________________________ bn_2_identity_2_b (BatchNormali (None, 31, 31, 64) 256 res_2_identity_2_b[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 31, 31, 64) 0 bn_2_identity_2_b[0][0] __________________________________________________________________________________________________ res_2_identity_2_c (Conv2D) (None, 31, 31, 256) 16640 activation_8[0][0] __________________________________________________________________________________________________ bn_2_identity_2_c (BatchNormali (None, 31, 31, 256) 1024 res_2_identity_2_c[0][0] __________________________________________________________________________________________________ add_2 (Add) (None, 31, 31, 256) 0 bn_2_identity_2_c[0][0] activation_6[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 31, 31, 256) 0 add_2[0][0] __________________________________________________________________________________________________ res_3_conv_a (Conv2D) (None, 31, 31, 128) 32896 activation_9[0][0] __________________________________________________________________________________________________ max_pooling2d_3 (MaxPooling2D) (None, 15, 15, 128) 0 res_3_conv_a[0][0] __________________________________________________________________________________________________ bn_3_conv_a (BatchNormalization (None, 15, 15, 128) 512 max_pooling2d_3[0][0] __________________________________________________________________________________________________ activation_10 (Activation) (None, 15, 15, 128) 0 bn_3_conv_a[0][0] __________________________________________________________________________________________________ res_3_conv_b (Conv2D) (None, 15, 15, 128) 147584 activation_10[0][0] __________________________________________________________________________________________________ bn_3_conv_b (BatchNormalization (None, 15, 15, 128) 512 res_3_conv_b[0][0] __________________________________________________________________________________________________ activation_11 (Activation) (None, 15, 15, 128) 0 bn_3_conv_b[0][0] __________________________________________________________________________________________________ res_3_conv_copy (Conv2D) (None, 31, 31, 512) 131584 activation_9[0][0] __________________________________________________________________________________________________ res_3_conv_c (Conv2D) (None, 15, 15, 512) 66048 activation_11[0][0] __________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 15, 15, 512) 0 res_3_conv_copy[0][0] __________________________________________________________________________________________________ bn_3_conv_c (BatchNormalization (None, 15, 15, 512) 2048 res_3_conv_c[0][0] __________________________________________________________________________________________________ bn_3_conv_copy (BatchNormalizat (None, 15, 15, 512) 2048 max_pooling2d_4[0][0] __________________________________________________________________________________________________ add_3 (Add) (None, 15, 15, 512) 0 bn_3_conv_c[0][0] bn_3_conv_copy[0][0] __________________________________________________________________________________________________ activation_12 (Activation) (None, 15, 15, 512) 0 add_3[0][0] __________________________________________________________________________________________________ res_3_identity_1_a (Conv2D) (None, 15, 15, 128) 65664 activation_12[0][0] __________________________________________________________________________________________________ bn_3_identity_1_a (BatchNormali (None, 15, 15, 128) 512 res_3_identity_1_a[0][0] __________________________________________________________________________________________________ activation_13 (Activation) (None, 15, 15, 128) 0 bn_3_identity_1_a[0][0] __________________________________________________________________________________________________ res_3_identity_1_b (Conv2D) (None, 15, 15, 128) 147584 activation_13[0][0] __________________________________________________________________________________________________ bn_3_identity_1_b (BatchNormali (None, 15, 15, 128) 512 res_3_identity_1_b[0][0] __________________________________________________________________________________________________ activation_14 (Activation) (None, 15, 15, 128) 0 bn_3_identity_1_b[0][0] __________________________________________________________________________________________________ res_3_identity_1_c (Conv2D) (None, 15, 15, 512) 66048 activation_14[0][0] __________________________________________________________________________________________________ bn_3_identity_1_c (BatchNormali (None, 15, 15, 512) 2048 res_3_identity_1_c[0][0] __________________________________________________________________________________________________ add_4 (Add) (None, 15, 15, 512) 0 bn_3_identity_1_c[0][0] activation_12[0][0] __________________________________________________________________________________________________ activation_15 (Activation) (None, 15, 15, 512) 0 add_4[0][0] __________________________________________________________________________________________________ res_3_identity_2_a (Conv2D) (None, 15, 15, 128) 65664 activation_15[0][0] __________________________________________________________________________________________________ bn_3_identity_2_a (BatchNormali (None, 15, 15, 128) 512 res_3_identity_2_a[0][0] __________________________________________________________________________________________________ activation_16 (Activation) (None, 15, 15, 128) 0 bn_3_identity_2_a[0][0] __________________________________________________________________________________________________ res_3_identity_2_b (Conv2D) (None, 15, 15, 128) 147584 activation_16[0][0] __________________________________________________________________________________________________ bn_3_identity_2_b (BatchNormali (None, 15, 15, 128) 512 res_3_identity_2_b[0][0] __________________________________________________________________________________________________ activation_17 (Activation) (None, 15, 15, 128) 0 bn_3_identity_2_b[0][0] __________________________________________________________________________________________________ res_3_identity_2_c (Conv2D) (None, 15, 15, 512) 66048 activation_17[0][0] __________________________________________________________________________________________________ bn_3_identity_2_c (BatchNormali (None, 15, 15, 512) 2048 res_3_identity_2_c[0][0] __________________________________________________________________________________________________ add_5 (Add) (None, 15, 15, 512) 0 bn_3_identity_2_c[0][0] activation_15[0][0] __________________________________________________________________________________________________ activation_18 (Activation) (None, 15, 15, 512) 0 add_5[0][0] __________________________________________________________________________________________________ res_4_conv_a (Conv2D) (None, 15, 15, 256) 131328 activation_18[0][0] __________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 7, 7, 256) 0 res_4_conv_a[0][0] __________________________________________________________________________________________________ bn_4_conv_a (BatchNormalization (None, 7, 7, 256) 1024 max_pooling2d_5[0][0] __________________________________________________________________________________________________ activation_19 (Activation) (None, 7, 7, 256) 0 bn_4_conv_a[0][0] __________________________________________________________________________________________________ res_4_conv_b (Conv2D) (None, 7, 7, 256) 590080 activation_19[0][0] __________________________________________________________________________________________________ bn_4_conv_b (BatchNormalization (None, 7, 7, 256) 1024 res_4_conv_b[0][0] __________________________________________________________________________________________________ activation_20 (Activation) (None, 7, 7, 256) 0 bn_4_conv_b[0][0] __________________________________________________________________________________________________ res_4_conv_copy (Conv2D) (None, 15, 15, 1024) 525312 activation_18[0][0] __________________________________________________________________________________________________ res_4_conv_c (Conv2D) (None, 7, 7, 1024) 263168 activation_20[0][0] __________________________________________________________________________________________________ max_pooling2d_6 (MaxPooling2D) (None, 7, 7, 1024) 0 res_4_conv_copy[0][0] __________________________________________________________________________________________________ bn_4_conv_c (BatchNormalization (None, 7, 7, 1024) 4096 res_4_conv_c[0][0] __________________________________________________________________________________________________ bn_4_conv_copy (BatchNormalizat (None, 7, 7, 1024) 4096 max_pooling2d_6[0][0] __________________________________________________________________________________________________ add_6 (Add) (None, 7, 7, 1024) 0 bn_4_conv_c[0][0] bn_4_conv_copy[0][0] __________________________________________________________________________________________________ activation_21 (Activation) (None, 7, 7, 1024) 0 add_6[0][0] __________________________________________________________________________________________________ res_4_identity_1_a (Conv2D) (None, 7, 7, 256) 262400 activation_21[0][0] __________________________________________________________________________________________________ bn_4_identity_1_a (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_1_a[0][0] __________________________________________________________________________________________________ activation_22 (Activation) (None, 7, 7, 256) 0 bn_4_identity_1_a[0][0] __________________________________________________________________________________________________ res_4_identity_1_b (Conv2D) (None, 7, 7, 256) 590080 activation_22[0][0] __________________________________________________________________________________________________ bn_4_identity_1_b (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_1_b[0][0] __________________________________________________________________________________________________ activation_23 (Activation) (None, 7, 7, 256) 0 bn_4_identity_1_b[0][0] __________________________________________________________________________________________________ res_4_identity_1_c (Conv2D) (None, 7, 7, 1024) 263168 activation_23[0][0] __________________________________________________________________________________________________ bn_4_identity_1_c (BatchNormali (None, 7, 7, 1024) 4096 res_4_identity_1_c[0][0] __________________________________________________________________________________________________ add_7 (Add) (None, 7, 7, 1024) 0 bn_4_identity_1_c[0][0] activation_21[0][0] __________________________________________________________________________________________________ activation_24 (Activation) (None, 7, 7, 1024) 0 add_7[0][0] __________________________________________________________________________________________________ res_4_identity_2_a (Conv2D) (None, 7, 7, 256) 262400 activation_24[0][0] __________________________________________________________________________________________________ bn_4_identity_2_a (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_2_a[0][0] __________________________________________________________________________________________________ activation_25 (Activation) (None, 7, 7, 256) 0 bn_4_identity_2_a[0][0] __________________________________________________________________________________________________ res_4_identity_2_b (Conv2D) (None, 7, 7, 256) 590080 activation_25[0][0] __________________________________________________________________________________________________ bn_4_identity_2_b (BatchNormali (None, 7, 7, 256) 1024 res_4_identity_2_b[0][0] __________________________________________________________________________________________________ activation_26 (Activation) (None, 7, 7, 256) 0 bn_4_identity_2_b[0][0] __________________________________________________________________________________________________ res_4_identity_2_c (Conv2D) (None, 7, 7, 1024) 263168 activation_26[0][0] __________________________________________________________________________________________________ bn_4_identity_2_c (BatchNormali (None, 7, 7, 1024) 4096 res_4_identity_2_c[0][0] __________________________________________________________________________________________________ add_8 (Add) (None, 7, 7, 1024) 0 bn_4_identity_2_c[0][0] activation_24[0][0] __________________________________________________________________________________________________ activation_27 (Activation) (None, 7, 7, 1024) 0 add_8[0][0] __________________________________________________________________________________________________ res_5_conv_a (Conv2D) (None, 7, 7, 512) 524800 activation_27[0][0] __________________________________________________________________________________________________ max_pooling2d_7 (MaxPooling2D) (None, 3, 3, 512) 0 res_5_conv_a[0][0] __________________________________________________________________________________________________ bn_5_conv_a (BatchNormalization (None, 3, 3, 512) 2048 max_pooling2d_7[0][0] __________________________________________________________________________________________________ activation_28 (Activation) (None, 3, 3, 512) 0 bn_5_conv_a[0][0] __________________________________________________________________________________________________ res_5_conv_b (Conv2D) (None, 3, 3, 512) 2359808 activation_28[0][0] __________________________________________________________________________________________________ bn_5_conv_b (BatchNormalization (None, 3, 3, 512) 2048 res_5_conv_b[0][0] __________________________________________________________________________________________________ activation_29 (Activation) (None, 3, 3, 512) 0 bn_5_conv_b[0][0] __________________________________________________________________________________________________ res_5_conv_copy (Conv2D) (None, 7, 7, 2048) 2099200 activation_27[0][0] __________________________________________________________________________________________________ res_5_conv_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_29[0][0] __________________________________________________________________________________________________ max_pooling2d_8 (MaxPooling2D) (None, 3, 3, 2048) 0 res_5_conv_copy[0][0] __________________________________________________________________________________________________ bn_5_conv_c (BatchNormalization (None, 3, 3, 2048) 8192 res_5_conv_c[0][0] __________________________________________________________________________________________________ bn_5_conv_copy (BatchNormalizat (None, 3, 3, 2048) 8192 max_pooling2d_8[0][0] __________________________________________________________________________________________________ add_9 (Add) (None, 3, 3, 2048) 0 bn_5_conv_c[0][0] bn_5_conv_copy[0][0] __________________________________________________________________________________________________ activation_30 (Activation) (None, 3, 3, 2048) 0 add_9[0][0] __________________________________________________________________________________________________ res_5_identity_1_a (Conv2D) (None, 3, 3, 512) 1049088 activation_30[0][0] __________________________________________________________________________________________________ bn_5_identity_1_a (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_1_a[0][0] __________________________________________________________________________________________________ activation_31 (Activation) (None, 3, 3, 512) 0 bn_5_identity_1_a[0][0] __________________________________________________________________________________________________ res_5_identity_1_b (Conv2D) (None, 3, 3, 512) 2359808 activation_31[0][0] __________________________________________________________________________________________________ bn_5_identity_1_b (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_1_b[0][0] __________________________________________________________________________________________________ activation_32 (Activation) (None, 3, 3, 512) 0 bn_5_identity_1_b[0][0] __________________________________________________________________________________________________ res_5_identity_1_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_32[0][0] __________________________________________________________________________________________________ bn_5_identity_1_c (BatchNormali (None, 3, 3, 2048) 8192 res_5_identity_1_c[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 3, 3, 2048) 0 bn_5_identity_1_c[0][0] activation_30[0][0] __________________________________________________________________________________________________ activation_33 (Activation) (None, 3, 3, 2048) 0 add_10[0][0] __________________________________________________________________________________________________ res_5_identity_2_a (Conv2D) (None, 3, 3, 512) 1049088 activation_33[0][0] __________________________________________________________________________________________________ bn_5_identity_2_a (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_2_a[0][0] __________________________________________________________________________________________________ activation_34 (Activation) (None, 3, 3, 512) 0 bn_5_identity_2_a[0][0] __________________________________________________________________________________________________ res_5_identity_2_b (Conv2D) (None, 3, 3, 512) 2359808 activation_34[0][0] __________________________________________________________________________________________________ bn_5_identity_2_b (BatchNormali (None, 3, 3, 512) 2048 res_5_identity_2_b[0][0] __________________________________________________________________________________________________ activation_35 (Activation) (None, 3, 3, 512) 0 bn_5_identity_2_b[0][0] __________________________________________________________________________________________________ res_5_identity_2_c (Conv2D) (None, 3, 3, 2048) 1050624 activation_35[0][0] __________________________________________________________________________________________________ bn_5_identity_2_c (BatchNormali (None, 3, 3, 2048) 8192 res_5_identity_2_c[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 3, 3, 2048) 0 bn_5_identity_2_c[0][0] activation_33[0][0] __________________________________________________________________________________________________ activation_36 (Activation) (None, 3, 3, 2048) 0 add_11[0][0] __________________________________________________________________________________________________ Averagea_Pooling (AveragePoolin (None, 1, 1, 2048) 0 activation_36[0][0] __________________________________________________________________________________________________ flatten (Flatten) (None, 2048) 0 Averagea_Pooling[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 2048) 0 flatten[0][0] __________________________________________________________________________________________________ Dense_final (Dense) (None, 6) 12294 dropout[0][0] ================================================================================================== Total params: 19,952,262 Trainable params: 19,909,894 Non-trainable params: 42,368 __________________________________________________________________________________________________ . Compile model . model.compile( optimizer = &quot;adam&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;] ) . Specify callbacks . earlystopping = EarlyStopping( monitor = &#39;loss&#39;, mode = &#39;min&#39;, verbose = 1, patience = 15 ) # Save the best model with lower validation loss checkpointer = ModelCheckpoint( filepath = &quot;weights.hdf5&quot;, verbose = 1, save_best_only = True ) . Model training . # Here we use 1 epoch for demonstration. history = model.fit_generator( train_generator, steps_per_epoch = train_generator.n // 32, epochs = 5, callbacks = [ checkpointer, earlystopping ] ) . /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators. warnings.warn(&#39;`Model.fit_generator` is deprecated and &#39; . Epoch 1/5 438/438 [==============================] - 180s 412ms/step - loss: 0.8367 - accuracy: 0.6858 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 2/5 438/438 [==============================] - 180s 411ms/step - loss: 0.7321 - accuracy: 0.7325 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 3/5 438/438 [==============================] - 180s 411ms/step - loss: 0.6380 - accuracy: 0.7719 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 4/5 438/438 [==============================] - 181s 412ms/step - loss: 0.6049 - accuracy: 0.7787 WARNING:tensorflow:Can save best model only with val_loss available, skipping. Epoch 5/5 438/438 [==============================] - 180s 411ms/step - loss: 0.5641 - accuracy: 0.7992 WARNING:tensorflow:Can save best model only with val_loss available, skipping. . Model evaluation . evaluate = model.evaluate_generator( test_generator, steps = test_generator.n // 32, verbose = 1 ) . /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1973: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators. warnings.warn(&#39;`Model.evaluate_generator` is deprecated and &#39; . 93/93 [==============================] - 7s 66ms/step - loss: 1.0486 - accuracy: 0.6116 . labels = { 0: &#39;buildings&#39;, 1: &#39;forest&#39;, 2: &#39;glacier&#39;, 3: &#39;mountain&#39;, 4: &#39;sea&#39;, 5: &#39;street&#39; } . prediction = [] original = [] image = [] count = 0 for i in os.listdir(path_test): for item in os.listdir(os.path.join(path_test, i)): # code to open the image img= PIL.Image.open(os.path.join(path_test, i, item)) # resizing the image to (256,256) img = img.resize((256, 256)) # appending image to the image list image.append(img) # converting image to array img = np.asarray(img, dtype = np.float32) # normalizing the image img = img / 255 # reshaping the image into a 4D array img = img.reshape(-1, 256, 256, 3) # making prediction of the model predict = model.predict(img) # getting the index corresponding to the highest value in the prediction predict = np.argmax(predict) # appending the predicted class to the list prediction.append(labels[predict]) # appending original class to the list original.append(i) . score = accuracy_score(original, prediction) print(&quot;Test Accuracy : {}&quot;.format(score)) . Test Accuracy : 0.6333333333333333 . fig = plt.figure(figsize = (100,100)) for i in range(20): j = random.randint(0, len(image)) fig.add_subplot(20, 1, i+1) plt.xlabel(&quot;Prediction: &quot; + prediction[j] +&quot; Original: &quot; + original[j]) plt.imshow(image[j]) fig.tight_layout() plt.show() . print(classification_report(np.asarray(prediction), np.asarray(original))) # Based on these values, you can try t improve your model. # For the sake of simplicity, hyperparameter tuning and model improvement was not done. . precision recall f1-score support buildings 0.79 0.68 0.73 510 forest 0.85 0.93 0.89 434 glacier 0.22 0.88 0.35 137 mountain 0.25 0.73 0.38 183 sea 0.94 0.41 0.57 1152 street 0.84 0.72 0.77 584 accuracy 0.63 3000 macro avg 0.65 0.72 0.62 3000 weighted avg 0.81 0.63 0.66 3000 . plt.figure(figsize = (7, 5)) cm = confusion_matrix(np.asarray(prediction), np.asarray(original)) sns.heatmap( cm, annot = True, fmt = &quot;d&quot; ) plt.show() . Grad Cam Visualization . def grad_cam(img): # Convert the image to array of type float32 img = np.asarray(img, dtype = np.float32) # Reshape the image from (256,256,3) to (1,256,256,3) img = img.reshape(-1, 256, 256, 3) img_scaled = img / 255 # Name of the average pooling layer and dense final (you can see these names in the model summary) classification_layers = [&quot;Averagea_Pooling&quot;, &quot;Dense_final&quot;] # Last convolutional layer in the model final_conv = model.get_layer(&quot;res_5_identity_2_c&quot;) # Create a model with original model inputs and the last conv_layer as the output final_conv_model = keras.Model(model.inputs, final_conv.output) # Then we create the input for classification layer, which is the output of last conv layer # In our case, output produced by the conv layer is of the shape (1,3,3,2048) # Since the classification input needs the features as input, we ignore the batch dimension classification_input = keras.Input(shape = final_conv.output.shape[1:]) # We iterate through the classification layers, to get the final layer and then append # the layer as the output layer to the classification model. temp = classification_input for layer in classification_layers: temp = model.get_layer(layer)(temp) classification_model = keras.Model(classification_input, temp) # We use gradient tape to monitor the &#39;final_conv_output&#39; to retrive the gradients # corresponding to the predicted class with tf.GradientTape() as tape: # Pass the image through the base model and get the feature map final_conv_output = final_conv_model(img_scaled) # Assign gradient tape to monitor the conv_output tape.watch(final_conv_output) # Pass the feature map through the classification model and use argmax to get the # index of the predicted class and then use the index to get the value produced by final # layer for that class prediction = classification_model(final_conv_output) predicted_class = tf.argmax(prediction[0][0][0]) predicted_class_value = prediction[:,:,:,predicted_class] # Get the gradient corresponding to the predicted class based on feature map. # which is of shape (1,3,3,2048) gradient = tape.gradient(predicted_class_value, final_conv_output) # Since we need the filter values (2048), we reduce the other dimensions, # which would result in a shape of (2048,) gradient_channels = tf.reduce_mean(gradient, axis=(0, 1, 2)) # We then convert the feature map produced by last conv layer(1,6,6,1536) to (6,6,1536) final_conv_output = final_conv_output.numpy()[0] gradient_channels = gradient_channels.numpy() # We multiply the filters in the feature map produced by final conv layer by the # filter values that are used to get the predicted class. By doing this we inrease the # value of areas that helped in making the prediction and lower the vlaue of areas, that # did not contribute towards the final prediction for i in range(gradient_channels.shape[-1]): final_conv_output[:, :, i] *= gradient_channels[i] # We take the mean accross the channels to get the feature map heatmap = np.mean(final_conv_output, axis=-1) # Normalizing the heat map between 0 and 1, to visualize it heatmap_normalized = np.maximum(heatmap, 0) / np.max(heatmap) # Rescaling and converting the type to int heatmap = np.uint8(255 * heatmap_normalized ) # Create the colormap color_map = plt.cm.get_cmap(&#39;jet&#39;) # get only the rb features from the heatmap color_map = color_map(np.arange(256))[:, :3] heatmap = color_map[heatmap] # convert the array to image, resize the image and then convert to array heatmap = keras.preprocessing.image.array_to_img(heatmap) heatmap = heatmap.resize((256, 256)) heatmap = np.asarray(heatmap, dtype = np.float32) # Add the heatmap on top of the original image final_img = heatmap * 0.4 + img[0] final_img = keras.preprocessing.image.array_to_img(final_img) return final_img, heatmap_normalized . fig, axs = plt.subplots(6,3, figsize = (16,32)) count = 0 for _ in range(6): i = random.randint(0, len(image)) gradcam, heatmap = grad_cam(image[i]) axs[count][0].title.set_text(&quot;Original -&quot; + original[i]) axs[count][0].imshow(image[i]) axs[count][1].title.set_text(&quot;Heatmap&quot;) axs[count][1].imshow(heatmap) axs[count][2].title.set_text(&quot;Prediction -&quot; + prediction[i]) axs[count][2].imshow(gradcam) count += 1 fig.tight_layout() . Future Work . To implement and visualize class activation map with Grad-CAM ++ and Score-CAM . - Grad-CAM++ https://arxiv.org/abs/1710.11063 - Score-CAM https://arxiv.org/abs/1910.01279 .",
            "url": "https://janmejaybhoi.github.io/w/computer%20vision/image%20processing/deep%20learning/multi-class%20classification/2021/05/10/GradCam-Resnet.html",
            "relUrl": "/computer%20vision/image%20processing/deep%20learning/multi-class%20classification/2021/05/10/GradCam-Resnet.html",
            "date": " • May 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Sequential Sentence Classification in Medical Abstracts",
            "content": "Overview: . Classify a Randomized clinical trials (RCTs) abstarct to subclasses for easier to read and understand. | Basically convert a medical abstarct to chunks of sentences of particaular classes like &quot;Background&quot;, &quot;Methods&quot;, &quot;Results&quot; and &quot;Conclusion&quot;. | Its a Many to One Text Classification problem. Where we categorize a sequence to a prticular class. | . Dataset Used: . PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts . PubMed 20k is a subset of PubMed 200k. I.e., any abstract present in PubMed 20k is also present in PubMed 200k. . | PubMed_200k_RCT is the same as PubMed_200k_RCT_numbers_replaced_with_at_sign, except that in the latter all numbers had been replaced by @. (same for PubMed_20k_RCT vs. PubMed_20k_RCT_numbers_replaced_with_at_sign) . | . !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git . Cloning into &#39;pubmed-rct&#39;... remote: Enumerating objects: 33, done. remote: Counting objects: 100% (3/3), done. remote: Compressing objects: 100% (3/3), done. remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 30 Unpacking objects: 100% (33/33), done. . !ls pubmed-rct . PubMed_200k_RCT PubMed_200k_RCT_numbers_replaced_with_at_sign PubMed_20k_RCT PubMed_20k_RCT_numbers_replaced_with_at_sign README.md . !ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign . dev.txt test.txt train.txt . train.txt - training samples. | dev.txt - dev is short for development set, which is another name for validation set (in our case, we&#39;ll be using and referring to this file as our validation set). | test.txt - test samples. | . data_dir = &quot;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/&quot; . import os filenames = [data_dir +filename for filename in os.listdir(data_dir)] . filenames . [&#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt&#39;, &#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt&#39;, &#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt&#39;] . Preprocessing . def get_lines(filename): &quot;&quot;&quot; &quot;&quot;&quot; with open(filename, &quot;r&quot;) as f: return f.readlines() . train_lines = get_lines(&#39;/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt&#39;) train_lines[:10] . [&#39;###24293578 n&#39;, &#39;OBJECTIVE tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) . n&#39;, &#39;METHODS tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks . n&#39;, &#39;METHODS tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers . n&#39;, &#39;METHODS tPain was assessed using the visual analog pain scale ( @-@ mm ) . n&#39;, &#39;METHODS tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) . n&#39;, &#39;METHODS tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured . n&#39;, &#39;RESULTS tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks . n&#39;, &#39;RESULTS tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively . n&#39;, &#39;RESULTS tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group . n&#39;] . Example returned preprocessed sample (a single line from an abstract): . Return all of the lines in the target text file as a list of dictionaries containing the key/value pairs: &quot;line_number&quot; - the position of the line in the abstract (e.g. 3). &quot;target&quot; - the role of the line in the abstract (e.g. OBJECTIVE). . &quot;text&quot; - the text of the line in the abstract. | &quot;total_lines&quot; - the total lines in an abstract sample (e.g. 14) | . [{&#39;line_number&#39;: 0, &#39;target&#39;: &#39;OBJECTIVE&#39;, &#39;text&#39;: &#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;total_lines&#39;: 11}, ...] . def preprocess_text_with_line_numbers(filename): input_lines = get_lines(filename) # get all lines from filename abstract_lines = &quot;&quot; # create an empty abstract abstract_samples = [] # create an empty list of abstracts for line in input_lines: if line.startswith(&quot;###&quot;): # check to see if line is an ID line abstract_id = line abstract_lines = &quot;&quot; # reset the abstract string elif line.isspace(): abstract_line_split = abstract_lines.splitlines() # split the abstract into separate lines for abstract_line_number, abstract_line in enumerate(abstract_line_split): line_data = {} target_text_split = abstract_line.split(&quot; t&quot;) line_data[&quot;target&quot;] = target_text_split[0] line_data[&quot;text&quot;] = target_text_split[1].lower() line_data[&quot;line_number&quot;] = abstract_line_number line_data[&quot;total_lines&quot;] = len(abstract_line_split) - 1 abstract_samples.append(line_data) else: abstract_lines += line return abstract_samples . %%time train_samples = preprocess_text_with_line_numbers(data_dir + &quot;train.txt&quot;) val_samples = preprocess_text_with_line_numbers(data_dir + &quot;dev.txt&quot;) # dev is another name for validation set test_samples = preprocess_text_with_line_numbers(data_dir + &quot;test.txt&quot;) len(train_samples), len(val_samples), len(test_samples) . CPU times: user 520 ms, sys: 123 ms, total: 643 ms Wall time: 648 ms . As we are experimenting Some Text Preprocessing are left (like url and special char removal) , we&#39;ll do it future and see acuuracy deference. . train_samples[:10] . [{&#39;line_number&#39;: 0, &#39;target&#39;: &#39;OBJECTIVE&#39;, &#39;text&#39;: &#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 1, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 2, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 3, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;pain was assessed using the visual analog pain scale ( @-@ mm ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 4, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 5, &#39;target&#39;: &#39;METHODS&#39;, &#39;text&#39;: &#39;serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 6, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 7, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 8, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .&#39;, &#39;total_lines&#39;: 11}, {&#39;line_number&#39;: 9, &#39;target&#39;: &#39;RESULTS&#39;, &#39;text&#39;: &#39;these differences remained significant at @ weeks .&#39;, &#39;total_lines&#39;: 11}] . import pandas as pd train_df = pd.DataFrame(train_samples) val_df = pd.DataFrame(val_samples) test_df = pd.DataFrame(test_samples) train_df.head(14) . target text line_number total_lines . 0 OBJECTIVE | to investigate the efficacy of @ weeks of dail... | 0 | 11 | . 1 METHODS | a total of @ patients with primary knee oa wer... | 1 | 11 | . 2 METHODS | outcome measures included pain reduction and i... | 2 | 11 | . 3 METHODS | pain was assessed using the visual analog pain... | 3 | 11 | . 4 METHODS | secondary outcome measures included the wester... | 4 | 11 | . 5 METHODS | serum levels of interleukin @ ( il-@ ) , il-@ ... | 5 | 11 | . 6 RESULTS | there was a clinically relevant reduction in t... | 6 | 11 | . 7 RESULTS | the mean difference between treatment arms ( @... | 7 | 11 | . 8 RESULTS | further , there was a clinically relevant redu... | 8 | 11 | . 9 RESULTS | these differences remained significant at @ we... | 9 | 11 | . 10 RESULTS | the outcome measures in rheumatology clinical ... | 10 | 11 | . 11 CONCLUSIONS | low-dose oral prednisolone had both a short-te... | 11 | 11 | . 12 BACKGROUND | emotional eating is associated with overeating... | 0 | 10 | . 13 BACKGROUND | yet , empirical evidence for individual ( trai... | 1 | 10 | . train_df[&quot;target&quot;].value_counts() . METHODS 59353 RESULTS 57953 CONCLUSIONS 27168 BACKGROUND 21727 OBJECTIVE 13839 Name: target, dtype: int64 . train_df[&quot;target&quot;].value_counts().plot(kind = &#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefbe5c6d10&gt; . train_df.total_lines.plot(kind= &quot;hist&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefb31f38d0&gt; . train_sentences = train_df[&quot;text&quot;].tolist() val_sentences = val_df[&quot;text&quot;].tolist() test_sentences = test_df[&quot;text&quot;].tolist() len(train_sentences), len(val_sentences), len(test_sentences) . (180040, 30212, 30135) . train_sentences[:10] . [&#39;to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .&#39;, &#39;a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .&#39;, &#39;outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .&#39;, &#39;pain was assessed using the visual analog pain scale ( @-@ mm ) .&#39;, &#39;secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .&#39;, &#39;serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .&#39;, &#39;there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .&#39;, &#39;the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .&#39;, &#39;further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .&#39;, &#39;these differences remained significant at @ weeks .&#39;] . One Hot Encoder . from sklearn.preprocessing import OneHotEncoder one_hot_encoder = OneHotEncoder(sparse=False) train_labels_one_hot = one_hot_encoder.fit_transform(train_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) val_labels_one_hot = one_hot_encoder.transform(val_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) test_labels_one_hot = one_hot_encoder.transform(test_df[&quot;target&quot;].to_numpy().reshape(-1, 1)) # Check what training labels look like train_labels_one_hot . array([[0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], ..., [0., 0., 0., 0., 1.], [0., 1., 0., 0., 0.], [0., 1., 0., 0., 0.]]) . Label encode labels . from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() train_labels_encoded = label_encoder.fit_transform(train_df[&quot;target&quot;].to_numpy()) val_labels_encoded = label_encoder.transform(val_df[&quot;target&quot;].to_numpy()) test_labels_encoded = label_encoder.transform(test_df[&quot;target&quot;].to_numpy()) # Check what training labels look like train_labels_encoded . array([3, 2, 2, ..., 4, 1, 1]) . num_classes = len(label_encoder.classes_) class_names = label_encoder.classes_ num_classes, class_names . (5, array([&#39;BACKGROUND&#39;, &#39;CONCLUSIONS&#39;, &#39;METHODS&#39;, &#39;OBJECTIVE&#39;, &#39;RESULTS&#39;], dtype=object)) . Model 0: Getting a baseline . Our first model we&#39;ll be a TF-IDF Multinomial Naive Bayes as recommended by Scikit-Learn&#39;s machine learning map. . we&#39;ll create a Scikit-Learn Pipeline which uses the TfidfVectorizer class to convert our abstract sentences to numbers using the TF-IDF (term frequency-inverse document frequecy) algorithm and then learns to classify our sentences using the MultinomialNB aglorithm. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Create a pipeline model_0 = Pipeline([ (&quot;tf-idf&quot;, TfidfVectorizer()), (&quot;clf&quot;, MultinomialNB()) ]) # Fit the pipeline to the training data model_0.fit(X=train_sentences, y=train_labels_encoded); . # Evaluate baseline on validation dataset model_0.score(X=val_sentences, y=val_labels_encoded) . 0.7218323844829869 . baseline_preds = model_0.predict(val_sentences) baseline_preds . array([4, 1, 3, ..., 4, 4, 1]) . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2021-06-16 16:53:10-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2021-06-16 16:53:10 (79.1 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import calculate_results . Model 0 Results . # Calculate baseline results baseline_results = calculate_results(y_true=val_labels_encoded, y_pred=baseline_preds) baseline_results . {&#39;accuracy&#39;: 72.1832384482987, &#39;f1&#39;: 0.6989250353450294, &#39;precision&#39;: 0.7186466952323352, &#39;recall&#39;: 0.7218323844829869} . Prepare Data For Deep Neural Network Models . When our model goes through our sentences, it works best when they&#39;re all the same length (this is important for creating batches of the same size tensors) . Finding the average sentence length in the Dataset. | . import numpy as np import tensorflow as tf from tensorflow.keras import layers . sen_len = [len(sentences.split()) for sentences in train_sentences] avg_sen_len = np.mean(sen_len) avg_sen_len . 26.338269273494777 . import matplotlib.pyplot as plt plt.hist(sen_len,bins=21) # Checking the Sequence Length Distribution and getting most occurance sequence length . (array([4.2075e+04, 7.9624e+04, 3.8291e+04, 1.2725e+04, 4.3900e+03, 1.6450e+03, 7.2600e+02, 2.8900e+02, 1.3600e+02, 5.5000e+01, 2.9000e+01, 1.5000e+01, 1.1000e+01, 9.0000e+00, 8.0000e+00, 5.0000e+00, 2.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00]), array([ 1. , 15.04761905, 29.0952381 , 43.14285714, 57.19047619, 71.23809524, 85.28571429, 99.33333333, 113.38095238, 127.42857143, 141.47619048, 155.52380952, 169.57142857, 183.61904762, 197.66666667, 211.71428571, 225.76190476, 239.80952381, 253.85714286, 267.9047619 , 281.95238095, 296. ]), &lt;a list of 21 Patch objects&gt;) . Looks like the vast majority of sentences are between 0 and 50 tokens in length. . We can use NumPy&#39;s percentile to find the value which covers 95% of the sentence lengthsHow long of a sentesnces cover majority of the data ? (95%) . np.percentile(sen_len,95) . 55.0 . max(sen_len) # max length sentence in training set . 296 . Creaating a text vectorizer layer . Create text vectorize . Section 3.2 of the PubMed 200k RCT paper states the vocabulary size of the PubMed 20k dataset as 68,000. So we&#39;ll use that as our max_tokens parameter. . max_tokens = 68000 . from tensorflow.keras.layers.experimental.preprocessing import TextVectorization text_vectorizer = TextVectorization(max_tokens=max_tokens,standardize=&#39;lower_and_strip_punctuation&#39;, output_sequence_length=55) . text_vectorizer.adapt(train_sentences) . import random target_sentence = random.choice(train_sentences) print(f&quot;Text: n{target_sentence}&quot;) print(f&quot; nLength of text: {len(target_sentence.split())}&quot;) print(f&quot; nVectorized text: n{text_vectorizer([target_sentence])}&quot;) . Text: after @ months of stable-dose treatment , patients receiving milnacipran @ and/or @ mg/d had significant improvement in mfi total and subscale scores ( p &lt; @ vs placebo ) . Length of text: 31 Vectorized text: [[ 21 41 4 34172 19 12 245 12540 727 1306 55 37 194 5 18393 76 3 2072 119 14 44 48 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] . rct_20k_text_vocab = text_vectorizer.get_vocabulary() most_common = rct_20k_text_vocab[:5] least_common = rct_20k_text_vocab[-5:] print(f&quot;Number of words in vocabulary: {len(rct_20k_text_vocab)}&quot;), print(f&quot;Most common words in the vocabulary: {most_common}&quot;) print(f&quot;Least common words in the vocabulary: {least_common}&quot;) . Number of words in vocabulary: 64841 Most common words in the vocabulary: [&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;and&#39;, &#39;of&#39;] Least common words in the vocabulary: [&#39;aainduced&#39;, &#39;aaigroup&#39;, &#39;aachener&#39;, &#39;aachen&#39;, &#39;aaacp&#39;] . text_vectorizer.get_config() . {&#39;dtype&#39;: &#39;string&#39;, &#39;max_tokens&#39;: 68000, &#39;name&#39;: &#39;text_vectorization&#39;, &#39;ngrams&#39;: None, &#39;output_mode&#39;: &#39;int&#39;, &#39;output_sequence_length&#39;: 55, &#39;pad_to_max_tokens&#39;: False, &#39;split&#39;: &#39;whitespace&#39;, &#39;standardize&#39;: &#39;lower_and_strip_punctuation&#39;, &#39;trainable&#39;: True, &#39;vocabulary_size&#39;: 64841} . Create custom text embedding . To create a richer numerical representation of our text, we can use an embedding. . The input_dim parameter defines the size of our vocabulary. And the output_dim parameter defines the dimension of the embedding output. . Once created, our embedding layer will take the integer outputs of our text_vectorization layer as inputs and convert them to feature vectors of size output_dim. . token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), output_dim= 128, mask_zero=True, input_length=55) print(f&quot;Sentence before Vectorization : n{target_sentence} n&quot;) vec_sentence = text_vectorizer([target_sentence]) print(f&quot;Sentence After vectorization : n {vec_sentence} n&quot;) embed_sentence = token_embed(vec_sentence) print(f&quot;Embedding Sentence : n{embed_sentence} n&quot;) . Sentence before Vectorization : after @ months of stable-dose treatment , patients receiving milnacipran @ and/or @ mg/d had significant improvement in mfi total and subscale scores ( p &lt; @ vs placebo ) . Sentence After vectorization : [[ 21 41 4 34172 19 12 245 12540 727 1306 55 37 194 5 18393 76 3 2072 119 14 44 48 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] Embedding Sentence : [[[ 0.01845442 0.00490598 0.02234909 ... -0.0251475 0.02369246 -0.03116907] [ 0.03540225 -0.01559786 -0.00104294 ... -0.0325914 -0.01999564 -0.03067011] [ 0.01375339 -0.0441448 -0.03379129 ... -0.04563415 -0.04586704 0.03887606] ... [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449] [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449] [-0.02985824 -0.01073467 -0.04434704 ... 0.0063103 -0.04246072 -0.02347449]]] . train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)) valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)) test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot)) . len(train_dataset) , train_dataset . (180040, &lt;TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)&gt;) . train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE) valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE) test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE) train_dataset . &lt;PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)&gt; . Model 1: Conv1D with token embedding . All of our deep models will follow a similar structure: . Input (text) -&gt; Tokenize -&gt; Embedding -&gt; Layers -&gt; Output (label probability) . inputs = layers.Input(shape = (1,),dtype = tf.string) text_vector = text_vectorizer(inputs) embed = token_embed(text_vector) x = layers.Conv1D(filters = 64, kernel_size= 5, padding=&quot;same&quot;,activation=&quot;relu&quot;,kernel_regularizer=tf.keras.regularizers.L2(0.01))(embed) x = layers.GlobalMaxPool1D()(x) x = layers.Dropout(0.2)(x) outputs = layers.Dense(num_classes,activation=&quot;softmax&quot;)(x) model = tf.keras.Model(inputs,outputs) model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 1)] 0 _________________________________________________________________ text_vectorization (TextVect (None, 55) 0 _________________________________________________________________ embedding (Embedding) (None, 55, 128) 8299648 _________________________________________________________________ conv1d (Conv1D) (None, 55, 64) 41024 _________________________________________________________________ global_max_pooling1d (Global (None, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense (Dense) (None, 5) 325 ================================================================= Total params: 8,340,997 Trainable params: 8,340,997 Non-trainable params: 0 _________________________________________________________________ . len(train_dataset) . 5627 . model.compile(optimizer=&#39;Adam&#39;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . model_1_history = model.fit(train_dataset, steps_per_epoch=int(0.1 * len(train_dataset)), epochs = 10, validation_data = valid_dataset, validation_steps=int(0.1 * len(valid_dataset)),) . Epoch 1/10 562/562 [==============================] - 58s 101ms/step - loss: 1.1068 - accuracy: 0.6201 - val_loss: 0.7933 - val_accuracy: 0.7487 Epoch 2/10 562/562 [==============================] - 56s 100ms/step - loss: 0.7738 - accuracy: 0.7517 - val_loss: 0.7109 - val_accuracy: 0.7736 Epoch 3/10 562/562 [==============================] - 56s 100ms/step - loss: 0.7241 - accuracy: 0.7625 - val_loss: 0.6671 - val_accuracy: 0.7872 Epoch 4/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6906 - accuracy: 0.7751 - val_loss: 0.6546 - val_accuracy: 0.7846 Epoch 5/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6888 - accuracy: 0.7782 - val_loss: 0.6390 - val_accuracy: 0.7902 Epoch 6/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6796 - accuracy: 0.7763 - val_loss: 0.6246 - val_accuracy: 0.7949 Epoch 7/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6509 - accuracy: 0.7854 - val_loss: 0.6183 - val_accuracy: 0.7999 Epoch 8/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6389 - accuracy: 0.7971 - val_loss: 0.6009 - val_accuracy: 0.8122 Epoch 9/10 562/562 [==============================] - 56s 100ms/step - loss: 0.6379 - accuracy: 0.7930 - val_loss: 0.6430 - val_accuracy: 0.7902 Epoch 10/10 562/562 [==============================] - 56s 99ms/step - loss: 0.6402 - accuracy: 0.7942 - val_loss: 0.6062 - val_accuracy: 0.8039 . model.evaluate(valid_dataset) . 945/945 [==============================] - 5s 5ms/step - loss: 0.5962 - accuracy: 0.8083 . [0.5962496995925903, 0.8082880973815918] . model_1_pred_probs = model.predict(valid_dataset) model_1_pred_probs . array([[6.4009082e-01, 1.5787475e-01, 1.1690257e-02, 1.7012680e-01, 2.0217434e-02], [2.1156156e-01, 7.0904827e-01, 1.1327777e-03, 7.0969328e-02, 7.2881221e-03], [1.7376225e-01, 8.4552346e-03, 1.4700086e-03, 8.1595337e-01, 3.5914412e-04], ..., [2.6505839e-04, 9.3591970e-04, 1.3166884e-02, 1.3237984e-04, 9.8549968e-01], [2.4481107e-02, 5.5618656e-01, 4.2747390e-02, 1.6767828e-02, 3.5981715e-01], [9.1729127e-02, 8.6072636e-01, 1.3366650e-02, 1.7221529e-02, 1.6956387e-02]], dtype=float32) . model_1_preds = tf.argmax(model_1_pred_probs, axis=1) model_1_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 1, 1])&gt; . Model 1 Results . model_1_results = calculate_results(y_true=val_labels_encoded, y_pred=model_1_preds) model_1_results . {&#39;accuracy&#39;: 80.8288097444724, &#39;f1&#39;: 0.8054731032957875, &#39;precision&#39;: 0.8047795569207609, &#39;recall&#39;: 0.808288097444724} . Model 2: Feature extraction with pretrained token embedding (USE) . Here We use Universal Sentence Encoder here from TF-HUB. . Since we&#39;re moving towards replicating the model architecture in Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it mentions they used a pretrained GloVe embedding as a way to initialise their token embeddings. . The model structure will look like: . Inputs (string) -&gt; Pretrained embeddings from TensorFlow Hub (Universal Sentence Encoder) -&gt; Layers -&gt; Output (prediction probabilities) . import tensorflow_hub as hub tf_hub_embedding_layer = hub.KerasLayer(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;, trainable=False, name=&quot;universal_sentence_encoder&quot;) . Beautiful, now our pretrained USE is downloaded and instantiated as a hub.KerasLayer instance, let&#39;s test it out on a random sentence . random_training_sentence = random.choice(train_sentences) print(f&quot;Random training sentence: n{random_training_sentence} n&quot;) use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence]) print(f&quot;Sentence after embedding: n{use_embedded_sentence[0][:30]} (truncated output)... n&quot;) print(f&quot;Length of sentence embedding: n{len(use_embedded_sentence[0])}&quot;) . Random training sentence: the subjects in the test group practiced cthe , while those in the control group did `` the @th radio calisthenics &#39;&#39; , an official recommended calisthenics for promoting healthcare in china , @ times a week , and @ weeks practicing overall . Sentence after embedding: [-0.04633829 -0.03948136 0.02896223 -0.07942989 -0.01642509 0.03321299 0.04372008 -0.01696645 -0.03029537 0.0055867 0.0817182 0.0511634 0.02764668 -0.04249508 0.02601314 0.02671395 -0.08149037 0.02041833 -0.05118315 -0.02549118 -0.02749129 -0.02932397 -0.07956979 -0.02773482 -0.03448042 0.03734251 -0.066794 0.01424676 -0.06487625 -0.05911339] (truncated output)... Length of sentence embedding: 512 . inputs = layers.Input(shape=[], dtype=tf.string) pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding x = layers.Dense(128, activation=&quot;relu&quot;)(pretrained_embedding) # add a fully connected layer on top of the embedding # x = layers.Dropout(0.2)(x) outputs = layers.Dense(5, activation=&quot;softmax&quot;,kernel_regularizer=None)(x) # create the output layer model_2 = tf.keras.Model(inputs=inputs, outputs=outputs) # Compile the model model_2.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) . model_2.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None,)] 0 _________________________________________________________________ universal_sentence_encoder ( (None, 512) 256797824 _________________________________________________________________ dense_1 (Dense) (None, 128) 65664 _________________________________________________________________ dense_2 (Dense) (None, 5) 645 ================================================================= Total params: 256,864,133 Trainable params: 66,309 Non-trainable params: 256,797,824 _________________________________________________________________ . model_2_history = model_2.fit(train_dataset, steps_per_epoch=int(0.1 * len(train_dataset)), epochs = 10, validation_data = valid_dataset, validation_steps=int(0.1 * len(valid_dataset))) . Epoch 1/10 562/562 [==============================] - 9s 12ms/step - loss: 0.9225 - accuracy: 0.6502 - val_loss: 0.7968 - val_accuracy: 0.6908 Epoch 2/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7692 - accuracy: 0.7017 - val_loss: 0.7549 - val_accuracy: 0.7071 Epoch 3/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7528 - accuracy: 0.7127 - val_loss: 0.7393 - val_accuracy: 0.7128 Epoch 4/10 562/562 [==============================] - 7s 12ms/step - loss: 0.7198 - accuracy: 0.7244 - val_loss: 0.7123 - val_accuracy: 0.7287 Epoch 5/10 562/562 [==============================] - 6s 11ms/step - loss: 0.7267 - accuracy: 0.7219 - val_loss: 0.6904 - val_accuracy: 0.7317 Epoch 6/10 562/562 [==============================] - 6s 12ms/step - loss: 0.7185 - accuracy: 0.7254 - val_loss: 0.6832 - val_accuracy: 0.7350 Epoch 7/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6862 - accuracy: 0.7390 - val_loss: 0.6673 - val_accuracy: 0.7463 Epoch 8/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6764 - accuracy: 0.7435 - val_loss: 0.6558 - val_accuracy: 0.7483 Epoch 9/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6742 - accuracy: 0.7430 - val_loss: 0.6571 - val_accuracy: 0.7480 Epoch 10/10 562/562 [==============================] - 6s 11ms/step - loss: 0.6690 - accuracy: 0.7479 - val_loss: 0.6518 - val_accuracy: 0.7563 . model_2.evaluate(valid_dataset) . 945/945 [==============================] - 9s 9ms/step - loss: 0.6559 - accuracy: 0.7511 . [0.6559162139892578, 0.7510591745376587] . model_2_pred_probs = model_2.predict(valid_dataset) model_2_pred_probs . array([[4.20623630e-01, 4.38490957e-01, 6.25136832e-04, 1.32154837e-01, 8.10540374e-03], [3.23061615e-01, 5.82224965e-01, 1.88950659e-03, 9.16711688e-02, 1.15277513e-03], [4.91145968e-01, 4.19516787e-02, 1.88930240e-02, 4.06143695e-01, 4.18655388e-02], ..., [7.41388998e-04, 7.53519998e-04, 2.20944248e-02, 2.34126986e-04, 9.76176500e-01], [3.42314062e-03, 6.01526648e-02, 1.98868886e-01, 8.81478831e-04, 7.36673832e-01], [4.39007245e-02, 8.41824710e-01, 1.04148194e-01, 9.37066041e-04, 9.18926578e-03]], dtype=float32) . model_2_preds = tf.argmax(model_2_pred_probs, axis=1) model_2_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 0, ..., 4, 4, 1])&gt; . Model 2 Results . model_2_results = calculate_results(y_true=val_labels_encoded, y_pred=model_2_preds) model_2_results . {&#39;accuracy&#39;: 75.10591817820733, &#39;f1&#39;: 0.746383502412015, &#39;precision&#39;: 0.7457372543188239, &#39;recall&#39;: 0.7510591817820733} . Model 3: Conv1D with character embedding . Creating a character-level tokenizer . The Neural Networks for Joint Sentence Classification in Medical Paper Abstracts paper mentions their model uses a hybrid of token and character embeddings. . The difference between a character and token embedding is that the character embedding is created using sequences split into characters (e.g. hello -&gt; [h, e, l, l, o]) where as a token embedding is created on sequences split into tokens. . Token level embeddings split sequences into tokens (words) and embeddings each of them, character embeddings split sequences into characters and creates a feature vector for each. . Before we can vectorize our sequences on a character-level we&#39;ll need to split them into characters. Let&#39;s write a function to do so . &quot; &quot;.join(list(train_sentences[0])) . &#39;t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) .&#39; . def split_chars(text): return &quot; &quot;.join(list(text)) split_chars(random_training_sentence) . &#34;t h e s u b j e c t s i n t h e t e s t g r o u p p r a c t i c e d c t h e , w h i l e t h o s e i n t h e c o n t r o l g r o u p d i d ` ` t h e @ t h r a d i o c a l i s t h e n i c s &#39; &#39; , a n o f f i c i a l r e c o m m e n d e d c a l i s t h e n i c s f o r p r o m o t i n g h e a l t h c a r e i n c h i n a , @ t i m e s a w e e k , a n d @ w e e k s p r a c t i c i n g o v e r a l l .&#34; . train_chars = [split_chars(sentence) for sentence in train_sentences] val_chars = [split_chars(sentence) for sentence in val_sentences] test_chars = [split_chars(sentence) for sentence in test_sentences] print(train_chars[0]) . t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) . . train_chars[:5] . [&#39;t o i n v e s t i g a t e t h e e f f i c a c y o f @ w e e k s o f d a i l y l o w - d o s e o r a l p r e d n i s o l o n e i n i m p r o v i n g p a i n , m o b i l i t y , a n d s y s t e m i c l o w - g r a d e i n f l a m m a t i o n i n t h e s h o r t t e r m a n d w h e t h e r t h e e f f e c t w o u l d b e s u s t a i n e d a t @ w e e k s i n o l d e r a d u l t s w i t h m o d e r a t e t o s e v e r e k n e e o s t e o a r t h r i t i s ( o a ) .&#39;, &#39;a t o t a l o f @ p a t i e n t s w i t h p r i m a r y k n e e o a w e r e r a n d o m i z e d @ : @ ; @ r e c e i v e d @ m g / d a y o f p r e d n i s o l o n e a n d @ r e c e i v e d p l a c e b o f o r @ w e e k s .&#39;, &#39;o u t c o m e m e a s u r e s i n c l u d e d p a i n r e d u c t i o n a n d i m p r o v e m e n t i n f u n c t i o n s c o r e s a n d s y s t e m i c i n f l a m m a t i o n m a r k e r s .&#39;, &#39;p a i n w a s a s s e s s e d u s i n g t h e v i s u a l a n a l o g p a i n s c a l e ( @ - @ m m ) .&#39;, &#39;s e c o n d a r y o u t c o m e m e a s u r e s i n c l u d e d t h e w e s t e r n o n t a r i o a n d m c m a s t e r u n i v e r s i t i e s o s t e o a r t h r i t i s i n d e x s c o r e s , p a t i e n t g l o b a l a s s e s s m e n t ( p g a ) o f t h e s e v e r i t y o f k n e e o a , a n d @ - m i n w a l k d i s t a n c e ( @ m w d ) .&#39;] . char_lens = [len(sentence) for sentence in train_sentences] avg_char_lens = sum(char_lens)/len(char_lens) avg_char_lens . 149.3662574983337 . import matplotlib.pyplot as plt plt.hist(char_lens,bins =25) . (array([1.2341e+04, 5.0061e+04, 5.6508e+04, 3.4345e+04, 1.5779e+04, 6.5770e+03, 2.4690e+03, 1.0890e+03, 4.4400e+02, 1.9900e+02, 1.0500e+02, 5.0000e+01, 2.4000e+01, 1.9000e+01, 1.1000e+01, 7.0000e+00, 4.0000e+00, 3.0000e+00, 0.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]), array([1.0000e+00, 5.6400e+01, 1.1180e+02, 1.6720e+02, 2.2260e+02, 2.7800e+02, 3.3340e+02, 3.8880e+02, 4.4420e+02, 4.9960e+02, 5.5500e+02, 6.1040e+02, 6.6580e+02, 7.2120e+02, 7.7660e+02, 8.3200e+02, 8.8740e+02, 9.4280e+02, 9.9820e+02, 1.0536e+03, 1.1090e+03, 1.1644e+03, 1.2198e+03, 1.2752e+03, 1.3306e+03, 1.3860e+03]), &lt;a list of 25 Patch objects&gt;) . Okay, looks like most of our sequences are between 0 and 200 characters long. . Let&#39;s use NumPy&#39;s percentile to figure out what length covers 95% of our sequences . output_seq_char_len = int(np.percentile(char_lens, 95)) output_seq_char_len . 290 . random.choice(train_sentences) . &#39;in patients with her@-positive metastatic breast cancer , the addition of pertuzumab to trastuzumab and docetaxel , as compared with the addition of placebo , significantly improved the median overall survival to @ months and extended the results of previous analyses showing the efficacy of this drug combination .&#39; . import string alphabet = string.ascii_lowercase + string.digits + string.punctuation alphabet . &#39;abcdefghijklmnopqrstuvwxyz0123456789!&#34;#$%&amp; &#39;()*+,-./:;&lt;=&gt;?@[ ]^_`{|}~&#39; . NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS, output_sequence_length=output_seq_char_len, standardize=&quot;lower_and_strip_punctuation&quot;, name=&quot;char_vectorizer&quot;) # Adapt character vectorizer to training characters char_vectorizer.adapt(train_chars) . char_vocab = char_vectorizer.get_vocabulary() print(f&quot;Number of different characters in character vocab: {len(char_vocab)}&quot;) print(f&quot;5 most common characters: {char_vocab[:5]}&quot;) print(f&quot;5 least common characters: {char_vocab[-5:]}&quot;) . Number of different characters in character vocab: 28 5 most common characters: [&#39;&#39;, &#39;[UNK]&#39;, &#39;e&#39;, &#39;t&#39;, &#39;i&#39;] 5 least common characters: [&#39;k&#39;, &#39;x&#39;, &#39;z&#39;, &#39;q&#39;, &#39;j&#39;] . random_train_chars = random.choice(train_chars) print(f&quot;Charified text: n{random_train_chars}&quot;) print(f&quot; nLength of chars: {len(random_train_chars.split())}&quot;) vectorized_chars = char_vectorizer([random_train_chars]) print(f&quot; nVectorized chars: n{vectorized_chars}&quot;) print(f&quot; nLength of vectorized chars: {len(vectorized_chars[0])}&quot;) . Charified text: a c h e s t x - r a y w a s t a k e n a t p o s t o p e r a t i v e d a y @ , a n d t h e r e s i d u a l i n t r a a b d o m i n a l g a s v o l u m e w a s m e a s u r e d . Length of chars: 88 Vectorized chars: [[ 5 11 13 2 9 3 24 8 5 19 20 5 9 3 5 23 2 6 5 3 14 7 9 3 7 14 2 8 5 3 4 21 2 10 5 19 5 6 10 3 13 2 8 2 9 4 10 16 5 12 4 6 3 8 5 5 22 10 7 15 4 6 5 12 18 5 9 21 7 12 16 15 2 20 5 9 15 2 5 9 16 8 2 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] Length of vectorized chars: 290 . Creating a character-level embedding . We&#39;ve got a way to vectorize our character-level sequences, now&#39;s time to create a character-level embedding. . The input dimension (input_dim) will be equal to the number of different characters in our char_vocab (28). And since we&#39;re following the structure of the model in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, the output dimension of the character embedding (output_dim) will be 25. . char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, output_dim= 25, mask_zero= True, name= &quot;char_embed&quot;) # Test out character embedding layer print(f&quot;Charified text (before vectorization and embedding): n{random_train_chars} n&quot;) char_embed_example = char_embed(char_vectorizer([random_train_chars])) print(f&quot;Embedded chars (after vectorization and embedding): n{char_embed_example} n&quot;) print(f&quot;Character embedding shape: {char_embed_example.shape}&quot;) . Charified text (before vectorization and embedding): a c h e s t x - r a y w a s t a k e n a t p o s t o p e r a t i v e d a y @ , a n d t h e r e s i d u a l i n t r a a b d o m i n a l g a s v o l u m e w a s m e a s u r e d . Embedded chars (after vectorization and embedding): [[[ 0.01198739 0.01475773 0.00853588 ... 0.04154284 -0.00139939 -0.04978269] [ 0.03823442 0.02977749 -0.03549073 ... -0.03217635 -0.00892054 0.03190038] [ 0.00683529 0.01736682 0.00283471 ... -0.00423067 -0.00172087 0.01615829] ... [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704] [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704] [ 0.00047475 0.01790795 -0.0195184 ... -0.03690598 0.01174641 0.02456704]]] Character embedding shape: (1, 290, 25) . Before fitting our model on the data, we&#39;ll create char-level batched PrefetchedDataset&#39;s. . train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE) val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE) train_char_dataset . &lt;PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)&gt; . Building a Conv1D model to fit on character embeddings . Now we&#39;ve got a way to turn our character-level sequences into numbers (char_vectorizer) as well as numerically represent them as an embedding (char_embed) let&#39;s test how effective they are at encoding the information in our sequences by creating a character-level sequence model. . The model will have the same structure as our custom token embedding model (model_1) except it&#39;ll take character-level sequences as input instead of token-level sequences. . Input (character-level text) -&gt; Tokenize -&gt; Embedding -&gt; Layers (Conv1D, GlobalMaxPool1D) -&gt; Output (label probability) . inputs = layers.Input(shape=(1,), dtype=&quot;string&quot;) char_vectors = char_vectorizer(inputs) char_embeddings = char_embed(char_vectors) x = layers.Conv1D(64, kernel_size=5, padding=&quot;same&quot;, activation=&quot;relu&quot;,kernel_regularizer=tf.keras.regularizers.L2(0.01))(char_embeddings) x = layers.GlobalMaxPool1D()(x) outputs = layers.Dense(num_classes, activation=&quot;softmax&quot;)(x) model_3 = tf.keras.Model(inputs=inputs, outputs=outputs, name=&quot;model_3_conv1D_char_embedding&quot;) # Compile model model_3.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) model_3.summary() . Model: &#34;model_3_conv1D_char_embedding&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 1)] 0 _________________________________________________________________ char_vectorizer (TextVectori (None, 290) 0 _________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 _________________________________________________________________ conv1d_1 (Conv1D) (None, 290, 64) 8064 _________________________________________________________________ global_max_pooling1d_1 (Glob (None, 64) 0 _________________________________________________________________ dense_3 (Dense) (None, 5) 325 ================================================================= Total params: 10,139 Trainable params: 10,139 Non-trainable params: 0 _________________________________________________________________ . model_3_history = model_3.fit(train_char_dataset, steps_per_epoch=int(0.1 * len(train_char_dataset)), epochs=10, validation_data=val_char_dataset, validation_steps=int(0.1 * len(val_char_dataset))) . Epoch 1/10 562/562 [==============================] - 14s 23ms/step - loss: 1.4009 - accuracy: 0.4576 - val_loss: 1.2213 - val_accuracy: 0.5432 Epoch 2/10 562/562 [==============================] - 13s 23ms/step - loss: 1.1807 - accuracy: 0.5530 - val_loss: 1.1203 - val_accuracy: 0.6001 Epoch 3/10 562/562 [==============================] - 13s 23ms/step - loss: 1.1121 - accuracy: 0.5955 - val_loss: 1.0601 - val_accuracy: 0.6303 Epoch 4/10 562/562 [==============================] - 13s 23ms/step - loss: 1.0576 - accuracy: 0.6207 - val_loss: 1.0272 - val_accuracy: 0.6400 Epoch 5/10 562/562 [==============================] - 13s 23ms/step - loss: 1.0327 - accuracy: 0.6365 - val_loss: 1.0117 - val_accuracy: 0.6592 Epoch 6/10 562/562 [==============================] - 13s 24ms/step - loss: 1.0137 - accuracy: 0.6437 - val_loss: 0.9768 - val_accuracy: 0.6576 Epoch 7/10 562/562 [==============================] - 13s 24ms/step - loss: 0.9968 - accuracy: 0.6497 - val_loss: 0.9487 - val_accuracy: 0.6729 Epoch 8/10 562/562 [==============================] - 13s 24ms/step - loss: 0.9578 - accuracy: 0.6701 - val_loss: 0.9473 - val_accuracy: 0.6712 Epoch 9/10 562/562 [==============================] - 13s 23ms/step - loss: 0.9539 - accuracy: 0.6697 - val_loss: 0.9498 - val_accuracy: 0.6659 Epoch 10/10 562/562 [==============================] - 13s 23ms/step - loss: 0.9476 - accuracy: 0.6668 - val_loss: 0.9276 - val_accuracy: 0.6689 . model_3.evaluate(val_char_dataset) . 945/945 [==============================] - 6s 7ms/step - loss: 0.9418 - accuracy: 0.6693 . [0.941790759563446, 0.6693366765975952] . model_3_pred_probs = model_3.predict(val_char_dataset) model_3_pred_probs . array([[0.21880805, 0.47745466, 0.06887063, 0.17308971, 0.06177694], [0.23530294, 0.43719485, 0.00282957, 0.3047214 , 0.01995131], [0.15760659, 0.33806348, 0.04908546, 0.3893838 , 0.06586069], ..., [0.01113645, 0.02624117, 0.0150859 , 0.00669696, 0.9408395 ], [0.01904003, 0.16237463, 0.07217802, 0.01535511, 0.7310522 ], [0.17195038, 0.6881715 , 0.10410943, 0.01864745, 0.01712121]], dtype=float32) . model_3_preds = tf.argmax(model_3_pred_probs, axis=1) model_3_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 3, ..., 4, 4, 1])&gt; . Model 3 Results . model_3_results = calculate_results(y_true=val_labels_encoded, y_pred=model_3_preds) model_3_results . {&#39;accuracy&#39;: 66.93366874089766, &#39;f1&#39;: 0.6597343453210113, &#39;precision&#39;: 0.6638601787800059, &#39;recall&#39;: 0.6693366874089766} . Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer) . In moving closer to build a model similar to the one in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it&#39;s time we tackled the hybrid token embedding layer they speak of. . . This hybrid token embedding layer is a combination of token embeddings and character embeddings. In other words, they create a stacked embedding to represent sequences before passing them to the sequence label prediction layer | . To start replicating (or getting close to replicating) the model in Figure 1, we&#39;re going to go through the following steps: . Create a token-level model (similar to model_1) | Create a character-level model (similar to model_3 with a slight modification to reflect the paper) | Combine (using layers.Concatenate) the outputs of 1 and 2 | Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts | Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output | 1 # Token_level Model (using Pretrained -- Universal Sentence Encoder) token_inputs = layers.Input(shape = [], dtype= tf.string, name = &quot;token_input&quot;) token_embedding = tf_hub_embedding_layer(token_inputs) token_dense = layers.Dense(128,activation=&quot;relu&quot;)(token_embedding) token_model = tf.keras.Model(inputs = token_inputs, outputs = token_dense) 2 # char_level Model char_inputs = layers.Input(shape=(1,), dtype= tf.string, name=&quot;char_input&quot;) char_vectors = char_vectorizer(char_inputs) char_embedding = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(25,activation=&quot;relu&quot;))(char_embedding) char_model = tf.keras.Model(inputs= char_inputs, # char_dense = layers.Dense(128,activation=&quot;relu&quot;)(char_bilstm) outputs =char_bi_lstm) 3 # Now Concatenate token_model and char_model concat_layer = layers.Concatenate(name = &quot;token_char_hybrid&quot;)([token_model.output, char_model.output]) 4 # Add Some Layer on top of concat_layer concat_dropout = layers.Dropout(0.5)(concat_layer) concat_dense = layers.Dense(256,activation=&quot;relu&quot;)(concat_dropout) final_dropout = layers.Dropout(0.2)(concat_dense) output_layer = layers.Dense(num_classes,activation=&quot;softmax&quot;)(final_dropout) model_4 = tf.keras.Model(inputs = [token_model.input, char_model.input], outputs = output_layer, name=&quot;model_4_token_and_char_embeddings&quot;) . model_4.summary() . Model: &#34;model_4_token_and_char_embeddings&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== char_input (InputLayer) [(None, 1)] 0 __________________________________________________________________________________________________ token_input (InputLayer) [(None,)] 0 __________________________________________________________________________________________________ char_vectorizer (TextVectorizat (None, 290) 0 char_input[0][0] __________________________________________________________________________________________________ universal_sentence_encoder (Ker (None, 512) 256797824 token_input[0][0] __________________________________________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 char_vectorizer[1][0] __________________________________________________________________________________________________ dense_4 (Dense) (None, 128) 65664 universal_sentence_encoder[1][0] __________________________________________________________________________________________________ bidirectional (Bidirectional) (None, 50) 10200 char_embed[1][0] __________________________________________________________________________________________________ token_char_hybrid (Concatenate) (None, 178) 0 dense_4[0][0] bidirectional[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 178) 0 token_char_hybrid[0][0] __________________________________________________________________________________________________ dense_5 (Dense) (None, 256) 45824 dropout_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 256) 0 dense_5[0][0] __________________________________________________________________________________________________ dense_6 (Dense) (None, 5) 1285 dropout_2[0][0] ================================================================================================== Total params: 256,922,547 Trainable params: 124,723 Non-trainable params: 256,797,824 __________________________________________________________________________________________________ . Visualize the Hybrid Model . tf.keras.utils.plot_model( model_4, to_file=&#39;model.png&#39;, show_shapes=False, show_dtype=False, show_layer_names=True, rankdir=&#39;TB&#39;, expand_nested=False, dpi=96 ) . model_4.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we&#39;ll stick with Adam metrics=[&quot;accuracy&quot;]) . train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels # Prefetch and batch train data train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Repeat same steps validation data val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars)) val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels)) val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) . train_char_token_dataset, val_char_token_dataset . (&lt;PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)&gt;, &lt;PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)&gt;) . model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters steps_per_epoch=int(0.1 * len(train_char_token_dataset)), epochs=10, validation_data=val_char_token_dataset, validation_steps=int(0.1 * len(val_char_token_dataset))) . Epoch 1/10 562/562 [==============================] - 167s 289ms/step - loss: 0.9316 - accuracy: 0.6286 - val_loss: 0.7693 - val_accuracy: 0.7048 Epoch 2/10 562/562 [==============================] - 162s 288ms/step - loss: 0.7773 - accuracy: 0.6966 - val_loss: 0.7133 - val_accuracy: 0.7271 Epoch 3/10 562/562 [==============================] - 163s 290ms/step - loss: 0.7541 - accuracy: 0.7097 - val_loss: 0.6872 - val_accuracy: 0.7424 Epoch 4/10 562/562 [==============================] - 163s 290ms/step - loss: 11.8752 - accuracy: 0.7228 - val_loss: 0.6679 - val_accuracy: 0.7463 Epoch 5/10 562/562 [==============================] - 163s 291ms/step - loss: 0.7405 - accuracy: 0.7171 - val_loss: 0.6611 - val_accuracy: 0.7473 Epoch 6/10 562/562 [==============================] - 164s 291ms/step - loss: 0.7305 - accuracy: 0.7208 - val_loss: 0.6454 - val_accuracy: 0.7580 Epoch 7/10 562/562 [==============================] - 164s 292ms/step - loss: 0.7063 - accuracy: 0.7299 - val_loss: 0.6443 - val_accuracy: 0.7600 Epoch 8/10 562/562 [==============================] - 164s 292ms/step - loss: 0.6984 - accuracy: 0.7340 - val_loss: 0.6335 - val_accuracy: 0.7557 Epoch 9/10 562/562 [==============================] - 162s 289ms/step - loss: 0.6996 - accuracy: 0.7311 - val_loss: 0.6434 - val_accuracy: 0.7557 Epoch 10/10 562/562 [==============================] - 162s 289ms/step - loss: 0.7018 - accuracy: 0.7309 - val_loss: 0.6321 - val_accuracy: 0.7603 . model_4.evaluate(val_char_token_dataset) . 945/945 [==============================] - 58s 61ms/step - loss: 23792656.0000 - accuracy: 0.7598 . [23792656.0, 0.7598305344581604] . model_4_pred_probs = model_4.predict(val_char_token_dataset) model_4_pred_probs . array([[4.81065422e-01, 3.40999186e-01, 1.79997389e-03, 1.66568533e-01, 9.56688821e-03], [2.74215668e-01, 6.57577336e-01, 1.48074096e-03, 6.47516549e-02, 1.97461597e-03], [4.36756134e-01, 5.47802337e-02, 5.64954877e-02, 4.24435914e-01, 2.75322516e-02], ..., [1.14926996e-04, 4.47407336e-04, 1.09152636e-02, 4.81208008e-05, 9.88474309e-01], [9.39613860e-03, 6.19878434e-02, 1.74138308e-01, 2.82176491e-03, 7.51655936e-01], [6.84660450e-02, 8.80190194e-01, 4.18375172e-02, 2.41007935e-03, 7.09615275e-03]], dtype=float32) . model_4_preds = tf.argmax(model_4_pred_probs, axis=1) model_4_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 0, ..., 4, 4, 1])&gt; . model_4_results = calculate_results(y_true=val_labels_encoded, y_pred=model_4_preds) model_4_results . {&#39;accuracy&#39;: 75.98305309148682, &#39;f1&#39;: 0.7545412454754323, &#39;precision&#39;: 0.7561886438045121, &#39;recall&#39;: 0.7598305309148683} . Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings . As it&#39;s a Sequential classification problem the sequences come in a particular order. Like OBJECTIVE comes first rather then CONCLUSION. . Abstracts typically come in a sequential order, such as: . OBJECTIVE ... | METHODS ... | METHODS ... | METHODS ... | RESULTS ... | CONCLUSIONS ... | . Or . BACKGROUND ... | OBJECTIVE ... | METHODS ... | METHODS ... | RESULTS ... | RESULTS ... | CONCLUSIONS ... | . Here we do some Feature Engineering so that our model can learn the order sentences in the Abstract and know where the sentence appear in the Abstract. The &quot;line_number&quot; and &quot;total_lines&quot; columns are features which didn&#39;t necessarily come with the training data but can be passed to our model as a positional embedding. . . Note: Positional Embedding make your model include the information about order of the input. The positional encoding step allows the model to recognize which part of the sequence an input belongs to. . But to avoid our model thinking a line with &quot;line_number&quot;=5 is five times greater than a line with &quot;line_number&quot;=1, we&#39;ll use one-hot-encoding to encode our &quot;line_number&quot; and &quot;total_lines&quot; features. . That is why we have to use one-hot encoding. We use tf.one_hot for it. . train_df[&quot;line_number&quot;].value_counts() . 0 15000 1 15000 2 15000 3 15000 4 14992 5 14949 6 14758 7 14279 8 13346 9 11981 10 10041 11 7892 12 5853 13 4152 14 2835 15 1861 16 1188 17 751 18 462 19 286 20 162 21 101 22 66 23 33 24 22 25 14 26 7 27 4 28 3 29 1 30 1 Name: line_number, dtype: int64 . train_df.line_number.plot.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fef52380690&gt; . Looking at the distribution of the &quot;line_number&quot; column, it looks like the majority of lines have a position of 15 or less. . Knowing this, let&#39;s set the depth parameter of tf.one_hot to 15. . train_line_numbers_one_hot = tf.one_hot(train_df[&quot;line_number&quot;].to_numpy(),depth= 15) val_line_numbers_one_hot = tf.one_hot(val_df[&quot;line_number&quot;].to_numpy(),depth= 15) test_line_numbers_one_hot = tf.one_hot(test_df[&quot;line_number&quot;].to_numpy(),depth= 15) . train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20] . (TensorShape([180040, 15]), &lt;tf.Tensor: shape=(20, 15), dtype=float32, numpy= array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;) . We could create a one-hot tensor which has room for all of the potential values of &quot;line_number&quot; (depth=30), however, this would end up in a tensor of double the size of our current one (depth=15) where the vast majority of values are 0. Plus, only ~2,000/180,000 samples have a &quot;line_number&quot; value of over 15. So we would not be gaining much information about our data for doubling our feature space. This kind of problem is called the curse of dimensionality. However, since this we&#39;re working with deep models, it might be worth trying to throw as much information at the model as possible and seeing what happens. I&#39;ll leave exploring values of the depth parameter as an extension. . We can do the same above process for the total line also in data. . train_df[&quot;total_lines&quot;].value_counts() . 11 24468 10 23639 12 22113 9 19400 13 18438 14 14610 8 12285 15 10768 7 7464 16 7429 17 5202 6 3353 18 3344 19 2480 20 1281 5 1146 21 770 22 759 23 264 4 215 24 200 25 182 26 81 28 58 3 32 30 31 27 28 Name: total_lines, dtype: int64 . train_df.total_lines.plot.hist(); . It shows that majority of data has line number below 20. We can perform numpy percentile to check this. . np.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples . 20.0 . train_total_lines_one_hot = tf.one_hot(train_df[&quot;total_lines&quot;].to_numpy(), depth=20) val_total_lines_one_hot = tf.one_hot(val_df[&quot;total_lines&quot;].to_numpy(), depth=20) test_total_lines_one_hot = tf.one_hot(test_df[&quot;total_lines&quot;].to_numpy(), depth=20) # Check shape and samples of total lines one-hot tensor train_total_lines_one_hot.shape, train_total_lines_one_hot[:10] . (TensorShape([180040, 20]), &lt;tf.Tensor: shape=(10, 20), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;) . Creating The Beast (tribrid embedding model) . Steps for Creating the Model: . Create a token-level model (similar to model_1) | Create a character-level model (similar to model_3 with a slight modification to reflect the paper) | Create a &quot;line_number&quot; model (takes in one-hot-encoded &quot;line_number&quot; tensor and passes it through a non-linear layer) | Create a &quot;total_lines&quot; model (takes in one-hot-encoded &quot;total_lines&quot; tensor and passes it through a non-linear layer) | Combine (using layers.Concatenate) the outputs of 1 and 2 into a token-character-hybrid embedding and pass it series of output to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts | Combine (using layers.Concatenate) the outputs of 3, 4 and 5 into a token-character-positional tribrid embedding | Create an output layer to accept the tribrid embedding and output predicted label probabilities | Combine the inputs of 1, 2, 3, 4 and outputs of 7 into a tf.keras.Model | # 1. Token Model token_inputs = layers.Input(shape=[], dtype=&quot;string&quot;, name=&quot;token_inputs&quot;) token_embeddings = tf_hub_embedding_layer(token_inputs) token_outputs = layers.Dense(128, activation=&quot;relu&quot;)(token_embeddings) token_model = tf.keras.Model(inputs=token_inputs, outputs=token_outputs) # 2. Char Model char_inputs = layers.Input(shape=(1,), dtype= tf.string, name=&quot;char_input&quot;) char_vectors = char_vectorizer(char_inputs) char_embedding = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(25,activation=&quot;relu&quot;))(char_embedding) char_model = tf.keras.Model(inputs= char_inputs, # char_dense = layers.Dense(128,activation=&quot;relu&quot;)(char_bilstm) outputs =char_bi_lstm) # 3. Line numbers inputs line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=&quot;line_number_input&quot;) x = layers.Dense(32, activation=&quot;relu&quot;)(line_number_inputs) line_number_model = tf.keras.Model(inputs=line_number_inputs, outputs=x) # 4. Total lines inputs total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=&quot;total_lines_input&quot;) y = layers.Dense(32, activation=&quot;relu&quot;)(total_lines_inputs) total_line_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y) # 5. Combine token and char embeddings into a hybrid embedding combined_embeddings = layers.Concatenate(name=&quot;token_char_hybrid_embedding&quot;)([token_model.output, char_model.output]) z = layers.Dense(256, activation=&quot;relu&quot;)(combined_embeddings) z = layers.Dropout(0.5)(z) # 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding z = layers.Concatenate(name=&quot;token_char_positional_embedding&quot;)([line_number_model.output, total_line_model.output, z]) # 7. Create output layer output_layer = layers.Dense(5, activation=&quot;softmax&quot;, name=&quot;output_layer&quot;)(z) # 8. Put together model model_5 = tf.keras.Model(inputs=[line_number_model.input, total_line_model.input, token_model.input, char_model.input], outputs=output_layer) . model_5.summary() . Model: &#34;model_8&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== char_input (InputLayer) [(None, 1)] 0 __________________________________________________________________________________________________ token_inputs (InputLayer) [(None,)] 0 __________________________________________________________________________________________________ char_vectorizer (TextVectorizat (None, 290) 0 char_input[0][0] __________________________________________________________________________________________________ universal_sentence_encoder (Ker (None, 512) 256797824 token_inputs[0][0] __________________________________________________________________________________________________ char_embed (Embedding) (None, 290, 25) 1750 char_vectorizer[2][0] __________________________________________________________________________________________________ dense_7 (Dense) (None, 128) 65664 universal_sentence_encoder[2][0] __________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 50) 10200 char_embed[2][0] __________________________________________________________________________________________________ token_char_hybrid_embedding (Co (None, 178) 0 dense_7[0][0] bidirectional_1[0][0] __________________________________________________________________________________________________ line_number_input (InputLayer) [(None, 15)] 0 __________________________________________________________________________________________________ total_lines_input (InputLayer) [(None, 20)] 0 __________________________________________________________________________________________________ dense_10 (Dense) (None, 256) 45824 token_char_hybrid_embedding[0][0] __________________________________________________________________________________________________ dense_8 (Dense) (None, 32) 512 line_number_input[0][0] __________________________________________________________________________________________________ dense_9 (Dense) (None, 32) 672 total_lines_input[0][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 256) 0 dense_10[0][0] __________________________________________________________________________________________________ token_char_positional_embedding (None, 320) 0 dense_8[0][0] dense_9[0][0] dropout_3[0][0] __________________________________________________________________________________________________ output_layer (Dense) (None, 5) 1605 token_char_positional_embedding[0 ================================================================================================== Total params: 256,924,051 Trainable params: 126,227 Non-trainable params: 256,797,824 __________________________________________________________________________________________________ . from tensorflow.keras.utils import plot_model plot_model(model_5) . model_5.compile(loss =tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), optimizer=&quot;Adam&quot;, metrics= [&quot;accuracy&quot;]) . train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers train_total_lines_one_hot, # total lines train_sentences, # train tokens train_chars)) # train chars train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately # Validation dataset val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot, val_total_lines_one_hot, val_sentences, val_chars)) val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels)) val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately # Check input shapes train_pos_char_token_dataset, val_pos_char_token_dataset . (&lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt;, &lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt;) . history_model_5 = model_5.fit(train_pos_char_token_dataset, steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)), epochs=10, validation_data=val_pos_char_token_dataset, validation_steps=int(0.1 * len(val_pos_char_token_dataset))) . Epoch 1/10 562/562 [==============================] - 166s 296ms/step - loss: 1.0464 - accuracy: 0.7606 - val_loss: 0.9803 - val_accuracy: 0.8042 Epoch 2/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9671 - accuracy: 0.8126 - val_loss: 0.9522 - val_accuracy: 0.8235 Epoch 3/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9502 - accuracy: 0.8240 - val_loss: 0.9400 - val_accuracy: 0.8301 Epoch 4/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9417 - accuracy: 0.8320 - val_loss: 0.9343 - val_accuracy: 0.8314 Epoch 5/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9384 - accuracy: 0.8335 - val_loss: 0.9268 - val_accuracy: 0.8378 Epoch 6/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9414 - accuracy: 0.8290 - val_loss: 0.9242 - val_accuracy: 0.8391 Epoch 7/10 562/562 [==============================] - 167s 298ms/step - loss: 0.9288 - accuracy: 0.8379 - val_loss: 0.9248 - val_accuracy: 0.8388 Epoch 8/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9249 - accuracy: 0.8428 - val_loss: 0.9119 - val_accuracy: 0.8507 Epoch 9/10 562/562 [==============================] - 166s 296ms/step - loss: 0.9270 - accuracy: 0.8385 - val_loss: 0.9236 - val_accuracy: 0.8411 Epoch 10/10 562/562 [==============================] - 167s 297ms/step - loss: 0.9226 - accuracy: 0.8458 - val_loss: 0.9117 - val_accuracy: 0.8411 . model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1) model_5_pred_probs . 945/945 [==============================] - 60s 62ms/step . array([[0.55797863, 0.11753111, 0.01437984, 0.28599715, 0.02411333], [0.5643995 , 0.12752993, 0.05248916, 0.24036951, 0.01521185], [0.35432863, 0.09137847, 0.1489166 , 0.3426122 , 0.06276409], ..., [0.02403956, 0.04742315, 0.01662572, 0.02715172, 0.88475984], [0.02105152, 0.34086674, 0.04562447, 0.02258216, 0.56987506], [0.10035124, 0.76991165, 0.05545753, 0.04023053, 0.03404894]], dtype=float32) . model_5_preds = tf.argmax(model_5_pred_probs, axis=1) model_5_preds . &lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 0, ..., 4, 4, 1])&gt; . model_5_results = calculate_results(y_true=val_labels_encoded, y_pred=model_5_preds) model_5_results . {&#39;accuracy&#39;: 84.80074142724744, &#39;f1&#39;: 0.8445848608307035, &#39;precision&#39;: 0.8499109192748401, &#39;recall&#39;: 0.8480074142724745} . all_model_results = pd.DataFrame({&quot;baseline&quot;: baseline_results, &quot;custom_token_embed_conv1d&quot;: model_1_results, &quot;pretrained_token_embed&quot;: model_2_results, &quot;custom_char_embed_conv1d&quot;: model_3_results, &quot;hybrid_char_token_embed&quot;: model_4_results, &quot;tribrid_pos_char_token_embed&quot;: model_5_results}) all_model_results = all_model_results.transpose() all_model_results . accuracy precision recall f1 . baseline 72.183238 | 0.718647 | 0.721832 | 0.698925 | . custom_token_embed_conv1d 80.828810 | 0.804780 | 0.808288 | 0.805473 | . pretrained_token_embed 75.105918 | 0.745737 | 0.751059 | 0.746384 | . custom_char_embed_conv1d 66.933669 | 0.663860 | 0.669337 | 0.659734 | . hybrid_char_token_embed 75.983053 | 0.756189 | 0.759831 | 0.754541 | . tribrid_pos_char_token_embed 84.800741 | 0.849911 | 0.848007 | 0.844585 | . all_model_results[&quot;accuracy&quot;] = all_model_results[&quot;accuracy&quot;]/100 . all_model_results.plot(kind=&quot;bar&quot;, figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0)); . all_model_results.sort_values(&quot;f1&quot;, ascending=False)[&quot;f1&quot;].plot(kind=&quot;bar&quot;, figsize=(10, 7)); . model_5.save(&quot;tribrid_model&quot;) . INFO:tensorflow:Assets written to: tribrid_model/assets . INFO:tensorflow:Assets written to: tribrid_model/assets . model_path = &quot;/content/tribrid_model&quot; . loaded_model = tf.keras.models.load_model(model_path) . loaded_pred_probs = loaded_model.predict(val_pos_char_token_dataset, verbose=1) loaded_preds = tf.argmax(loaded_pred_probs, axis=1) loaded_preds[:10] . 945/945 [==============================] - 61s 63ms/step . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 2, 2, 4, 4, 4, 4, 1])&gt; . loaded_model_results = calculate_results(val_labels_encoded, loaded_preds) loaded_model_results . {&#39;accuracy&#39;: 84.80074142724744, &#39;f1&#39;: 0.8445848608307035, &#39;precision&#39;: 0.8499109192748401, &#39;recall&#39;: 0.8480074142724745} . Evaluate model on test dataset . To make our model&#39;s performance more comparable with the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, let&#39;s make predictions on the test dataset and evaluate them. . test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot, test_total_lines_one_hot, test_sentences, test_chars)) test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot) test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels)) test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # Check shapes test_pos_char_token_dataset . &lt;PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)&gt; . test_pred_probs = loaded_model.predict(test_pos_char_token_dataset, verbose=1) test_preds = tf.argmax(test_pred_probs, axis=1) test_preds[:10] . 942/942 [==============================] - 58s 62ms/step . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 2, 2, 2, 4, 4, 4, 1, 1, 0])&gt; . loaded_model_test_results = calculate_results(y_true=test_labels_encoded, y_pred=test_preds) loaded_model_test_results . {&#39;accuracy&#39;: 84.48979591836735, &#39;f1&#39;: 0.8415058973152829, &#39;precision&#39;: 0.8456318536204341, &#39;recall&#39;: 0.8448979591836735} . %%time # Get list of class names of test predictions test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds] test_pred_classes . CPU times: user 3.43 s, sys: 10 ms, total: 3.44 s Wall time: 3.44 s . test_df[&quot;prediction&quot;] = test_pred_classes # create column with test prediction class names test_df[&quot;pred_prob&quot;] = tf.reduce_max(test_pred_probs, axis=1).numpy() # get the maximum prediction probability test_df[&quot;correct&quot;] = test_df[&quot;prediction&quot;] == test_df[&quot;target&quot;] # create binary column for whether the prediction is right or not test_df.head(20) . target text line_number total_lines prediction pred_prob correct . 0 BACKGROUND | this study analyzed liver function abnormaliti... | 0 | 8 | OBJECTIVE | 0.359128 | False | . 1 RESULTS | a post hoc analysis was conducted with the use... | 1 | 8 | METHODS | 0.395141 | False | . 2 RESULTS | liver function tests ( lfts ) were measured at... | 2 | 8 | METHODS | 0.857965 | False | . 3 RESULTS | survival analyses were used to assess the asso... | 3 | 8 | METHODS | 0.733716 | False | . 4 RESULTS | the percentage of patients with abnormal lfts ... | 4 | 8 | RESULTS | 0.734855 | True | . 5 RESULTS | when mean hemodynamic profiles were compared i... | 5 | 8 | RESULTS | 0.888223 | True | . 6 RESULTS | multivariable analyses revealed that patients ... | 6 | 8 | RESULTS | 0.569608 | True | . 7 CONCLUSIONS | abnormal lfts are common in the adhf populatio... | 7 | 8 | CONCLUSIONS | 0.476394 | True | . 8 CONCLUSIONS | elevated meld-xi scores are associated with po... | 8 | 8 | CONCLUSIONS | 0.583600 | True | . 9 BACKGROUND | minimally invasive endovascular aneurysm repai... | 0 | 12 | BACKGROUND | 0.633004 | True | . 10 BACKGROUND | the aim of this study was to analyse the cost-... | 1 | 12 | OBJECTIVE | 0.445351 | False | . 11 METHODS | resource use was determined from the amsterdam... | 2 | 12 | METHODS | 0.773279 | True | . 12 METHODS | the analysis was performed from a provider per... | 3 | 12 | METHODS | 0.865364 | True | . 13 METHODS | all costs were calculated as if all patients h... | 4 | 12 | METHODS | 0.490311 | True | . 14 RESULTS | a total of @ patients were randomized . | 5 | 12 | RESULTS | 0.788061 | True | . 15 RESULTS | the @-day mortality rate was @ per cent after ... | 6 | 12 | RESULTS | 0.816789 | True | . 16 RESULTS | at @months , the total mortality rate for evar... | 7 | 12 | RESULTS | 0.890838 | True | . 17 RESULTS | the mean cost difference between evar and or w... | 8 | 12 | RESULTS | 0.881075 | True | . 18 RESULTS | the incremental cost-effectiveness ratio per p... | 9 | 12 | RESULTS | 0.832961 | True | . 19 RESULTS | there was no significant difference in quality... | 10 | 12 | RESULTS | 0.817365 | True | . Future Work . As we trained our above Models with subset of actual data(PubMed 20k), training the same model with larger samples(PubMed 200k) might have chance of Increase in Accuracy. . | Except Universal Sentence Encoder, we&#39;ll try to replace embedding layers with preratined embedding (Contex Independent) like Word2Vec, GloVe and FastText and compare between them. . | Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the TensorFlow Hub BERT PubMed expert (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results? Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the TensorFlow Hub guide). Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf What happens if you were to merge our line_number and total_lines features for each sequence? For example, created a X_of_Y feature instead? Does this effect model performance? . | . Note: The main difference above is a consequence of the fact Word2vec and Glove do not take into account word order in their training - ELMo and BERT take into account word order (ELMo uses LSTMS; BERT uses Transformer - an attention based model with positional encodings to represent word positions). . .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/text%20classification/health%20care/deep%20learning/2021/04/10/Sentence-Classification.html",
            "relUrl": "/natural%20language%20processing/text%20classification/health%20care/deep%20learning/2021/04/10/Sentence-Classification.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Quora Insincere Questions Classification Using BERT",
            "content": ". Figure 1: BERT Classification Model . Overview . Predicting whether a question asked on Quora is sincere or not.The Model input a Question of Quora and Output &quot;sincere&quot; or &quot;Insincere&quot;. So Basically its a Binary Classification problem with Text data. . | As Quora has a large database and solving this probelm with Recurrent Neural Network (LSTM, GRU, Conv-1D) is little hard, So here we use State-Of-Art Transformer BERT(Bidirectional Encoder Representations from Transformers). . | The pretrained BERT model used in this project is available on TensorFlow Hub. . !nvidia-smi . Wed Oct 7 12:07:44 2020 +--+ | NVIDIA-SMI 455.23.05 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 35C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | | | | ERR! | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . Install TensorFlow and TensorFlow Model Garden . import tensorflow as tf print(tf.version.VERSION) . 2.3.0 . !pip install -q tensorflow==2.3.0 . !git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git . Cloning into &#39;models&#39;... remote: Enumerating objects: 2650, done. remote: Counting objects: 100% (2650/2650), done. remote: Compressing objects: 100% (2318/2318), done. remote: Total 2650 (delta 512), reused 1350 (delta 299), pack-reused 0 Receiving objects: 100% (2650/2650), 34.01 MiB | 3.93 MiB/s, done. Resolving deltas: 100% (512/512), done. Note: checking out &#39;400d68abbccda2f0f6609e3a924467718b144233&#39;. You are in &#39;detached HEAD&#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt; . !pip install -Uqr models/official/requirements.txt # you may have to restart the runtime afterwards . |████████████████████████████████| 61kB 1.9MB/s |████████████████████████████████| 194kB 7.4MB/s |████████████████████████████████| 14.5MB 240kB/s |████████████████████████████████| 9.5MB 56.1MB/s |████████████████████████████████| 460kB 70.0MB/s |████████████████████████████████| 102kB 12.7MB/s |████████████████████████████████| 25.9MB 95kB/s |████████████████████████████████| 174kB 66.8MB/s |████████████████████████████████| 3.5MB 24.2MB/s |████████████████████████████████| 1.1MB 69.9MB/s |████████████████████████████████| 358kB 81.1MB/s |████████████████████████████████| 1.1MB 79.4MB/s |████████████████████████████████| 11.6MB 222kB/s |████████████████████████████████| 36.7MB 89kB/s |████████████████████████████████| 276kB 63.8MB/s |████████████████████████████████| 2.2MB 59.1MB/s |████████████████████████████████| 92kB 11.7MB/s |████████████████████████████████| 81kB 11.8MB/s |████████████████████████████████| 501kB 61.7MB/s Building wheel for psutil (setup.py) ... done Building wheel for py-cpuinfo (setup.py) ... done Building wheel for pyyaml (setup.py) ... done Building wheel for proto-plus (setup.py) ... done ERROR: tensorflow 2.3.0 has requirement numpy&lt;1.19.0,&gt;=1.16.0, but you&#39;ll have numpy 1.19.2 which is incompatible. ERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you&#39;ll have scipy 1.5.2 which is incompatible. ERROR: google-cloud-storage 1.18.1 has requirement google-resumable-media&lt;0.5.0dev,&gt;=0.3.1, but you&#39;ll have google-resumable-media 1.1.0 which is incompatible. ERROR: google-api-core 1.22.4 has requirement google-auth&lt;2.0dev,&gt;=1.21.1, but you&#39;ll have google-auth 1.17.2 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: albumentations 0.1.12 has requirement imgaug&lt;0.2.7,&gt;=0.2.5, but you&#39;ll have imgaug 0.2.9 which is incompatible. . Download and Import the Quora Insincere Questions Dataset . import numpy as np import tensorflow as tf import tensorflow_hub as hub import sys sys.path.append(&#39;models&#39;) from official.nlp.data import classifier_data_lib from official.nlp.bert import tokenization from official.nlp import optimization . print(&quot;TF Version: &quot;, tf.__version__) print(&quot;Eager mode: &quot;, tf.executing_eagerly()) print(&quot;Hub version: &quot;, hub.__version__) print(&quot;GPU is&quot;, &quot;available&quot; if tf.config.experimental.list_physical_devices(&quot;GPU&quot;) else &quot;NOT AVAILABLE&quot;) . TF Version: 2.3.0 Eager mode: True Hub version: 0.9.0 GPU is available . A downloadable copy of the Quora Insincere Questions Classification data can be found https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip. Decompress and read the data into a pandas DataFrame. . import numpy as np import pandas as pd # reading the dataset from the link or you can download it from kaggle alos # https://www.kaggle.com/c/quora-insincere-questions-classification/data df = pd.read_csv(&#39;https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip&#39;, compression = &#39;zip&#39;, low_memory = False) # checking the no of row and columns in the dataset df.shape . (1306122, 3) . df.head(20) . qid question_text target . 0 00002165364db923c7e6 | How did Quebec nationalists see their province... | 0 | . 1 000032939017120e6e44 | Do you have an adopted dog, how would you enco... | 0 | . 2 0000412ca6e4628ce2cf | Why does velocity affect time? Does velocity a... | 0 | . 3 000042bf85aa498cd78e | How did Otto von Guericke used the Magdeburg h... | 0 | . 4 0000455dfa3e01eae3af | Can I convert montra helicon D to a mountain b... | 0 | . 5 00004f9a462a357c33be | Is Gaza slowly becoming Auschwitz, Dachau or T... | 0 | . 6 00005059a06ee19e11ad | Why does Quora automatically ban conservative ... | 0 | . 7 0000559f875832745e2e | Is it crazy if I wash or wipe my groceries off... | 0 | . 8 00005bd3426b2d0c8305 | Is there such a thing as dressing moderately, ... | 0 | . 9 00006e6928c5df60eacb | Is it just me or have you ever been in this ph... | 0 | . 10 000075f67dd595c3deb5 | What can you say about feminism? | 0 | . 11 000076f3b42776c692de | How were the Calgary Flames founded? | 0 | . 12 000089792b3fc8026741 | What is the dumbest, yet possibly true explana... | 0 | . 13 000092a90bcfbfe8cd88 | Can we use our external hard disk as a OS as w... | 0 | . 14 000095680e41a9a6f6e3 | I am 30, living at home and have no boyfriend.... | 0 | . 15 0000a89942e3143e333a | What do you know about Bram Fischer and the Ri... | 0 | . 16 0000b8e1279eaa0a7062 | How difficult is it to find a good instructor ... | 0 | . 17 0000bc0f62500f55959f | Have you licked the skin of a corpse? | 0 | . 18 0000ce6c31f14d3e09ec | Do you think Amazon will adopt an in house app... | 0 | . 19 0000d329332845b8a7fa | How many baronies might exist within a county ... | 0 | . df.target.plot(kind = &#39;hist&#39;, title = &#39;target distribution&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;target distribution&#39;}, ylabel=&#39;Frequency&#39;&gt; . As you can see here its a imbalanced dataset, we have to set our split data show that both the target labels should be present in train and test set. So we use stratified sampling to overcome this. . Create tf.data.Datasets for Training and Evaluation . . Note: As our dataset has a class imbalance issue, we use stratify sampling to overcome this. . from sklearn.model_selection import train_test_split train_df , remaining = train_test_split(df, random_state = 42 , train_size = 0.0075, stratify = df.target.values) valid_df , _ = train_test_split(remaining, random_state= 42 , train_size = 0.00075, stratify =remaining.target.values) # ensure the shape of both train and test data train_df.shape , valid_df.shape . ((9795, 3), (972, 3)) . As the dataset is preety huge,the whole dataset take much longer to train so we use only a small train and test portion, set a ratio of 90% train and 10% test.Secondly to overcome the io bottleneck we use tf.data.Dataset pipelines. . with tf.device(&#39;/cpu:0&#39;): train_data = tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values)) valid_data = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values)) # reading from tensorflow data pipeline for text,label in train_data.take(1): print(text) print(label) . tf.Tensor(b&#39;Why are unhealthy relationships so desirable?&#39;, shape=(), dtype=string) tf.Tensor(0, shape=(), dtype=int64) . Download a Pre-trained BERT Model from TensorFlow Hub . &quot;&quot;&quot; Each line of the dataset is composed of the review text and its label - Data preprocessing consists of transforming text to BERT input features: input_word_ids, input_mask, segment_ids - In the process, tokenizing the text is done with the provided BERT model tokenizer &quot;&quot;&quot; # Label categories label_list = [0,1] # maximum length of (token) input sequences max_seq_length = 128 # Define the batch size train_batch_size = 32 # Get BERT layer and tokenizer: # BERT details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2 bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;, trainable=True) vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case) . tokenizer.wordpiece_tokenizer.tokenize(&#39;hey, how are you ?&#39;) . [&#39;hey&#39;, &#39;##,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;?&#39;] . tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize(&#39;hey, how are you ?&#39;)) . [4931, 29623, 2129, 2024, 2017, 1029] . Tokenize and Preprocess Text for BERT . We&#39;ll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExamples using classifier_data_lib&#39;s constructor InputExample provided in the BERT library. . def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer): example = classifier_data_lib.InputExample(guid = None, text_a= text.numpy(), text_b =None, label= label.numpy()) feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer) return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id) . You want to use Dataset.map to apply this function to each element of the dataset. Dataset.map runs in graph mode. . Graph tensors do not have a value. | In graph mode you can only use TensorFlow Ops and functions. | . So you can&#39;t .map this function directly: You need to wrap it in a tf.py_function. The tf.py_function will pass regular tensors (with a value and a .numpy() method to access it), to the wrapped python function. . Wrap a Python Function into a TensorFlow op for Eager Execution . def to_feature_map(text, label): input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature,inp =[text, label], Tout=[tf.int32,tf.int32,tf.int32,tf.int32]) input_ids.set_shape([max_seq_length]) input_mask.set_shape([max_seq_length]) segment_ids.set_shape([max_seq_length]) label_id.set_shape([]) x = { &#39;input_word_ids&#39;: input_ids, &#39;input_mask&#39;: input_mask, &#39;input_type_ids&#39;: segment_ids } return (x,label_id) . Create a TensorFlow Input Pipeline with tf.data . with tf.device(&#39;/cpu:0&#39;): # train train_data = (train_data.map(to_feature_map, num_parallel_calls =tf.data.experimental.AUTOTUNE) .shuffle(1000) .batch(32, drop_remainder = True) .prefetch(tf.data.experimental.AUTOTUNE)) # valid valid_data = (valid_data.map(to_feature_map, num_parallel_calls =tf.data.experimental.AUTOTUNE) .batch(32, drop_remainder = True) .prefetch(tf.data.experimental.AUTOTUNE)) . The resulting tf.data.Datasets return (features, labels) pairs, as expected by keras.Model.fit: . train_data.element_spec . ({&#39;input_mask&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_type_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_word_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(32,), dtype=tf.int32, name=None)) . valid_data.element_spec . ({&#39;input_mask&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_type_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), &#39;input_word_ids&#39;: TensorSpec(shape=(32, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(32,), dtype=tf.int32, name=None)) . Add a Classification Head to the BERT Layer . Figure 3: BERT Layer . def create_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_word_ids&quot;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_mask&quot;) input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&quot;input_type_ids&quot;) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids]) drop = tf.keras.layers.Dropout(0.4)(pooled_output) output = tf.keras.layers.Dense(1, activation= &#39;sigmoid&#39;, name = &#39;output&#39;)(drop) model = tf.keras.Model( inputs = { &#39;input_word_ids&#39;: input_word_ids, &#39;input_mask&#39;: input_mask, &#39;input_type_ids&#39;: input_type_ids }, outputs = output ) return model . Fine-Tune BERT for Text Classification . model = create_model() model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=2e-5), loss = tf.keras.losses.BinaryCrossentropy(), metrics = [tf.keras.metrics.BinaryAccuracy()]) model.summary() . Model: &#34;functional_7&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_word_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_mask (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ input_type_ids (InputLayer) [(None, 128)] 0 __________________________________________________________________________________________________ keras_layer (KerasLayer) [(None, 768), (None, 109482241 input_word_ids[0][0] input_mask[0][0] input_type_ids[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 768) 0 keras_layer[4][0] __________________________________________________________________________________________________ output (Dense) (None, 1) 769 dropout_4[0][0] ================================================================================================== Total params: 109,483,010 Trainable params: 109,483,009 Non-trainable params: 1 __________________________________________________________________________________________________ . tf.keras.utils.plot_model(model=model, show_shapes=True, dpi =75) . epochs = 4 history = model.fit(train_data, validation_data=valid_data, epochs = epochs, verbose = 1) . Epoch 1/4 306/306 [==============================] - ETA: 0s - loss: 0.0148 - binary_accuracy: 0.9953WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_test_batch_end` time: 0.1383s). Check your callbacks. . WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_test_batch_end` time: 0.1383s). Check your callbacks. . 306/306 [==============================] - 146s 478ms/step - loss: 0.0148 - binary_accuracy: 0.9953 - val_loss: 0.2203 - val_binary_accuracy: 0.9500 Epoch 2/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0068 - binary_accuracy: 0.9979 - val_loss: 0.2746 - val_binary_accuracy: 0.9594 Epoch 3/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0089 - binary_accuracy: 0.9971 - val_loss: 0.2665 - val_binary_accuracy: 0.9479 Epoch 4/4 306/306 [==============================] - 146s 478ms/step - loss: 0.0081 - binary_accuracy: 0.9975 - val_loss: 0.2688 - val_binary_accuracy: 0.9573 . Evaluate the BERT Text Classification Model . import matplotlib.pyplot as plt def plot_graphs(history, metric): plt.plot(history.history[metric]) plt.plot(history.history[&#39;val_&#39;+metric], &#39;&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(metric) plt.legend([metric, &#39;val_&#39;+metric]) plt.show() . plot_graphs(history, &#39;binary_accuracy&#39;) . plot_graphs(history, &#39;loss&#39;) . sample_examples = [&#39;Are you ashamed of being an Indian?&#39;,&#39; you are a racist&#39;, &#39; Its really helpfull, thank you&#39;, &#39; Thanks for you help&#39;,] test_data = tf.data.Dataset.from_tensor_slices((sample_examples, [0]*len(sample_examples))) test_data = (test_data.map(to_feature_map).batch(1)) preds = model.predict(test_data) threshold = 0.7 [&#39;Insincere&#39; if pred&gt;= threshold else &#39;Sincere&#39; for pred in preds] . [&#39;Insincere&#39;, &#39;Insincere&#39;, &#39;Sincere&#39;, &#39;Sincere&#39;] . Useful Links . https://jalammar.github.io/illustrated-transformer/ | https://jalammar.github.io/illustrated-bert/ | . https://nlp.seas.harvard.edu/2018/04/03/attention.html | https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/ | .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/text%20classification/deep%20learning/2021/03/10/Quora_Insincere-Classification-BERT.html",
            "relUrl": "/natural%20language%20processing/text%20classification/deep%20learning/2021/03/10/Quora_Insincere-Classification-BERT.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Sentiment Analysis of Play store apps review with BERT and PyTorch",
            "content": "Overview . In this Project, we&#39;ll learn how to fine-tune BERT for sentiment analysis. You&#39;ll do the required text preprocessing (special tokens, padding, and attention masks) and build a Sentiment Classifier using the amazing Transformers library by Hugging Face! . You&#39;ll learn how to: . Intuitively understand what BERT | Preprocess text data for BERT and build PyTorch Dataset (tokenization, attention masks, and padding) | Use Transfer Learning to build Sentiment Classifier using the Transformers library by Hugging Face | Evaluate the model on test data | Predict sentiment on raw text | . !nvidia-smi . Mon Apr 20 19:22:31 2020 +--+ | NVIDIA-SMI 440.64.00 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 27W / 250W | 0MiB / 16280MiB | 0% Default | +-+-+-+ +--+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +--+ . What is BERT? . BERT (introduced in this paper) stands for Bidirectional Encoder Representations from Transformers. If you don&#39;t know what most of that means - you&#39;ve come to the right place! Let&#39;s unpack the main ideas: . Bidirectional - to understand the text you&#39;re looking you&#39;ll have to look back (at the previous words) and forward (at the next words) | Transformers - The Attention Is All You Need paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. his in a sentence refers to Jim). | (Pre-trained) contextualized word embeddings - The ELMO paper introduced a way to encode words based on their meaning/context. Nails has multiple meanings - fingernails and metal nails. | . BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence. Let&#39;s look at examples of these tasks: . Masked Language Modeling (Masked LM) . The objective of this task is to guess the masked tokens. Let&#39;s look at an example, and try to not make it harder than it has to be: . That&#39;s [mask] she [mask] -&gt; That&#39;s what she said . Next Sentence Prediction (NSP) . Given a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let&#39;s continue with the example: . Input = [CLS] That&#39;s [mask] she [mask]. [SEP] Hahaha, nice! [SEP] . Label = IsNext . Input = [CLS] That&#39;s [mask] she [mask]. [SEP] Dwight, you ignorant [mask]! [SEP] . Label = NotNext . The training corpus was comprised of two entries: Toronto Book Corpus (800M words) and English Wikipedia (2,500M words). While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder. . BERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large). . Is This Thing Useful in Practice? . The BERT paper was released along with the source code and pre-trained models. . The best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance! . Setup . We&#39;ll need the Transformers library by Hugging Face: . . Tip: An IPython magic extension for printing date and time stamps, version numbers, and hardware information. . !pip install -q -U watermark . !pip install -qq transformers . %reload_ext watermark %watermark -v -p numpy,pandas,torch,transformers . CPython 3.6.9 IPython 5.5.0 numpy 1.18.2 pandas 1.0.3 torch 1.4.0 transformers 2.8.0 . import transformers from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup import torch import numpy as np import pandas as pd import seaborn as sns from pylab import rcParams import matplotlib.pyplot as plt from matplotlib import rc from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, classification_report from collections import defaultdict from textwrap import wrap from torch import nn, optim from torch.utils.data import Dataset, DataLoader import torch.nn.functional as F %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; sns.set(style=&#39;whitegrid&#39;, palette=&#39;muted&#39;, font_scale=1.2) HAPPY_COLORS_PALETTE = [&quot;#01BEFE&quot;, &quot;#FFDD00&quot;, &quot;#FF7D00&quot;, &quot;#FF006D&quot;, &quot;#ADFF02&quot;, &quot;#8F00FF&quot;] sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE)) rcParams[&#39;figure.figsize&#39;] = 12, 8 RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) torch.manual_seed(RANDOM_SEED) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) device . device(type=&#39;cuda&#39;, index=0) . Data Exploration . We&#39;ll load the Google Play app reviews dataset, that we&#39;ve put together in the previous part: . !gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV !gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv . Downloading... From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV To: /content/apps.csv 100% 134k/134k [00:00&lt;00:00, 50.2MB/s] Downloading... From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv To: /content/reviews.csv 7.17MB [00:00, 33.4MB/s] . df = pd.read_csv(&quot;reviews.csv&quot;) df.head() . userName userImage content score thumbsUpCount reviewCreatedVersion at replyContent repliedAt sortOrder appId . 0 Andrew Thomas | https://lh3.googleusercontent.com/a-/AOh14GiHd... | Update: After getting a response from the deve... | 1 | 21 | 4.17.0.3 | 2020-04-05 22:25:57 | According to our TOS, and the term you have ag... | 2020-04-05 15:10:24 | most_relevant | com.anydo | . 1 Craig Haines | https://lh3.googleusercontent.com/-hoe0kwSJgPQ... | Used it for a fair amount of time without any ... | 1 | 11 | 4.17.0.3 | 2020-04-04 13:40:01 | It sounds like you logged in with a different ... | 2020-04-05 15:11:35 | most_relevant | com.anydo | . 2 steven adkins | https://lh3.googleusercontent.com/a-/AOh14GiXw... | Your app sucks now!!!!! Used to be good but no... | 1 | 17 | 4.17.0.3 | 2020-04-01 16:18:13 | This sounds odd! We are not aware of any issue... | 2020-04-02 16:05:56 | most_relevant | com.anydo | . 3 Lars Panzerbjørn | https://lh3.googleusercontent.com/a-/AOh14Gg-h... | It seems OK, but very basic. Recurring tasks n... | 1 | 192 | 4.17.0.2 | 2020-03-12 08:17:34 | We do offer this option as part of the Advance... | 2020-03-15 06:20:13 | most_relevant | com.anydo | . 4 Scott Prewitt | https://lh3.googleusercontent.com/-K-X1-YsVd6U... | Absolutely worthless. This app runs a prohibit... | 1 | 42 | 4.17.0.2 | 2020-03-14 17:41:01 | We&#39;re sorry you feel this way! 90% of the app ... | 2020-03-15 23:45:51 | most_relevant | com.anydo | . df.shape . (15746, 11) . We have about 16k examples. Let&#39;s check for missing values: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 15746 entries, 0 to 15745 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 userName 15746 non-null object 1 userImage 15746 non-null object 2 content 15746 non-null object 3 score 15746 non-null int64 4 thumbsUpCount 15746 non-null int64 5 reviewCreatedVersion 13533 non-null object 6 at 15746 non-null object 7 replyContent 7367 non-null object 8 repliedAt 7367 non-null object 9 sortOrder 15746 non-null object 10 appId 15746 non-null object dtypes: int64(2), object(9) memory usage: 1.3+ MB . Great, no missing values in the score and review texts! Do we have class imbalance? . sns.countplot(df.score) plt.xlabel(&#39;review score&#39;); . That&#39;s hugely imbalanced, but it&#39;s okay. We&#39;re going to convert the dataset into negative, neutral and positive sentiment: . def to_sentiment(rating): rating = int(rating) if rating &lt;= 2: return 0 elif rating == 3: return 1 else: return 2 df[&#39;sentiment&#39;] = df.score.apply(to_sentiment) . class_names = [&#39;negative&#39;, &#39;neutral&#39;, &#39;positive&#39;] . ax = sns.countplot(df.sentiment) plt.xlabel(&#39;review sentiment&#39;) ax.set_xticklabels(class_names); . The balance was (mostly) restored. . Data Preprocessing . You might already know that Machine Learning models don&#39;t work with raw text. You need to convert text to numbers (of some sort). BERT requires even more attention (good one, right?). Here are the requirements: . Add special tokens to separate sentences and do classification | Pass sequences of constant length (introduce padding) | Create array of 0s (pad token) and 1s (real token) called attention mask | . The Transformers library provides (you&#39;ve guessed it) a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers that do the heavy lifting for us! . PRE_TRAINED_MODEL_NAME = &#39;bert-base-cased&#39; . You can use a cased and uncased version of BERT and tokenizer. I&#39;ve experimented with both. The cased version works better. Intuitively, that makes sense, since &quot;BAD&quot; might convey more sentiment than &quot;bad&quot;. . Let&#39;s load a pre-trained BertTokenizer: . tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) . We&#39;ll use this text to understand the tokenization process: . sample_txt = &#39;When was I last outside? I am stuck at home for 2 weeks.&#39; . Some basic operations can convert the text to tokens and tokens to unique integers (ids): . tokens = tokenizer.tokenize(sample_txt) token_ids = tokenizer.convert_tokens_to_ids(tokens) print(f&#39; Sentence: {sample_txt}&#39;) print(f&#39; Tokens: {tokens}&#39;) print(f&#39;Token IDs: {token_ids}&#39;) . Sentence: When was I last outside? I am stuck at home for 2 weeks. Tokens: [&#39;When&#39;, &#39;was&#39;, &#39;I&#39;, &#39;last&#39;, &#39;outside&#39;, &#39;?&#39;, &#39;I&#39;, &#39;am&#39;, &#39;stuck&#39;, &#39;at&#39;, &#39;home&#39;, &#39;for&#39;, &#39;2&#39;, &#39;weeks&#39;, &#39;.&#39;] Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119] . Special Tokens . [SEP] - marker for ending of a sentence . tokenizer.sep_token, tokenizer.sep_token_id . (&#39;[SEP]&#39;, 102) . [CLS] - we must add this token to the start of each sentence, so BERT knows we&#39;re doing classification . tokenizer.cls_token, tokenizer.cls_token_id . (&#39;[CLS]&#39;, 101) . There is also a special token for padding: . tokenizer.pad_token, tokenizer.pad_token_id . (&#39;[PAD]&#39;, 0) . BERT understands tokens that were in the training set. Everything else can be encoded using the [UNK] (unknown) token: . tokenizer.unk_token, tokenizer.unk_token_id . (&#39;[UNK]&#39;, 100) . All of that work can be done using the encode_plus() method: . encoding = tokenizer.encode_plus( sample_txt, max_length=32, add_special_tokens=True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, # Return PyTorch tensors ) encoding.keys() . dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]) . The token ids are now stored in a Tensor and padded to a length of 32: . print(len(encoding[&#39;input_ids&#39;][0])) encoding[&#39;input_ids&#39;][0] . 32 . tensor([ 101, 1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . The attention mask has the same length: . print(len(encoding[&#39;attention_mask&#39;][0])) encoding[&#39;attention_mask&#39;] . 32 . tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) . We can inverse the tokenization to have a look at the special tokens: . tokenizer.convert_ids_to_tokens(encoding[&#39;input_ids&#39;][0]) . [&#39;[CLS]&#39;, &#39;When&#39;, &#39;was&#39;, &#39;I&#39;, &#39;last&#39;, &#39;outside&#39;, &#39;?&#39;, &#39;I&#39;, &#39;am&#39;, &#39;stuck&#39;, &#39;at&#39;, &#39;home&#39;, &#39;for&#39;, &#39;2&#39;, &#39;weeks&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;, &#39;[PAD]&#39;] . Choosing Sequence Length . BERT works with fixed-length sequences. We&#39;ll use a simple strategy to choose the max length. Let&#39;s store the token length of each review: . token_lens = [] for txt in df.content: tokens = tokenizer.encode(txt, max_length=512) token_lens.append(len(tokens)) . and plot the distribution: . sns.distplot(token_lens) plt.xlim([0, 256]); plt.xlabel(&#39;Token count&#39;); . Most of the reviews seem to contain less than 128 tokens, but we&#39;ll be on the safe side and choose a maximum length of 160. . MAX_LEN = 160 . We have all building blocks required to create a PyTorch dataset. Let&#39;s do it: . class GPReviewDataset(Dataset): def __init__(self, reviews, targets, tokenizer, max_len): self.reviews = reviews self.targets = targets self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.reviews) def __getitem__(self, item): review = str(self.reviews[item]) target = self.targets[item] encoding = self.tokenizer.encode_plus( review, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, ) return { &#39;review_text&#39;: review, &#39;input_ids&#39;: encoding[&#39;input_ids&#39;].flatten(), &#39;attention_mask&#39;: encoding[&#39;attention_mask&#39;].flatten(), &#39;targets&#39;: torch.tensor(target, dtype=torch.long) } . The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it&#39;ll be easier to evaluate the predictions from our model. Let&#39;s split the data: . df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED) df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED) . df_train.shape, df_val.shape, df_test.shape . ((14171, 12), (787, 12), (788, 12)) . We also need to create a couple of data loaders. Here&#39;s a helper function to do it: . def create_data_loader(df, tokenizer, max_len, batch_size): ds = GPReviewDataset( reviews=df.content.to_numpy(), targets=df.sentiment.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 ) . BATCH_SIZE = 16 train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE) test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE) . Let&#39;s have a look at an example batch from our training data loader: . data = next(iter(train_data_loader)) data.keys() . dict_keys([&#39;review_text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;targets&#39;]) . print(data[&#39;input_ids&#39;].shape) print(data[&#39;attention_mask&#39;].shape) print(data[&#39;targets&#39;].shape) . torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16]) . Sentiment Classification with BERT and Hugging Face . There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use BertForSequenceClassification, BertForQuestionAnswering or something else. . But who cares, right? We&#39;re hardcore! We&#39;ll use the basic BertModel and build our sentiment classifier on top of it. Let&#39;s load the model: . bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) . And try to use it on the encoding of our sample text: . last_hidden_state, pooled_output = bert_model( input_ids=encoding[&#39;input_ids&#39;], attention_mask=encoding[&#39;attention_mask&#39;] ) . The last_hidden_state is a sequence of hidden states of the last layer of the model. Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state: . last_hidden_state.shape . torch.Size([1, 32, 768]) . We have the hidden state for each of our 32 tokens (the length of our example sequence). But why 768? This is the number of hidden units in the feedforward-networks. We can verify that by checking the config: . bert_model.config.hidden_size . 768 . You can think of the pooled_output as a summary of the content, according to BERT. Albeit, you might try and do better. Let&#39;s look at the shape of the output: . pooled_output.shape . torch.Size([1, 768]) . We can use all of this knowledge to create a classifier that uses the BERT model: . class SentimentClassifier(nn.Module): def __init__(self, n_classes): super(SentimentClassifier, self).__init__() self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) . Our classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we&#39;re returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work. . This should work like any other PyTorch model. Let&#39;s create an instance and move it to the GPU: . model = SentimentClassifier(len(class_names)) model = model.to(device) . We&#39;ll move the example batch of our training data to the GPU: . input_ids = data[&#39;input_ids&#39;].to(device) attention_mask = data[&#39;attention_mask&#39;].to(device) print(input_ids.shape) # batch size x seq length print(attention_mask.shape) # batch size x seq length . torch.Size([16, 160]) torch.Size([16, 160]) . To get the predicted probabilities from our trained model, we&#39;ll apply the softmax function to the outputs: . F.softmax(model(input_ids, attention_mask), dim=1) . tensor([[0.5879, 0.0842, 0.3279], [0.4308, 0.1888, 0.3804], [0.4871, 0.1766, 0.3363], [0.3364, 0.0778, 0.5858], [0.4025, 0.1040, 0.4935], [0.3599, 0.1026, 0.5374], [0.5054, 0.1552, 0.3394], [0.5962, 0.1464, 0.2574], [0.3274, 0.1967, 0.4759], [0.3026, 0.1118, 0.5856], [0.4103, 0.1571, 0.4326], [0.4879, 0.2121, 0.3000], [0.3811, 0.1477, 0.4712], [0.3354, 0.1354, 0.5292], [0.3999, 0.2822, 0.3179], [0.5075, 0.1684, 0.3242]], device=&#39;cuda:0&#39;, grad_fn=&lt;SoftmaxBackward&gt;) . Training . To reproduce the training procedure from the BERT paper, we&#39;ll use the AdamW optimizer provided by Hugging Face. It corrects weight decay, so it&#39;s similar to the original paper. We&#39;ll also use a linear scheduler with no warmup steps: . EPOCHS = 10 optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False) total_steps = len(train_data_loader) * EPOCHS scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(device) . How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning: . Batch size: 16, 32 | Learning rate (Adam): 5e-5, 3e-5, 2e-5 | Number of epochs: 2, 3, 4 | . We&#39;re going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy. . Let&#39;s continue with writing a helper function for training our model for one epoch: . def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples ): model = model.train() losses = [] correct_predictions = 0 for d in data_loader: input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) . Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We&#39;re avoiding exploding gradients by clipping the gradients of the model using clip_gradnorm. . Let&#39;s write another one that helps us evaluate the model on a given data loader: . def eval_model(model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) . Using those two, we can write our training loop. We&#39;ll also store the training history: . %%time history = defaultdict(list) best_accuracy = 0 for epoch in range(EPOCHS): print(f&#39;Epoch {epoch + 1}/{EPOCHS}&#39;) print(&#39;-&#39; * 10) train_acc, train_loss = train_epoch( model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train) ) print(f&#39;Train loss {train_loss} accuracy {train_acc}&#39;) val_acc, val_loss = eval_model( model, val_data_loader, loss_fn, device, len(df_val) ) print(f&#39;Val loss {val_loss} accuracy {val_acc}&#39;) print() history[&#39;train_acc&#39;].append(train_acc) history[&#39;train_loss&#39;].append(train_loss) history[&#39;val_acc&#39;].append(val_acc) history[&#39;val_loss&#39;].append(val_loss) if val_acc &gt; best_accuracy: torch.save(model.state_dict(), &#39;best_model_state.bin&#39;) best_accuracy = val_acc . Epoch 1/10 - Train loss 0.7330631300571541 accuracy 0.6653729447463129 Val loss 0.5767546480894089 accuracy 0.7776365946632783 Epoch 2/10 - Train loss 0.4158683338330777 accuracy 0.8420012701997036 Val loss 0.5365073362737894 accuracy 0.832274459974587 Epoch 3/10 - Train loss 0.24015077009679367 accuracy 0.922023851527768 Val loss 0.5074492372572422 accuracy 0.8716645489199493 Epoch 4/10 - Train loss 0.16012676668187295 accuracy 0.9546962105708843 Val loss 0.6009970247745514 accuracy 0.8703939008894537 Epoch 5/10 - Train loss 0.11209654617575301 accuracy 0.9675393409074872 Val loss 0.7367783848941326 accuracy 0.8742058449809403 Epoch 6/10 - Train loss 0.08572274737026433 accuracy 0.9764307388328276 Val loss 0.7251267762482166 accuracy 0.8843710292249047 Epoch 7/10 - Train loss 0.06132202987342602 accuracy 0.9833462705525369 Val loss 0.7083295831084251 accuracy 0.889453621346887 Epoch 8/10 - Train loss 0.050604159273123096 accuracy 0.9849693035071626 Val loss 0.753860274553299 accuracy 0.8907242693773825 Epoch 9/10 - Train loss 0.04373276197092931 accuracy 0.9862395032107826 Val loss 0.7506809896230697 accuracy 0.8919949174078781 Epoch 10/10 - Train loss 0.03768671146314381 accuracy 0.9880036694658105 Val loss 0.7431786182522774 accuracy 0.8932655654383737 CPU times: user 29min 54s, sys: 13min 28s, total: 43min 23s Wall time: 43min 43s . Note that we&#39;re storing the state of the best model, indicated by the highest validation accuracy. . Whoo, this took some time! We can look at the training vs validation accuracy: . plt.plot(history[&#39;train_acc&#39;], label=&#39;train accuracy&#39;) plt.plot(history[&#39;val_acc&#39;], label=&#39;validation accuracy&#39;) plt.title(&#39;Training history&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend() plt.ylim([0, 1]); . The training accuracy starts to approach 100% after 10 epochs or so. You might try to fine-tune the parameters a bit more, but this will be good enough for us. . Uncomment the next cell and download the pre-trained model that you have saved. . # model = SentimentClassifier(len(class_names)) # model.load_state_dict(torch.load(&#39;best_model_state.bin&#39;)) # model = model.to(device) . Evaluation . So how good is our model on predicting sentiment? Let&#39;s start by calculating the accuracy on the test data: . test_acc, _ = eval_model( model, test_data_loader, loss_fn, device, len(df_test) ) test_acc.item() . 0.883248730964467 . The accuracy is about 1% lower on the test set. Our model seems to generalize well. . We&#39;ll define a helper function to get the predictions from our model: . def get_predictions(model, data_loader): model = model.eval() review_texts = [] predictions = [] prediction_probs = [] real_values = [] with torch.no_grad(): for d in data_loader: texts = d[&quot;review_text&quot;] input_ids = d[&quot;input_ids&quot;].to(device) attention_mask = d[&quot;attention_mask&quot;].to(device) targets = d[&quot;targets&quot;].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) probs = F.softmax(outputs, dim=1) review_texts.extend(texts) predictions.extend(preds) prediction_probs.extend(probs) real_values.extend(targets) predictions = torch.stack(predictions).cpu() prediction_probs = torch.stack(prediction_probs).cpu() real_values = torch.stack(real_values).cpu() return review_texts, predictions, prediction_probs, real_values . This is similar to the evaluation function, except that we&#39;re storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs): . y_review_texts, y_pred, y_pred_probs, y_test = get_predictions( model, test_data_loader ) . Let&#39;s have a look at the classification report . print(classification_report(y_test, y_pred, target_names=class_names)) . precision recall f1-score support negative 0.89 0.87 0.88 245 neutral 0.83 0.85 0.84 254 positive 0.92 0.93 0.92 289 accuracy 0.88 788 macro avg 0.88 0.88 0.88 788 weighted avg 0.88 0.88 0.88 788 . Looks like it is really hard to classify neutral (3 stars) reviews. And I can tell you from experience, looking at many reviews, those are hard to classify. . We&#39;ll continue with the confusion matrix: . def show_confusion_matrix(confusion_matrix): hmap = sns.heatmap(confusion_matrix, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;) hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha=&#39;right&#39;) hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha=&#39;right&#39;) plt.ylabel(&#39;True sentiment&#39;) plt.xlabel(&#39;Predicted sentiment&#39;); cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm, index=class_names, columns=class_names) show_confusion_matrix(df_cm) . This confirms that our model is having difficulty classifying neutral reviews. It mistakes those for negative and positive at a roughly equal frequency. . That&#39;s a good overview of the performance of our model. But let&#39;s have a look at an example from our test data: . idx = 2 review_text = y_review_texts[idx] true_sentiment = y_test[idx] pred_df = pd.DataFrame({ &#39;class_names&#39;: class_names, &#39;values&#39;: y_pred_probs[idx] }) . print(&quot; n&quot;.join(wrap(review_text))) print() print(f&#39;True sentiment: {class_names[true_sentiment]}&#39;) . I used to use Habitica, and I must say this is a great step up. I&#39;d like to see more social features, such as sharing tasks - only one person has to perform said task for it to be checked off, but only giving that person the experience and gold. Otherwise, the price for subscription is too steep, thus resulting in a sub-perfect score. I could easily justify $0.99/month or eternal subscription for $15. If that price could be met, as well as fine tuning, this would be easily worth 5 stars. True sentiment: neutral . Now we can look at the confidence of each sentiment of our model: . sns.barplot(x=&#39;values&#39;, y=&#39;class_names&#39;, data=pred_df, orient=&#39;h&#39;) plt.ylabel(&#39;sentiment&#39;) plt.xlabel(&#39;probability&#39;) plt.xlim([0, 1]); . Predicting on Raw Text . Let&#39;s use our model to predict the sentiment of some raw text: . review_text = &quot;I love completing my todos! Best app ever!!!&quot; . We have to use the tokenizer to encode the text: . encoded_review = tokenizer.encode_plus( review_text, max_length=MAX_LEN, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors=&#39;pt&#39;, ) . Let&#39;s get the predictions from our model: . input_ids = encoded_review[&#39;input_ids&#39;].to(device) attention_mask = encoded_review[&#39;attention_mask&#39;].to(device) output = model(input_ids, attention_mask) _, prediction = torch.max(output, dim=1) print(f&#39;Review text: {review_text}&#39;) print(f&#39;Sentiment : {class_names[prediction]}&#39;) . Review text: I love completing my todos! Best app ever!!! Sentiment : positive . Summary . Nice job! WE learned how to use BERT for sentiment analysis. You built a custom classifier using the Hugging Face library and trained it on our app reviews dataset! . Next, I&#39;ll learn how to optimize better and deploy our trained transformers using some webframework. . References . BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | L11 Language Models - Alec Radford (OpenAI) | The Illustrated BERT, ELMo, and co. | BERT Fine-Tuning Tutorial with PyTorch | How to Fine-Tune BERT for Text Classification? | Huggingface Transformers | BERT Explained: State of the art language model for NLP | .",
            "url": "https://janmejaybhoi.github.io/w/natural%20language%20processing/transformers/text%20classification/deep%20learning/2021/02/10/Sentiment-Analysis.html",
            "relUrl": "/natural%20language%20processing/transformers/text%20classification/deep%20learning/2021/02/10/Sentiment-Analysis.html",
            "date": " • Feb 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "hey there . I’m Janmejay Bhoi, a passionate self-taught and aspiring for Data Science job roles from Odisha, India. My passion for Data Science/AI lies with dreaming up ideas and making them come true with elegant interfaces. I take great care in the experience, architecture, and code quality of the things I build. Currently i’m focusing on Machine Learning and Natural Language Processing. I am also an open-source enthusiast and maintainer. . Education: . Veer Surendra Sai University Of Technology, Burla,Odisha (Aug 2016- Sep 2020) | Jupiter Science College, Bhubaneswar,Odisha (May 2013- June 2015) | . Timeline: . Natural Language Processing Intern (July 2021 – Sep 2021) | Data Analyst Intern, National Aluminium Company Limited (May 2019 – June 2019) | . Skills: . Languages - . Python, C++ | . Database - . SQL, Spark | . Machine Learning - . Regression And Classification | Ensembel Learning (Bagging &amp; Boosting) | Exploratory data analysis | Dimensionality Reduction | Clusteing | Statistical Analysis | . Deep Learning (NLP) - . Text Mining | Sentiment Analysis | Text Summarization | Named Entity Recognition | LSTM, GRU, Conv1D | Transformers | Embedding | . Libraries - . Numpy, Pandas | NLTK, SpaCy | Stats Model | Hugging face | TenssorFlow, PyTorch | Scikit Learn | FastText | TextHero | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://janmejaybhoi.github.io/w/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://janmejaybhoi.github.io/w/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}